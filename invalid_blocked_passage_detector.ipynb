{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Invalid Permanent Passage Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"kind\": \"spark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_route_analytics\", \n",
    "  \"numExecutors\": 100, \n",
    "  \"executorCores\": 1, \n",
    "  \"driverCores\": 4,\n",
    "  \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.executor.memoryOverhead\": 3072, \n",
    "    \"spark.locality.wait\": \"0\",\n",
    "    \"spark.default.parallelism\":10000\n",
    "  },\n",
    "  \"executorMemory\": \"24g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"phx2/Secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "class definition used in the script\n",
    "*/\n",
    "case class Segment (\n",
    "    segment_uuid: String,\n",
    "    start_junction_uuid: String,\n",
    "    end_junction_uuid: String\n",
    ")\n",
    "\n",
    "case class Location (\n",
    "    latitude: Double,\n",
    "    longitude: Double\n",
    ")\n",
    "\n",
    "case class SegmentTraversalCount (\n",
    "    segment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class TransitionTraversalCount (\n",
    "    firstSegment: Segment,\n",
    "    lastSegment: Segment,\n",
    "    viaSegment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class MapFeature (\n",
    "    uuid: String,\n",
    "    featureType: String,\n",
    "    segment: String,\n",
    "    direction: String,\n",
    "    isCondition: Boolean\n",
    ")\n",
    "\n",
    "case class UMMIssue (\n",
    "    issueuuid: String,\n",
    "    ummbuilduuid: String,\n",
    "    latitude: Double,\n",
    "    longitude: Double,\n",
    "    sampletripuuids: List[String],\n",
    "    featureuuids: String,\n",
    "    numberoftrips: Int,\n",
    "    cityid: Int\n",
    ")\n",
    "\n",
    "case class NavRouteDivergence (\n",
    "    trip_id: List[String],\n",
    "    pre_div_segment: Segment,\n",
    "    div_segment: Segment,\n",
    "    post_div_suggested_segment: Segment,\n",
    "    post_div_traversed_segment: Segment,\n",
    "    observations: Int\n",
    ")\n",
    "\n",
    "case class NavRouteDivergenceCount (\n",
    "    preDivSegment: Segment,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int,\n",
    "    sampleTrips: List[String]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "input params\n",
    "*/\n",
    "val startDate= \"2022-11-20\"\n",
    "val endDate = \"2022-11-26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object SegmentTraversalCountLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select segment_uuid, start_junction_uuid, end_junction_uuid, \n",
    "         | sum(suggested_traversals) as suggested_traversals,\n",
    "         | sum(overlap_traversals) as overlap_traversals,\n",
    "         | sum(actual_traversals) as actual_traversals\n",
    "         | from route_corpus_features.segment_traversal_counts\n",
    "         | where segment_uuid is not null \n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | and datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | group by segment_uuid, start_junction_uuid, end_junction_uuid\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[SegmentTraversalCount] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      SegmentTraversalCount(\n",
    "        segment = Segment(r.getAs[String](\"segment_uuid\"), r.getAs[String](\"start_junction_uuid\"), r.getAs[String](\"end_junction_uuid\")),\n",
    "        suggestedCount = r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        overlapCount = r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        actualCount = r.getAs[Long](\"actual_traversals\").toInt\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "val stcRaw = SegmentTraversalCountLoader.load(startDate, endDate)\n",
    "val stc = SegmentTraversalCountLoader.makeDataset(stcRaw).cache\n",
    "stc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "This class loads transitions from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession, Column}\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import scala.collection.mutable.Map\n",
    "\n",
    "\n",
    "object TransitionTraversalCountsLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadTTC(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select first_segment, last_segment, via_segments, \n",
    "         | sum(suggested_traversals) as suggested_traversals, \n",
    "         | sum(actual_traversals) as actual_traversals, \n",
    "         | sum(overlap_traversals) as overlap_traversals \n",
    "         | from route_corpus_features.transition_traversal_counts\n",
    "         | where datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | AND first_segment is not NULL\n",
    "         | AND last_segment is not NULL\n",
    "         | AND via_segments is not NULL\n",
    "         | AND via_segments[0] is not NULL\n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | group by first_segment, last_segment, via_segments\"\"\".stripMargin.replaceAll(\"\\n\", \" \")\n",
    "      \n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[TransitionTraversalCount] = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        \n",
    "        val firstSegment = Segment(r.getAs[Row](\"first_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        val lastSegment = Segment(r.getAs[Row](\"last_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        var viaSegmentsBuffer = ListBuffer[Segment]()\n",
    "        \n",
    "        r.getAs[Seq[Any]](\"via_segments\").foreach(row => {\n",
    "            val segmentInfo = row.asInstanceOf[Row]\n",
    "            viaSegmentsBuffer += Segment(segmentInfo.getAs[String](\"segment_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"start_junction_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"end_junction_uuid\")\n",
    "                                        )\n",
    "        })\n",
    "        \n",
    "        \n",
    "      TransitionTraversalCount(\n",
    "        firstSegment,\n",
    "        lastSegment,\n",
    "        viaSegmentsBuffer.toList.head,  \n",
    "        r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        r.getAs[Long](\"actual_traversals\").toInt)\n",
    "    })\n",
    "      .filter(T => T.firstSegment.end_junction_uuid == T.viaSegment.start_junction_uuid && \n",
    "            T.viaSegment.end_junction_uuid == T.lastSegment.start_junction_uuid)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load divergences from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "val ttcRaw = TransitionTraversalCountsLoader.loadTTC(startDate, endDate)\n",
    "val ttc = TransitionTraversalCountsLoader.makeDataset(ttcRaw).cache\n",
    "ttc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object UmmMapFeatureLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(buildId: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select uuid, \n",
    "         | data.roadfurniture.type as type, \n",
    "         | data.roadfurniture.condition as condition, \n",
    "         | data.roadfurniture.onsegment as onsegment\n",
    "         | from umm.map_feature_road_furnitures_tomtom\n",
    "         | where uuid is not null\n",
    "         | AND builduuid = '$buildId'\n",
    "         | AND data.roadfurniture.onsegment is not null\n",
    "         | AND data.roadfurniture.onsegment.uuid is not null\n",
    "         | AND data.roadfurniture.condition is null\n",
    "         | AND data.roadfurniture.type = 'PERMANENT_BARRIER'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[MapFeature] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      MapFeature(\n",
    "        uuid = r.getAs[String](\"uuid\"),\n",
    "        featureType = r.getAs[String](\"type\"),\n",
    "        segment = r.getAs[Row](\"onsegment\").getAs[String](\"uuid\"),\n",
    "        direction = r.getAs[Row](\"onsegment\").getAs[String](\"direction\"),\n",
    "        isCondition = false\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "val mapFeaturesRaw = UmmMapFeatureLoader.load(\"41837d28-6a6e-11ed-b801-b026282546b0\")\n",
    "val mapFeatures = UmmMapFeatureLoader.makeDataset(mapFeaturesRaw).cache\n",
    "mapFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "create features for invalid blocked passage using segments\n",
    "*/\n",
    "val ibpdFeatures = mapFeatures.\n",
    "    filter(feature => feature.segment != null).alias(\"F\").\n",
    "    dropDuplicates(\"segment\").\n",
    "    joinWith(stc.alias(\"S\"), col(\"F.segment\")===col(\"S.segment.segment_uuid\")).\n",
    "    map(tuple => (tuple._1.uuid, tuple._2.actualCount)).\n",
    "    cache\n",
    "\n",
    "ibpdFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "create features for invalid blocked passage using segments\n",
    "*/\n",
    "val ibpdFeatures = mapFeatures.\n",
    "    filter(feature => feature.segment != null).alias(\"F\").\n",
    "    dropDuplicates(\"segment\").\n",
    "    joinWith(ttc.alias(\"T\"), col(\"F.segment\")===col(\"T.viaSegment.segment_uuid\")).\n",
    "    groupBy(col(\"_1.uuid\").alias(\"uuid\")).\n",
    "    agg(sum(\"_2.actualCount\").alias(\"actualCount\")).\n",
    "    cache\n",
    "\n",
    "ibpdFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "compute issues count by applying trip count filter \n",
    "*/\n",
    "val issues = ibpdFeatures.filter(tuple => tuple.getAs[Long](\"actualCount\") >= 20)\n",
    "issues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "create issues by joining divergence and segment data and then joining back with map features.\n",
    "*/\n",
    "\n",
    "val seg_div = div.alias(\"D\").joinWith(stc.alias(\"S\"), col(\"D.post_div_traversed_segment.segment_uuid\")===col(\"S.segment.segment_uuid\"))\n",
    "\n",
    "val ibpdFeatures_Div = mapFeatures.\n",
    "    filter(feature => feature.segment != null).alias(\"F\").\n",
    "    dropDuplicates(\"segment\").\n",
    "    joinWith(seg_div.alias(\"D\"), col(\"F.segment\")===col(\"D._1.post_div_traversed_segment.segment_uuid\")&&col(\"D._2.suggestedCount\")===0).\n",
    "    map(tuple => (tuple._1.uuid, tuple._2._1.observations)).\n",
    "    cache\n",
    "\n",
    "ibpdFeatures_Div.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "compute issues count by applying trip count filter \n",
    "*/\n",
    "val issues_Div = ibpdFeatures_Div.filter(tuple => tuple._2 >= 3)\n",
    "issues_Div.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads issues from map_creation.meds_umm_issues\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object UmmIssueLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadUmmIssues(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | issueuuid,\n",
    "         | ummbuilduuid,\n",
    "         | latitude,\n",
    "         | longitude,\n",
    "         | sampletripuuids,\n",
    "         | featureuuids,\n",
    "         | numberoftrips,\n",
    "         | cityid\n",
    "         | from map_creation.meds_umm_issues\n",
    "         | where createddate between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | and productionrun = false\n",
    "         | and detectorname = 'InvalidPermanentBlockPassageDetector'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[UMMIssue] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        var segments = ListBuffer[String]()\n",
    "        r.getAs[Seq[String]](\"featureuuids\").foreach(row => segments += row)\n",
    "        \n",
    "        var trips = ListBuffer[String]()            \n",
    "        \n",
    "        UMMIssue(\n",
    "            issueuuid = r.getAs[String](\"issueuuid\"),\n",
    "            ummbuilduuid = r.getAs[String](\"ummbuilduuid\"),\n",
    "            latitude = r.getAs[Double](\"latitude\"),\n",
    "            longitude = r.getAs[Double](\"longitude\"),\n",
    "            sampletripuuids = trips.toList,\n",
    "            featureuuids = segments.toList.head,\n",
    "            numberoftrips = r.getAs[Int](\"numberoftrips\"),\n",
    "            cityid = r.getAs[Int](\"cityid\")\n",
    "          )\n",
    "    })\n",
    "      \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** \n",
    "load map_creation.meds_umm_issues \n",
    "*/\n",
    "val ummIssuesRaw = UmmIssueLoader.loadUmmIssues(\"2022-07-21\", \"2022-07-21\")\n",
    "val ummIssues = UmmIssueLoader.makeDataset(ummIssuesRaw)\n",
    "ummIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Join issues without divergence and with divergence\n",
    "*/\n",
    "val joinedIssues = issues.alias(\"I\").joinWith(ummIssues.alias(\"U\"), col(\"I._1\")===col(\"U.featureuuids\"), \"outer\")\n",
    "joinedIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Show top 10 issues\n",
    "*/\n",
    "joinedIssues.filter(issue => issue._1 == null && issue._2 != null).limit(10).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_Div.alias(\"I\").joinWith(ummIssues.alias(\"U\"), col(\"I._1\")===col(\"U.featureuuids\"), \"outer\").\n",
    "filter(issue => issue._1 == null && issue._2 != null).limit(10).collect().foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
