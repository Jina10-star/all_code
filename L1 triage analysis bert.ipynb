{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eaac7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "788f6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8245471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(message):\n",
    "\n",
    "    #stopwords\n",
    "    new_stopwords=['Hi','Hello','Team','Thanks','Hey','regards','please','jira','@uber.com','@ext.uber.com',' !image.png|thumbnail!','!Capture.PNG|thumbnail!']\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    stpwrd.extend(new_stopwords)\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #removing the numerical values and working only with text values\n",
    "    message = re.sub('[^a-zA-Z]', \" \", message )\n",
    "    #lowering and removing punctuation\n",
    "    message = re.sub(r'[^\\w\\s]','', message.lower())\n",
    "    #removing the stopwords\n",
    "    message = ' '.join([word for word in message.split() if word not in stpwrd and len(word)>1])\n",
    "    #lemmatizing the text\n",
    "    message =  \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation])\n",
    "    #message = [lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation]\n",
    "    #tagged_sentence = nltk.tag.pos_tag(message.split())\n",
    "    #edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' or tag != 'NNPS']\n",
    "    #message =  \" \".join(edited_sentence)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d59a70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44513, 5)\n",
      "(40025, 5)\n"
     ]
    }
   ],
   "source": [
    "#read input from historical data into dataframe\n",
    "data_df = pd.read_excel('/Users/jghosh2/Documents/my-notebook/L1 triage POC/data/Updated-JIRA_DUMP_Mar_2020_March_2022_Sourabh.xlsx',usecols=['Summary','Description','Component'])\n",
    "data_df['Summary']=data_df['Summary'].astype(str)\n",
    "data_df['Description']=data_df['Description'].astype(str)\n",
    "data_df.dropna()\n",
    "#choose sample data from entire data\n",
    "data_df = data_df.sample(frac=1, random_state=42)\n",
    "data_df['combined_text'] = data_df[['Summary','Description']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_df['processed_text'] = data_df['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_df.dropna()\n",
    "print(data_df.shape)\n",
    "data_df=data_df.drop_duplicates(subset=['processed_text','Component'],keep='first')\n",
    "#data_df_duplicated=data_df[data_df.duplicated(subset=['processed_text','target'],keep=False)]\n",
    "#print(data_df_duplicated.shape)\n",
    "print(data_df.shape)\n",
    "data_df = data_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "262d9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode labels in column 'species'.\n",
    "data_df['Component_label']= label_encoder.fit_transform(data_df['Component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfac22aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Component_label'] = data_df['Component_label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a944e298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9743dcf0de5942caa1bd037d7c6be9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678371be0ba04f62a288ee1ac890c5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1439f48a28f94debb15c13cdf7ffb6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12267f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.encode_plus(\n",
    "    data_df['processed_text'].iloc[0], \n",
    "    max_length=256, \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03c682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_ids = np.zeros((len(data_df), 256))\n",
    "X_attn_masks = np.zeros((len(data_df), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f0fd632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(data_df, ids, masks, tokenizer):\n",
    "    for i, text in tqdm(enumerate(data_df['processed_text'])):\n",
    "        tokenized_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=256, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            add_special_tokens=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        ids[i, :] = tokenized_text.input_ids\n",
    "        masks[i, :] = tokenized_text.attention_mask\n",
    "    return ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ac4c662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9431323e7f2c4ed592b77ed3efe3daf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_input_ids, X_attn_masks = generate_training_data(data_df, X_input_ids, X_attn_masks, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40635072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40025, 61)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros((len(data_df), 61))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b26b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[np.arange(len(data_df)), data_df['Component_label'].values] = 1 # one-hot encoded target tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db615a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(256,), dtype=tf.float64, name=None), TensorSpec(shape=(61,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "492eba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attn_masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dd87872",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(SentimentDatasetMapFunction) # converting to required format for tensorflow dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff450870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(256,), dtype=tf.float64, name=None), 'attention_mask': TensorSpec(shape=(256,), dtype=tf.float64, name=None)}, TensorSpec(shape=(61,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bf1ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b280be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.8\n",
    "train_size = int((len(data_df)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b65fd4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30235a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f67fe328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c4086708b4442382a9d4dfe940bd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5429129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " intermediate_layer (Dense)     (None, 512)          393728      ['bert[1][1]']                   \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 61)           31293       ['intermediate_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,735,293\n",
      "Trainable params: 108,735,293\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining 2 input layers for input_ids and attn_masks\n",
    "input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')\n",
    "\n",
    "bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\n",
    "intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\n",
    "output_layer = tf.keras.layers.Dense(61, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n",
    "\n",
    "sentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f421ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75b40a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1acee38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 573/2000 [=======>......................] - ETA: 8:06:56 - loss: 2.6467 - accuracy: 0.3743"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-36d7783701c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = sentiment_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = sentiment_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model.save('sentiment_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e749e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9beee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b41cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cf1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1191e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data_df, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d61416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = list(compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(data_df['Component']),\n",
    "                                        y = data_df['Component']                                                    \n",
    "                                    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1be8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights.sort()\n",
    "weights={}\n",
    "for index, weight in enumerate(class_weights) :\n",
    "    weights[index]=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2885b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train['processed_text'].values, X_train['Component'].values))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test['processed_text'].values, X_test['Component'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bd7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,len(data_df['Component'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4fb1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(data_df['Component'].unique()),\n",
    "        values=tf.constant(list(range(0,len(data_df['Component'].unique())))),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"target_encoding\"\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def target(x):\n",
    "    return table.lookup(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2738215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset, size=5):\n",
    "      for batch, label in dataset.take(size):\n",
    "          print(batch.numpy())\n",
    "          print(target(label).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a87ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34e3d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(text, labels):\n",
    "    return text, tf.one_hot(target(labels),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e7f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_f=dataset_train.map(fetch)\n",
    "test_data_f=dataset_test.map(fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02645847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'need help split invoice hi team need help splitting invoice prepare mass addition',\n",
       "        b'request create po another entity po creator nangulo uber com manager detailed justification benefit manager latam responsible vendor management benefit granted employee costa rica creation po entity require access entity portier costa rica srl',\n",
       "        b'access oracle apex hi recently joined strategic finance team london would possible receive access oracle apex fp permission thank help best shaked',\n",
       "        b'clone delegate request aj greulich hi fintech team set michael huaco delegate aj greulich coupa oracle approval date range start date end date reason paternity leave let know question thanks blaine http uber box com shared static xdf wm te bi pior dc gif blaine milner finops procure pay gpo bmilner uber com mailto bmilner uber com uber com http www uber com twitter http twitter com uber facebook http www facebook com uber blog http blog uber com instagram http www instagram com uber supplier resource found http www uber com info supplier confidentiality notice mail message including attachment sole use intended recipient may contain privileged confidential information unauthorized review use disclosure distribution prohibited intended recipient contact sender reply mail destroy copy original message',\n",
       "        b'unable access coupa hey trying access coupa onelogin getting error page login errorunable locate user record sso id melody kuo uber com http uberinternal com servicedesk customershim image icon mail small gif fromissue mailto melody kuo uber com valid sso id assigned user able access service whober concur workday etc fine idea thanks melody'],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(5, 6), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_labels = next(iter(train_data_f.batch(5)))\n",
    "train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29bdf826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[ 0.14832334,  0.11642308, -0.00259464, -0.2741825 , -0.03419008,\n",
       "         0.05448474,  0.09951627,  0.13680217, -0.21613999,  0.45808873,\n",
       "        -0.10294521, -0.11740037, -0.03200293,  0.08250536, -0.05801883,\n",
       "        -0.20927465, -0.0221511 ,  0.09924161, -0.09119239,  0.0615944 ,\n",
       "         0.13973267,  0.16179985, -0.08059154, -0.08594684,  0.0673902 ,\n",
       "        -0.00145557, -0.00130037,  0.11433198, -0.23935963, -0.00845843,\n",
       "        -0.05642676, -0.01102808,  0.10869243,  0.01076306, -0.06180575,\n",
       "         0.00575203, -0.19492295,  0.08626429,  0.04200781,  0.01668094,\n",
       "         0.08677208, -0.13424514,  0.04854997, -0.10535169, -0.09532416,\n",
       "         0.05975296, -0.10966925,  0.21322154,  0.14673808, -0.08014761,\n",
       "        -0.00512229, -0.16650414, -0.02921075,  0.08152414, -0.18004051,\n",
       "         0.16847077, -0.02556576, -0.30985662, -0.05359058,  0.14351867,\n",
       "         0.10968975,  0.2667437 ,  0.01079381, -0.16184346, -0.15252608,\n",
       "         0.33485833, -0.18355441, -0.15465418, -0.1767907 ,  0.05760702,\n",
       "        -0.00303852, -0.00916222,  0.06596314, -0.04850843,  0.07315758,\n",
       "         0.05555885,  0.1242366 , -0.11820403,  0.00952304, -0.13448174,\n",
       "         0.22442801,  0.0781167 , -0.07080161, -0.17793061,  0.25694665,\n",
       "         0.141633  ,  0.04885754, -0.00645167,  0.3063877 ,  0.25064746,\n",
       "        -0.05089708, -0.07296604, -0.10931824, -0.12802964,  0.07215   ,\n",
       "        -0.19113162, -0.25385243, -0.01959343, -0.17271605,  0.00639918,\n",
       "         0.12255625,  0.06904496,  0.0090955 , -0.07386574, -0.04699721,\n",
       "         0.14155184,  0.116033  ,  0.1852015 ,  0.13416424,  0.1260025 ,\n",
       "        -0.162079  ,  0.04108867, -0.27624035, -0.0513684 ,  0.05885069,\n",
       "         0.1081138 , -0.03346517, -0.0488877 ,  0.09956218,  0.13959822,\n",
       "        -0.08125585, -0.14201838, -0.22590266,  0.04869226,  0.06685221,\n",
       "        -0.03320739,  0.06051927,  0.08628503]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, output_shape=[128], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e55975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 128)               124642688 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,686,246\n",
      "Trainable params: 124,686,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "for units in [128, 128, 64 , 32]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8148a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "802629f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_f=train_data_f.shuffle(70000).batch(512)\n",
    "test_data_f=test_data_f.batch(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c123bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 77s 1s/step - loss: 2.5733 - accuracy: 0.1537 - val_loss: 6.8391 - val_accuracy: 0.0586\n",
      "Epoch 2/4\n",
      "63/63 [==============================] - 76s 1s/step - loss: 1014.4958 - accuracy: 0.1072 - val_loss: 6157.3218 - val_accuracy: 0.0643\n",
      "Epoch 3/4\n",
      "63/63 [==============================] - 76s 1s/step - loss: 35850.4180 - accuracy: 0.1046 - val_loss: 130183.4688 - val_accuracy: 0.0643\n",
      "Epoch 4/4\n",
      "63/63 [==============================] - 77s 1s/step - loss: 358917.6250 - accuracy: 0.1081 - val_loss: 497689.4375 - val_accuracy: 0.0586\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data_f,\n",
    "                    epochs=4,\n",
    "                    validation_data=test_data_f,\n",
    "                    verbose=1,\n",
    "                    class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2fe1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 811630.8125 - accuracy: 0.1026 - 414ms/epoch - 414ms/step\n",
      "[811630.8125, 0.10256090015172958]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dataset_test.map(fetch).batch(11491), verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94df8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = next(iter(dataset_test.map(fetch).batch(45963)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bdffe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67bde187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf4b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5837\n",
      "           1       0.00      0.00      0.00        95\n",
      "           2       0.00      0.00      0.00       515\n",
      "           3       0.00      0.00      0.00       469\n",
      "           4       0.00      0.00      0.00       268\n",
      "           5       0.10      1.00      0.19       821\n",
      "\n",
      "    accuracy                           0.10      8005\n",
      "   macro avg       0.02      0.17      0.03      8005\n",
      "weighted avg       0.01      0.10      0.02      8005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels.numpy().argmax(axis=1), y_pred.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4481c1d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shallow_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b7b5adbdd1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilename_primary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'finalized_model.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshallow_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shallow_rf' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import datetime\n",
    "# save the model to disk\n",
    "filename_primary= 'finalized_model.sav'\n",
    "pickle.dump(shallow_rf, open(filename_primary, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
