{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook runs LSR with WCOF as the OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://michelangelo-studio.uberinternal.com/file/f379d46c-10ff-4631-b320-5ff5d6f96f70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "\n",
      "Package operations: 1 install, 6 updates, 0 removals\n",
      "\n",
      "  - Updating tornado (4.5.3 -> 5.1.1)\n",
      "  - Updating cachetools (3.1.1 -> 4.1.1)\n",
      "  - Updating idna (2.8 -> 2.10)\n",
      "  - Updating protobuf (3.12.2 -> 3.13.0)\n",
      "  - Updating requests (2.22.0 -> 2.24.0)\n",
      "  - Installing wheel (0.35.1)\n",
      "  - Updating h3 (3.6.4 -> 3.7.0)\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: galileo in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: requests in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo) (2.24.0)\n",
      "Requirement already satisfied: pyusb>=1a in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo) (1.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->galileo) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->galileo) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->galileo) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->galileo) (3.0.4)\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: galileo-py in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (1.19.5)\n",
      "Requirement already satisfied: wonkapy>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (3.0.2)\n",
      "Requirement already satisfied: tornado-extras>=1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (1.4.0)\n",
      "Collecting tornado<5,>=4.4\n",
      "  Using cached tornado-4.5.3-cp36-cp36m-linux_x86_64.whl\n",
      "Requirement already satisfied: jaeger-client-python>=4.0.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (4.0.0)\n",
      "Requirement already satisfied: opentracing-instrumentation in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (3.3.1)\n",
      "Requirement already satisfied: tchannel>=2.0.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (2.1.0)\n",
      "Collecting cachetools~=3.0\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: ujson in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (1.35)\n",
      "Requirement already satisfied: clay-sortsol-logging>1.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (1.2.0)\n",
      "Requirement already satisfied: flask in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (1.1.2)\n",
      "Requirement already satisfied: m3>=3.2.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (4.3.1)\n",
      "Requirement already satisfied: six in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (1.15.0)\n",
      "Requirement already satisfied: deprecation<3,>=1.0.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from galileo-py) (2.0.7)\n",
      "Requirement already satisfied: raven in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-sortsol-logging>1.1->galileo-py) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-sortsol-logging>1.1->galileo-py) (5.3.1)\n",
      "Requirement already satisfied: sortsol>=1.0.298 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-sortsol-logging>1.1->galileo-py) (1.1.0)\n",
      "Requirement already satisfied: packaging in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from deprecation<3,>=1.0.1->galileo-py) (20.4)\n",
      "Requirement already satisfied: jaeger-client<5,>4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from jaeger-client-python>=4.0.0->galileo-py) (4.3.0)\n",
      "Requirement already satisfied: futures in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from jaeger-client-python>=4.0.0->galileo-py) (3.1.1)\n",
      "Requirement already satisfied: clay-config in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from jaeger-client-python>=4.0.0->galileo-py) (2.1.1)\n",
      "Requirement already satisfied: future in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=3.2.1->galileo-py) (0.16.0)\n",
      "Requirement already satisfied: thriftrw>=1.8 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=3.2.1->galileo-py) (1.8.1)\n",
      "Requirement already satisfied: wrapt in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from opentracing-instrumentation->galileo-py) (1.12.1)\n",
      "Requirement already satisfied: opentracing<3,>=2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from opentracing-instrumentation->galileo-py) (2.3.0)\n",
      "Requirement already satisfied: contextlib2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from opentracing-instrumentation->galileo-py) (0.6.0.post1)\n",
      "Requirement already satisfied: crcmod<2,>=1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel>=2.0.1->galileo-py) (1.7)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel>=2.0.1->galileo-py) (1.0.2)\n",
      "Requirement already satisfied: toro in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tornado-extras>=1->galileo-py) (1.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from flask->galileo-py) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from flask->galileo-py) (1.0.1)\n",
      "Requirement already satisfied: click>=5.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from flask->galileo-py) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from flask->galileo-py) (1.1.0)\n",
      "Requirement already satisfied: thrift in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from jaeger-client<5,>4->jaeger-client-python>=4.0.0->galileo-py) (0.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask->galileo-py) (1.1.1)\n",
      "Requirement already satisfied: pycrypto>=2.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (2.6.1)\n",
      "Requirement already satisfied: dnspython>=1.9.4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (1.16.0)\n",
      "Requirement already satisfied: kafka-rest-py>=0.2.10 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (0.3.4)\n",
      "Requirement already satisfied: kazoo>=0.7 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (2.8.0)\n",
      "Requirement already satisfied: send-nsca>=0.1.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (0.1.4.1)\n",
      "Requirement already satisfied: uhashring>=1.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (1.1)\n",
      "Requirement already satisfied: boto<3.0,>=2.2.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (2.49.0)\n",
      "Requirement already satisfied: ubermon>=0.4.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (0.4.1)\n",
      "Requirement already satisfied: ply in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from thriftrw>=1.8->m3>=3.2.1->galileo-py) (3.11)\n",
      "Requirement already satisfied: clay-config-file in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-config->jaeger-client-python>=4.0.0->galileo-py) (1.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from packaging->deprecation<3,>=1.0.1->galileo-py) (2.4.7)\n",
      "Requirement already satisfied: redis>=2.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from ubermon>=0.4.1->sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (3.4.1)\n",
      "Requirement already satisfied: python-dateutil>=1.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from ubermon>=0.4.1->sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (2.8.1)\n",
      "Requirement already satisfied: simplejson in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from ubermon>=0.4.1->sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (3.10.0)\n",
      "Requirement already satisfied: psutil>=1.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from ubermon>=0.4.1->sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (5.7.3)\n",
      "Requirement already satisfied: setuptools>=31.0.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from ubermon>=0.4.1->sortsol>=1.0.298->clay-sortsol-logging>1.1->galileo-py) (54.1.2)\n",
      "Installing collected packages: tornado, cachetools\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 5.1.1\n",
      "    Uninstalling tornado-5.1.1:\n",
      "      Successfully uninstalled tornado-5.1.1\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.1.1\n",
      "    Uninstalling cachetools-4.1.1:\n",
      "      Successfully uninstalled cachetools-4.1.1\n",
      "Successfully installed cachetools-3.1.1 tornado-4.5.3\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: tchannel in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (2.1.0)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (1.0.2)\n",
      "Requirement already satisfied: future in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (0.16.0)\n",
      "Requirement already satisfied: thriftrw<2,>=0.4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (1.8.1)\n",
      "Requirement already satisfied: opentracing-instrumentation>3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (3.3.1)\n",
      "Requirement already satisfied: crcmod<2,>=1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (1.7)\n",
      "Requirement already satisfied: tornado<5,>=4.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (4.5.3)\n",
      "Requirement already satisfied: six in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (1.15.0)\n",
      "Requirement already satisfied: opentracing>2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (2.3.0)\n",
      "Requirement already satisfied: contextlib2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (0.6.0.post1)\n",
      "Requirement already satisfied: futures in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tchannel) (3.1.1)\n",
      "Requirement already satisfied: wrapt in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from opentracing-instrumentation>3->tchannel) (1.12.1)\n",
      "Requirement already satisfied: ply in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from thriftrw<2,>=0.4->tchannel) (3.11)\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: matching-ds-tools==0.7.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.7.3)\n",
      "Requirement already satisfied: dataclasses==0.8 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools==0.7.3) (0.8)\n",
      "Collecting pandas>1\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: pytz>2021 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools==0.7.3) (2021.3)\n",
      "Requirement already satisfied: queryrunner-client>3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools==0.7.3) (3.4.1)\n",
      "Collecting requests==2.22.0\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "Collecting protobuf==3.12.2\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: haversine==2.1.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools==0.7.3) (2.1.2)\n",
      "Requirement already satisfied: ujson==1.35 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools==0.7.3) (1.35)\n",
      "Collecting h3==3.6.4\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/bf/77/df4b7b3f989cbd3aec5894f6058a31981f05f151f9bbfc3f5827178eeedc/h3-3.6.4-cp36-cp36m-manylinux2010_x86_64.whl (761 kB)\n",
      "Requirement already satisfied: six>=1.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from protobuf==3.12.2->matching-ds-tools==0.7.3) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from protobuf==3.12.2->matching-ds-tools==0.7.3) (54.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools==0.7.3) (2020.6.20)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools==0.7.3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools==0.7.3) (1.25.11)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas>1->matching-ds-tools==0.7.3) (1.16.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas>1->matching-ds-tools==0.7.3) (2.8.1)\n",
      "Requirement already satisfied: wonkapy>=3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (3.0.2)\n",
      "Requirement already satisfied: future==0.16.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (0.16.0)\n",
      "Requirement already satisfied: pysocks>=1.5.7 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (1.7.1)\n",
      "Requirement already satisfied: querybuilder-client==0.6.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (0.6.1)\n",
      "Requirement already satisfied: clay-config>=2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (2.1.1)\n",
      "Requirement already satisfied: pyyaml in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (5.3.1)\n",
      "Requirement already satisfied: m3>=4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (4.3.1)\n",
      "Requirement already satisfied: hive-csv-reader==0.1.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools==0.7.3) (0.1.9)\n",
      "Requirement already satisfied: clay-config-file in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-config>=2->queryrunner-client>3->matching-ds-tools==0.7.3) (1.2.1)\n",
      "Requirement already satisfied: thriftrw>=1.8 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=4->queryrunner-client>3->matching-ds-tools==0.7.3) (1.8.1)\n",
      "Requirement already satisfied: tornado<6 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=4->queryrunner-client>3->matching-ds-tools==0.7.3) (4.5.3)\n",
      "Requirement already satisfied: ply in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from thriftrw>=1.8->m3>=4->queryrunner-client>3->matching-ds-tools==0.7.3) (3.11)\n",
      "Installing collected packages: idna, requests, protobuf, pandas, h3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.13.0\n",
      "    Uninstalling protobuf-3.13.0:\n",
      "      Successfully uninstalled protobuf-3.13.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 0.25.3\n",
      "    Uninstalling pandas-0.25.3:\n",
      "      Successfully uninstalled pandas-0.25.3\n",
      "  Attempting uninstall: h3\n",
      "    Found existing installation: h3 3.7.0\n",
      "    Uninstalling h3-3.7.0:\n",
      "      Successfully uninstalled h3-3.7.0\n",
      "Successfully installed h3-3.6.4 idna-2.8 pandas-1.1.5 protobuf-3.12.2 requests-2.22.0\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: dataclasses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.8)\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Collecting pandas==0.25.3\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas==0.25.3) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas==0.25.3) (1.16.6)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas==0.25.3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==0.25.3) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "Successfully installed pandas-0.25.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 0.71.0 requires cachetools>=4.0, but you have cachetools 3.1.1 which is incompatible.\n",
      "streamlit 0.71.0 requires tornado>=5.0, but you have tornado 4.5.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires bcrypt==3.1.7, but you have bcrypt 3.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires certifi==2020.4.5.1, but you have certifi 2020.6.20 which is incompatible.\n",
      "mxpkg 1.1.65 requires cffi==1.14.0, but you have cffi 1.14.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires clay-config-file==1.2.0, but you have clay-config-file 1.2.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires click==7.1.1, but you have click 7.1.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires colorama==0.4.3, but you have colorama 0.4.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires cryptography==2.9, but you have cryptography 3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires cython==0.29.16, but you have cython 0.29.21 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitdb==4.0.2, but you have gitdb 4.0.5 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitpython==3.1.0, but you have gitpython 3.1.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio==1.28.1, but you have grpcio 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio-tools==1.28.1, but you have grpcio-tools 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires h3==3.7.4, but you have h3 3.7.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires humanize==2.3.0, but you have humanize 3.1.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires idna==2.9, but you have idna 2.10 which is incompatible.\n",
      "mxpkg 1.1.65 requires jinja2==2.11.1, but you have jinja2 2.11.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires joblib==0.10.3, but you have joblib 0.13.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires kazoo==2.7.0, but you have kazoo 2.8.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires m3==4.2.1, but you have m3 4.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires matplotlib==2.2.4, but you have matplotlib 3.3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires neuropod==0.1.1, but you have neuropod 0.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires numpy==1.16.4, but you have numpy 1.16.6 which is incompatible.\n",
      "mxpkg 1.1.65 requires opentracing-instrumentation==3.2.1, but you have opentracing-instrumentation 3.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires packaging==20.3, but you have packaging 20.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pandas==0.25, but you have pandas 0.25.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires paramiko==2.7.1, but you have paramiko 2.7.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires plotly==4.1.1, but you have plotly 4.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires psutil==5.7.0, but you have psutil 5.7.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires py4j==0.10.4, but you have py4j 0.10.7 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyarrow==0.16.0, but you have pyarrow 2.0.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pynacl==1.3.0, but you have pynacl 1.4.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pypika==0.37.11, but you have pypika 0.47.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pytz==2022.2, but you have pytz 2021.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyyaml==5.1, but you have pyyaml 5.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires requests==2.23.0, but you have requests 2.24.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires scikit-learn==0.20.3, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires scipy==1.2.1, but you have scipy 1.5.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires seaborn==0.9.0, but you have seaborn 0.11.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires semver==2.9.1, but you have semver 2.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires six==1.14.0, but you have six 1.15.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires smmap==3.0.1, but you have smmap 3.0.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires statsmodels==0.9.0, but you have statsmodels 0.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires thrift==0.11.0, but you have thrift 0.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires tqdm==4.33.0, but you have tqdm 4.51.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires typing==3.7.4.1, but you have typing 3.7.4.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires urllib3==1.25.8, but you have urllib3 1.25.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires watchdog==0.9.0, but you have watchdog 0.10.3 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyenv3 0.1.0 requires scikit-learn==0.18.1, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "streamlit 0.71.0 requires cachetools>=4.0, but you have cachetools 3.1.1 which is incompatible.\n",
      "streamlit 0.71.0 requires tornado>=5.0, but you have tornado 4.5.3 which is incompatible.\n",
      "production-function 0.1.38 requires scikit-learn==0.20.3, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "production-function 0.1.38 requires scipy==1.2.1, but you have scipy 1.5.3 which is incompatible.\n",
      "production-function 0.1.38 requires seaborn==0.9.0, but you have seaborn 0.11.0 which is incompatible.\n",
      "production-function 0.1.38 requires statsmodels==0.9.0, but you have statsmodels 0.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires bcrypt==3.1.7, but you have bcrypt 3.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires certifi==2020.4.5.1, but you have certifi 2020.6.20 which is incompatible.\n",
      "mxpkg 1.1.65 requires cffi==1.14.0, but you have cffi 1.14.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires clay-config-file==1.2.0, but you have clay-config-file 1.2.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires click==7.1.1, but you have click 7.1.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires colorama==0.4.3, but you have colorama 0.4.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires cryptography==2.9, but you have cryptography 3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires cython==0.29.16, but you have cython 0.29.21 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitdb==4.0.2, but you have gitdb 4.0.5 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitpython==3.1.0, but you have gitpython 3.1.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio==1.28.1, but you have grpcio 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio-tools==1.28.1, but you have grpcio-tools 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires h3==3.7.4, but you have h3 3.6.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires humanize==2.3.0, but you have humanize 3.1.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires idna==2.9, but you have idna 2.8 which is incompatible.\n",
      "mxpkg 1.1.65 requires jinja2==2.11.1, but you have jinja2 2.11.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires joblib==0.10.3, but you have joblib 0.13.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires kazoo==2.7.0, but you have kazoo 2.8.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires m3==4.2.1, but you have m3 4.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires matplotlib==2.2.4, but you have matplotlib 3.3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires neuropod==0.1.1, but you have neuropod 0.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires numpy==1.16.4, but you have numpy 1.16.6 which is incompatible.\n",
      "mxpkg 1.1.65 requires opentracing-instrumentation==3.2.1, but you have opentracing-instrumentation 3.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires packaging==20.3, but you have packaging 20.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pandas==0.25, but you have pandas 1.1.5 which is incompatible.\n",
      "mxpkg 1.1.65 requires paramiko==2.7.1, but you have paramiko 2.7.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires plotly==4.1.1, but you have plotly 4.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires protobuf>=3.13.0, but you have protobuf 3.12.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires psutil==5.7.0, but you have psutil 5.7.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires py4j==0.10.4, but you have py4j 0.10.7 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyarrow==0.16.0, but you have pyarrow 2.0.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pynacl==1.3.0, but you have pynacl 1.4.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pypika==0.37.11, but you have pypika 0.47.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pytz==2022.2, but you have pytz 2021.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyyaml==5.1, but you have pyyaml 5.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires requests==2.23.0, but you have requests 2.22.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires scikit-learn==0.20.3, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires scipy==1.2.1, but you have scipy 1.5.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires seaborn==0.9.0, but you have seaborn 0.11.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires semver==2.9.1, but you have semver 2.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires six==1.14.0, but you have six 1.15.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires smmap==3.0.1, but you have smmap 3.0.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires statsmodels==0.9.0, but you have statsmodels 0.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires thrift==0.11.0, but you have thrift 0.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires tqdm==4.33.0, but you have tqdm 4.51.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires typing==3.7.4.1, but you have typing 3.7.4.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires urllib3==1.25.8, but you have urllib3 1.25.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires watchdog==0.9.0, but you have watchdog 0.10.3 which is incompatible.\n",
      "geoip2 4.1.0 requires requests<3.0.0,>=2.24.0, but you have requests 2.22.0 which is incompatible.\n",
      "abpkg 1.0.8 requires protobuf>=3.13.0, but you have protobuf 3.12.2 which is incompatible.\n",
      "abpkg 1.0.8 requires requests>=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (/dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyenv3 0.1.0 requires scikit-learn==0.18.1, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "streamlit 0.71.0 requires cachetools>=4.0, but you have cachetools 3.1.1 which is incompatible.\n",
      "streamlit 0.71.0 requires tornado>=5.0, but you have tornado 4.5.3 which is incompatible.\n",
      "production-function 0.1.38 requires scikit-learn==0.20.3, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "production-function 0.1.38 requires scipy==1.2.1, but you have scipy 1.5.3 which is incompatible.\n",
      "production-function 0.1.38 requires seaborn==0.9.0, but you have seaborn 0.11.0 which is incompatible.\n",
      "production-function 0.1.38 requires statsmodels==0.9.0, but you have statsmodels 0.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires bcrypt==3.1.7, but you have bcrypt 3.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires certifi==2020.4.5.1, but you have certifi 2020.6.20 which is incompatible.\n",
      "mxpkg 1.1.65 requires cffi==1.14.0, but you have cffi 1.14.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires clay-config-file==1.2.0, but you have clay-config-file 1.2.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires click==7.1.1, but you have click 7.1.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires colorama==0.4.3, but you have colorama 0.4.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires cryptography==2.9, but you have cryptography 3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires cython==0.29.16, but you have cython 0.29.21 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitdb==4.0.2, but you have gitdb 4.0.5 which is incompatible.\n",
      "mxpkg 1.1.65 requires gitpython==3.1.0, but you have gitpython 3.1.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio==1.28.1, but you have grpcio 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires grpcio-tools==1.28.1, but you have grpcio-tools 1.29.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires h3==3.7.4, but you have h3 3.6.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires humanize==2.3.0, but you have humanize 3.1.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires idna==2.9, but you have idna 2.8 which is incompatible.\n",
      "mxpkg 1.1.65 requires jinja2==2.11.1, but you have jinja2 2.11.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires joblib==0.10.3, but you have joblib 0.13.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires kazoo==2.7.0, but you have kazoo 2.8.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires m3==4.2.1, but you have m3 4.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires matplotlib==2.2.4, but you have matplotlib 3.3.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires neuropod==0.1.1, but you have neuropod 0.2.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires numpy==1.16.4, but you have numpy 1.16.6 which is incompatible.\n",
      "mxpkg 1.1.65 requires opentracing-instrumentation==3.2.1, but you have opentracing-instrumentation 3.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires packaging==20.3, but you have packaging 20.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pandas==0.25, but you have pandas 0.25.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires paramiko==2.7.1, but you have paramiko 2.7.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires plotly==4.1.1, but you have plotly 4.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires protobuf>=3.13.0, but you have protobuf 3.12.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires psutil==5.7.0, but you have psutil 5.7.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires py4j==0.10.4, but you have py4j 0.10.7 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyarrow==0.16.0, but you have pyarrow 2.0.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pynacl==1.3.0, but you have pynacl 1.4.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires pypika==0.37.11, but you have pypika 0.47.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires pytz==2022.2, but you have pytz 2021.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires pyyaml==5.1, but you have pyyaml 5.3.1 which is incompatible.\n",
      "mxpkg 1.1.65 requires requests==2.23.0, but you have requests 2.22.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires scikit-learn==0.20.3, but you have scikit-learn 0.22.2 which is incompatible.\n",
      "mxpkg 1.1.65 requires scipy==1.2.1, but you have scipy 1.5.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires seaborn==0.9.0, but you have seaborn 0.11.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires semver==2.9.1, but you have semver 2.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires six==1.14.0, but you have six 1.15.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires smmap==3.0.1, but you have smmap 3.0.4 which is incompatible.\n",
      "mxpkg 1.1.65 requires statsmodels==0.9.0, but you have statsmodels 0.12.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires thrift==0.11.0, but you have thrift 0.13.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires tqdm==4.33.0, but you have tqdm 4.51.0 which is incompatible.\n",
      "mxpkg 1.1.65 requires typing==3.7.4.1, but you have typing 3.7.4.3 which is incompatible.\n",
      "mxpkg 1.1.65 requires urllib3==1.25.8, but you have urllib3 1.25.11 which is incompatible.\n",
      "mxpkg 1.1.65 requires watchdog==0.9.0, but you have watchdog 0.10.3 which is incompatible.\n",
      "matching-ds-tools 0.7.3 requires pandas>1, but you have pandas 0.25.3 which is incompatible.\n",
      "abpkg 1.0.8 requires protobuf>=3.13.0, but you have protobuf 3.12.2 which is incompatible.\n",
      "abpkg 1.0.8 requires requests>=2.23.0, but you have requests 2.22.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source $VIRTUAL_ENV_DIR/python3/bin/activate\n",
    "\n",
    "install_package_python3.sh add dsw_qr==0.1.13\n",
    "\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install galileo\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install galileo-py\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install tchannel\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install matching-ds-tools==0.7.3\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install dataclasses\n",
    "$VIRTUAL_ENV_DIR/python3/bin/python -m pip install pandas==0.25.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import logging\n",
    "import math\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from queryrunner_client import Client\n",
    "qclient = Client(user_email='mshehata@uber.com')\n",
    "USER_EMAIL = 'mshehata@uber.com'\n",
    "CONSUMER_NAME = 'intelligentdispatch'\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#num_cores = multiprocessing.cpu_count()\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from queryrunner_client import Client as QRClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdstk\n",
    "from mdstk.data_fetcher.data_fetcher import DataFetcher\n",
    "from mdstk.data_fetcher.cached_data_fetcher import CachedDataFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdstk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection for lsr\n",
    "QUERY = \"\"\"\n",
    "with dispatch as (select distinct\n",
    "    datestr,\n",
    "    msg.cityid,\n",
    "    msg.ctplangenrequestuuid as plangen_uuid,\n",
    "    msg.ctrequestuuid as scan_uuid,\n",
    "    j as job_uuid,\n",
    "    msg.supplyuuid,\n",
    "    msg.planactiontype\n",
    "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
    "cross join unnest(msg.jobuuid) jobs(j)\n",
    "where datestr = '{datestr}'\n",
    "and msg.cityid = {city_id}\n",
    "and msg.vehicleviewid = {vvid} \n",
    "and msg.tenancy = 'uber/production'\n",
    "and CARDINALITY(msg.jobuuid) > 0\n",
    "and substr(msg.ctrequestuuid, 1, length('{digits}')) = '{digits}'\n",
    "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= {sample_percentage}\n",
    "),\n",
    "plangen as (\n",
    "select distinct\n",
    "msg.scanuuid as plangen_uuid, \n",
    "p.uuid as job_uuid,\n",
    "j.supplyuuid\n",
    "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
    "cross join unnest(msg.proposals) as job(j)\n",
    "cross join unnest(j.jobs) as plan(p)\n",
    "where datestr = '{datestr}'\n",
    "and msg.cityid = {city_id}\n",
    "and msg.flowtype = 'solo_batch'\n",
    "and msg.tenancy = 'uber/production'\n",
    "and j.status = 'eligible'\n",
    "),\n",
    "created as (\n",
    "select distinct\n",
    "    msg.jobuuid,\n",
    "    msg.timestamp\n",
    "from rawdata.kafka_hp_demand_job_created_nodedup\n",
    "where datestr = '{datestr}'\n",
    "and msg.region.id = {city_id}\n",
    "and msg.tenancy = 'uber/production'\n",
    "),\n",
    "mgv as (\n",
    "select distinct datestr,\n",
    "       msg.city_id,\n",
    "       msg.job_uuid,\n",
    "       msg.request_ts_ms,\n",
    "       msg.client_uuid,\n",
    "       msg.ct_request_uuid as plangen_uuid,\n",
    "       msg.supply_uuid,\n",
    "       msg.supply_plan_uuid as plan_uuid,\n",
    "       msg.unadjusted_eta as eta,\n",
    "       msg.adjustedeta,\n",
    "       msg.ranking_metric,\n",
    "       msg.job_surge,\n",
    "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
    "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
    "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
    "       msg.preferred_destination_adjustment,\n",
    "       msg.objective_value as of_value,\n",
    "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
    "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
    "where  datestr = '{datestr}'\n",
    "and    msg.city_id = {city_id}\n",
    "and    msg.tenancy = 'uber/production'\n",
    "and    msg.vehicle_view_id = {vvid} \n",
    "and    msg.flow_type = 'solo_batch'\n",
    "and    msg.job_uuid <> msg.client_uuid\n",
    "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
    "),\n",
    "mgv1 as (\n",
    "select distinct msg.job_uuid,\n",
    "       msg.supply_uuid,\n",
    "       max(msg.eventual_completion_probability) as continuation_cr\n",
    "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
    "where  datestr = '{datestr}'\n",
    "and    msg.city_id = {city_id}\n",
    "and    msg.environment = 'production'\n",
    "and    msg.vehicle_view_id = {vvid} \n",
    "and    msg.flow_type = 'solo_batch'\n",
    "and    msg.job_uuid <> msg.client_uuid\n",
    "group by 1,2\n",
    "),\n",
    "test as (\n",
    "select distinct\n",
    "    mgv.datestr,\n",
    "    mgv.city_id,\n",
    "    dispatch.scan_uuid,\n",
    "    mgv.plangen_uuid,\n",
    "    mgv.job_uuid,\n",
    "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
    "    dispatch.planactiontype,\n",
    "    mgv.supply_uuid,\n",
    "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
    "    mgv.eta,\n",
    "    mgv.adjustedeta,\n",
    "    mgv.ranking_metric,\n",
    "    mgv.d_proba,\n",
    "    mgv.r_proba,\n",
    "    mgv.s_proba,\n",
    "    mgv1.continuation_cr,\n",
    "    mgv.preferred_destination_adjustment,\n",
    "    mgv.of_value,\n",
    "    mgv.trip_length,\n",
    "    fare.est_rider_quoted_final_fare as fare,\n",
    "    mgv.job_surge \n",
    "from mgv\n",
    "join mgv1\n",
    "on mgv.job_uuid = mgv1.job_uuid\n",
    "and mgv.supply_uuid = mgv1.supply_uuid\n",
    "join created\n",
    "on mgv.job_uuid = created.jobuuid\n",
    "join plangen\n",
    "on mgv.plangen_uuid = plangen.plangen_uuid\n",
    "and mgv.job_uuid = plangen.job_uuid\n",
    "and mgv.supply_uuid = plangen.supplyuuid\n",
    "join dispatch\n",
    "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
    "and mgv.job_uuid = dispatch.job_uuid\n",
    "join dwh.fact_trip_fare fare \n",
    "on mgv.job_uuid = fare.trip_uuid\n",
    "and fare.datestr = '{datestr}'\n",
    "and fare.city_id = {city_id}\n",
    ")\n",
    "select * from test\n",
    "order by job_Uuid\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    hex_digits: str\n",
    "    city_id: int\n",
    "    vvid: str\n",
    "    datestr: str\n",
    "    sample_percentage: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.vvid}_{self.datestr}_segment{self.hex_digits}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, vvid=self.vvid, digits=self.hex_digits, datestr=self.datestr, sample_percentage=self.sample_percentage)\n",
    "        print(self.qry)\n",
    "        #print(abc)\n",
    "        #if self.city_id == 5:\n",
    "        #    print(\"------\")\n",
    "        #    print(self.qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, d = 0.5, r = 0.2, s = 0.01):\n",
    "    df = df[df['fare'].notnull()]\n",
    "    df = df[df['d_proba'].notnull()]\n",
    "    df = df[df['r_proba'].notnull()]\n",
    "    df = df[df['s_proba'].notnull()]\n",
    "    df = df[df['adjustedeta']<1500]\n",
    "    df['d_proba'] = df['d_proba'].fillna(d)\n",
    "    df['r_proba'] = df['r_proba'].fillna(r)\n",
    "    df['s_proba'] = df['s_proba'].fillna(s)\n",
    "    df['trip_length'][df['trip_length'] <= 100] = 100\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def compute_new_of(df, gamma = 1):\n",
    "    df['cr'] = (1 - df['d_proba']) * (1 - df['r_proba']) + df['d_proba'] * df['continuation_cr']\n",
    "    beta = 0.05\n",
    "    normalizingFactor = 80\n",
    "    # WCOF\n",
    "    df['new_of'] = - (1/df['job_surge']**beta + (1 - 1/(df['job_surge']**beta))*df['fare']/normalizingFactor) * df['cr'] * (1 - (df['adjustedeta']/1500))**gamma * np.abs(1500 - df['of_value']) / (1500 - df['of_value']) #df['fare'] *\n",
    "    # CROF\n",
    "    #df['new_of'] = -  df['cr'] * (1 - (df['adjustedeta']/1500))**gamma * np.abs(1500 - df['of_value']) / (1500 - df['of_value']) #df['fare'] *\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local solver\n",
    "def solve_dict(\n",
    "    scan: dict, \n",
    "    cost_col: str, \n",
    "    job_singleton: float = 1500,\n",
    "    infinity: float = 1000000\n",
    "):\n",
    "    job_list = list(set([k[0] for k in scan.keys()]))\n",
    "    job_idx = {j: i for i, j in enumerate(job_list)}\n",
    "    job_count = len(job_list)\n",
    "\n",
    "    supply_list = list(set([k[1] for k in scan.keys()]))\n",
    "    supply_idx = {s: i for i, s in enumerate(supply_list)}\n",
    "    supply_count = len(supply_list)\n",
    "    \n",
    "    utility = np.full((len(job_list), len(supply_list) + len(job_list)), infinity, dtype=np.float32)\n",
    "    for k in scan.keys():\n",
    "        jidx = job_idx[k[0]]\n",
    "        sidx = supply_idx[k[1]]\n",
    "        utility[jidx, sidx] = scan[k][cost_col]\n",
    "    for i in range(len(job_list)):\n",
    "        utility[i, supply_count + i] = job_singleton\n",
    "            \n",
    "    # solve\n",
    "    job_sol, supply_sol = linear_sum_assignment(utility)\n",
    "\n",
    "    result = set()\n",
    "    for jidx, sidx in zip(job_sol, supply_sol):\n",
    "        j = job_list[jidx]\n",
    "        if sidx >= supply_count:\n",
    "            result.add((j,))\n",
    "        else:\n",
    "            s = supply_list[sidx]\n",
    "            result.add((j, s))\n",
    "            \n",
    "    assert len(result) == len(job_list)\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScanMetrics:\n",
    "    total_jobs: int = 0.\n",
    "    total_eta: float = 0.\n",
    "    total_offer: float = 0.\n",
    "    total_ar: float = 0.\n",
    "    total_rc: float = 0.\n",
    "    total_cr: float = 0.\n",
    "    total_trip: float = 0.\n",
    "    total_gb: float = 0.\n",
    "    total_overwrite: int = 0.\n",
    "    \n",
    "    def __add__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        return ScanMetrics(\n",
    "            self.total_jobs + o.total_jobs,\n",
    "            self.total_eta + o.total_eta,\n",
    "            self.total_offer + o.total_offer,\n",
    "            self.total_ar + o.total_ar,\n",
    "            self.total_rc + o.total_rc,\n",
    "            self.total_trip + o.total_trip,\n",
    "            self.total_overwrite + o.total_overwrite,\n",
    "            self.total_gb + o.total_gb,\n",
    "            self.total_cr + o.total_cr\n",
    "        )\n",
    "    def __iadd__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        self.total_jobs += o.total_jobs\n",
    "        self.total_eta += o.total_eta\n",
    "        self.total_offer += o.total_offer\n",
    "        self.total_ar += o.total_ar\n",
    "        self.total_rc += o.total_rc\n",
    "        self.total_trip += o.total_trip\n",
    "        self.total_overwrite += o.total_overwrite\n",
    "        self.total_gb += o.total_gb\n",
    "        self.total_cr += o.total_cr\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_summary_dict(\n",
    "    scan_dict: Dict[str, Dict[str, Any]],\n",
    "    matching: set, \n",
    "    overwrite: int,\n",
    ") -> ScanMetrics:\n",
    "    sm = ScanMetrics()\n",
    "    sm.total_jobs = len(matching)\n",
    "    sm.total_overwrite = overwrite\n",
    "    \n",
    "    for m in matching:\n",
    "        if len(m) == 2:\n",
    "            row = scan_dict[(m[0], m[1])]\n",
    "            sm.total_offer += 1\n",
    "            sm.total_eta += row['adjustedeta']\n",
    "            sm.total_ar += 1 - row['d_proba']\n",
    "            sm.total_rc += row['r_proba']\n",
    "            sm.total_cr += row['cr']\n",
    "            if row['trip_length'] < 7200:\n",
    "                sm.total_trip += row['trip_length']\n",
    "            if row['fare'] > 0:\n",
    "                sm.total_gb += row['cr'] * row['fare']\n",
    "    return sm\n",
    "\n",
    "def solve_all_dict(df, solver: Callable[[dict], set]):\n",
    "    total_scans = dict(tuple(df.groupby('scan_uuid')))\n",
    "\n",
    "    sm = ScanMetrics()\n",
    "    for scan_uuid, scan_df in total_scans.items():\n",
    "        scan = (scan_df.set_index(['job_uuid', 'supply_uuid']).to_dict(orient='index'))\n",
    "        matching, overwrite = solver(scan)\n",
    "        sm += metric_summary_dict(scan, matching, overwrite)\n",
    "        \n",
    "    return {'total_jobs': sm.total_jobs,\n",
    "            'match_rate': sm.total_offer * 1.0 / sm.total_jobs,\n",
    "            'overwrite': sm.total_overwrite * 1.0 / sm.total_jobs,\n",
    "            'Average Matched ETA': sm.total_eta * 1.0 / sm.total_offer,\n",
    "            'Average CR': sm.total_cr * 1.0 / sm.total_offer,\n",
    "            'Driver AR': sm.total_ar * 1.0 / sm.total_offer,\n",
    "            'Rider cancel': sm.total_rc * 1.0 / sm.total_offer,\n",
    "            'Average trip length': sm.total_trip * 1.0 / sm.total_offer,\n",
    "            'Average GB': sm.total_gb * 1.0 / sm.total_offer,\n",
    "            'Total GB': sm.total_gb\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_matching_decision(m1,m2):\n",
    "    return m1.difference(m2), m2.difference(m1)\n",
    "\n",
    "def supply_cost_solve_dict(scan, markov = False, secondary_singleton = 0.0):\n",
    "    primary_matching = solve_dict(scan, 'of_value', job_singleton = 1500)\n",
    "    if markov:      \n",
    "        return primary_matching, 0\n",
    "    secondary_matching = solve_dict(scan, 'new_of', job_singleton = secondary_singleton)\n",
    "    different_matches = len(different_matching_decision(primary_matching, secondary_matching)[0])\n",
    "    return secondary_matching, different_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict(d):\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['jobs'] = [d['total_jobs']]\n",
    "    tmp_df['match_rate'] = [d['match_rate']]\n",
    "    tmp_df['overwrite'] = [d['overwrite']]\n",
    "    tmp_df['ETA'] = [d['Average Matched ETA']]\n",
    "    tmp_df['AR'] = [d['Driver AR']]\n",
    "    tmp_df['RC'] = [d['Rider cancel']]\n",
    "    tmp_df['Average trip length'] = [d['Average trip length']]\n",
    "    tmp_df['Total GB'] = [d['Total GB']]\n",
    "    tmp_df['Average GB'] = [d['Average GB']]\n",
    "    tmp_df['Average CR'] = [d['Average CR']]\n",
    "    return tmp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2022 06:22:00 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n",
      "10/13/2022 06:22:00 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 458\n",
      "and msg.vehicleviewid = (3825) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 458\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-22'\n",
      "and msg.region.id = 458\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 458\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (3825) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 458\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (3825) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-22'\n",
      "and fare.city_id = 458\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 458\n",
      "and msg.vehicleviewid = (3825) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 458\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-23'\n",
      "and msg.region.id = 458\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 458\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (3825) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 458\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (3825) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-23'\n",
      "and fare.city_id = 458\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 90\n",
      "and msg.vehicleviewid = (651) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 90\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-22'\n",
      "and msg.region.id = 90\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 90\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (651) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 90\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (651) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-22'\n",
      "and fare.city_id = 90\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 90\n",
      "and msg.vehicleviewid = (651) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 90\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-23'\n",
      "and msg.region.id = 90\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 90\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (651) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 90\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (651) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-23'\n",
      "and fare.city_id = 90\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 148\n",
      "and msg.vehicleviewid = (3227) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-22'\n",
      "and msg.cityid = 148\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-22'\n",
      "and msg.region.id = 148\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 148\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (3227) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-22'\n",
      "and    msg.city_id = 148\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (3227) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-22'\n",
      "and fare.city_id = 148\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "\n",
      "with dispatch as (select distinct\n",
      "    datestr,\n",
      "    msg.cityid,\n",
      "    msg.ctplangenrequestuuid as plangen_uuid,\n",
      "    msg.ctrequestuuid as scan_uuid,\n",
      "    j as job_uuid,\n",
      "    msg.supplyuuid,\n",
      "    msg.planactiontype\n",
      "from rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
      "cross join unnest(msg.jobuuid) jobs(j)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 148\n",
      "and msg.vehicleviewid = (3227) \n",
      "and msg.tenancy = 'uber/production'\n",
      "and CARDINALITY(msg.jobuuid) > 0\n",
      "and substr(msg.ctrequestuuid, 1, length('0')) = '0'\n",
      "and abs(mod(from_big_endian_64(xxhash64(CAST(msg.ctrequestuuid AS varbinary))), 100)) <= 1\n",
      "),\n",
      "plangen as (\n",
      "select distinct\n",
      "msg.scanuuid as plangen_uuid, \n",
      "p.uuid as job_uuid,\n",
      "j.supplyuuid\n",
      "from rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
      "cross join unnest(msg.proposals) as job(j)\n",
      "cross join unnest(j.jobs) as plan(p)\n",
      "where datestr = '2022-09-23'\n",
      "and msg.cityid = 148\n",
      "and msg.flowtype = 'solo_batch'\n",
      "and msg.tenancy = 'uber/production'\n",
      "and j.status = 'eligible'\n",
      "),\n",
      "created as (\n",
      "select distinct\n",
      "    msg.jobuuid,\n",
      "    msg.timestamp\n",
      "from rawdata.kafka_hp_demand_job_created_nodedup\n",
      "where datestr = '2022-09-23'\n",
      "and msg.region.id = 148\n",
      "and msg.tenancy = 'uber/production'\n",
      "),\n",
      "mgv as (\n",
      "select distinct datestr,\n",
      "       msg.city_id,\n",
      "       msg.job_uuid,\n",
      "       msg.request_ts_ms,\n",
      "       msg.client_uuid,\n",
      "       msg.ct_request_uuid as plangen_uuid,\n",
      "       msg.supply_uuid,\n",
      "       msg.supply_plan_uuid as plan_uuid,\n",
      "       msg.unadjusted_eta as eta,\n",
      "       msg.adjustedeta,\n",
      "       msg.ranking_metric,\n",
      "       msg.job_surge,\n",
      "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
      "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
      "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
      "       msg.preferred_destination_adjustment,\n",
      "       msg.objective_value as of_value,\n",
      "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 148\n",
      "and    msg.tenancy = 'uber/production'\n",
      "and    msg.vehicle_view_id = (3227) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "and    msg.calculator_type in ('markov_eta_v2', 'compound_completion_rate_of')\n",
      "),\n",
      "mgv1 as (\n",
      "select distinct msg.job_uuid,\n",
      "       msg.supply_uuid,\n",
      "       max(msg.eventual_completion_probability) as continuation_cr\n",
      "from   rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
      "where  datestr = '2022-09-23'\n",
      "and    msg.city_id = 148\n",
      "and    msg.environment = 'production'\n",
      "and    msg.vehicle_view_id = (3227) \n",
      "and    msg.flow_type = 'solo_batch'\n",
      "and    msg.job_uuid <> msg.client_uuid\n",
      "group by 1,2\n",
      "),\n",
      "test as (\n",
      "select distinct\n",
      "    mgv.datestr,\n",
      "    mgv.city_id,\n",
      "    dispatch.scan_uuid,\n",
      "    mgv.plangen_uuid,\n",
      "    mgv.job_uuid,\n",
      "    (mgv.request_ts_ms - created.timestamp)/1000 as time_since_creation,\n",
      "    dispatch.planactiontype,\n",
      "    mgv.supply_uuid,\n",
      "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
      "    mgv.eta,\n",
      "    mgv.adjustedeta,\n",
      "    mgv.ranking_metric,\n",
      "    mgv.d_proba,\n",
      "    mgv.r_proba,\n",
      "    mgv.s_proba,\n",
      "    mgv1.continuation_cr,\n",
      "    mgv.preferred_destination_adjustment,\n",
      "    mgv.of_value,\n",
      "    mgv.trip_length,\n",
      "    fare.est_rider_quoted_final_fare as fare,\n",
      "    mgv.job_surge \n",
      "from mgv\n",
      "join mgv1\n",
      "on mgv.job_uuid = mgv1.job_uuid\n",
      "and mgv.supply_uuid = mgv1.supply_uuid\n",
      "join created\n",
      "on mgv.job_uuid = created.jobuuid\n",
      "join plangen\n",
      "on mgv.plangen_uuid = plangen.plangen_uuid\n",
      "and mgv.job_uuid = plangen.job_uuid\n",
      "and mgv.supply_uuid = plangen.supplyuuid\n",
      "join dispatch\n",
      "on mgv.plangen_uuid = dispatch.plangen_uuid\n",
      "and mgv.job_uuid = dispatch.job_uuid\n",
      "join dwh.fact_trip_fare fare \n",
      "on mgv.job_uuid = fare.trip_uuid\n",
      "and fare.datestr = '2022-09-23'\n",
      "and fare.city_id = 148\n",
      ")\n",
      "select * from test\n",
      "order by job_Uuid\n",
      "\n",
      "\n",
      "Loaded 0/6 dataframes from cache!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/13/2022 06:22:00 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n",
      "10/13/2022 06:22:01 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n",
      "10/13/2022 06:22:01 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n",
      "10/13/2022 06:22:01 PM Send empty tier_metadata {} to queryrunner. Query is default to tier 5.\n"
     ]
    }
   ],
   "source": [
    "prefix = 'replay'\n",
    "hex_digits = '0'\n",
    "\n",
    "city_id_vvids = {#14: '(940)',\n",
    "                 #5: '(39)',\n",
    "                 #18: '(285)',\n",
    "                 458: '(3825)',\n",
    "                 #7: '(180)',\n",
    "                 #12: '(125)',\n",
    "                 90: '(651)',\n",
    "                 #39: '(539)',\n",
    "                 #143: '(9697)',\n",
    "                 #16: '(7423)',\n",
    "                 148: '(3227)',\n",
    "                 #531: '(1964)',\n",
    "                 #214: '(1720)',\n",
    "                 #1542: '(10004302)',\n",
    "                }\n",
    "\n",
    "#city_id_vvids = {\n",
    "#                 458: '(3825)',\n",
    "#                }\n",
    "\n",
    "datestrs = [  # 1 week\n",
    "   '2022-09-22', '2022-09-23'\n",
    "]\n",
    "\n",
    "sample_percentage = 1\n",
    "\n",
    "queries = [\n",
    "    Query(prefix=prefix, hex_digits=hex_digits, city_id=city_id, vvid=vvid, datestr=datestr, sample_percentage=sample_percentage)\n",
    "    for (city_id, vvid), datestr in itertools.product(city_id_vvids.items(), datestrs)\n",
    "]\n",
    "\n",
    "cache_qry_map = {\n",
    "    q.name: q.qry \n",
    "    for q in queries\n",
    "}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='dca1',\n",
    "    datasource='presto-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clean data\n",
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True)\n",
    "scans.shape\n",
    "df = scans\n",
    "df = clean_df(df)\n",
    "df = compute_new_of(df, gamma = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df['job_uuid']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['city_id', 'datestr']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['job_uuid', 'supply_uuid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = False))\n",
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = True))\n",
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# clean data\n",
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True)\n",
    "scans.shape\n",
    "df = scans\n",
    "df = clean_df(df)\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "gamma_list = [0.1 * i for i in range(101)]\n",
    "\n",
    "#sca\n",
    "for gamma in gamma_list:\n",
    "    df = compute_new_of(df, gamma = gamma)\n",
    "    df.drop_duplicates(subset=['job_uuid', 'supply_uuid'], inplace=True)\n",
    "    matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = False))\n",
    "    tmp_df = convert_dict(matching)\n",
    "    tmp_df['gamma'] = [gamma]\n",
    "    results_df = results_df.append(tmp_df, ignore_index = True)\n",
    "        \n",
    "#markov\n",
    "matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = True))\n",
    "tmp_df = convert_dict(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('wcof_lsr_global_9_22_2022_9_23_2022_latam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#markov\n",
    "matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = True))\n",
    "tmp_df = convert_dict(matching)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
