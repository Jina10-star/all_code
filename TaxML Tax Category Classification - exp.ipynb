{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa18879",
   "metadata": {},
   "source": [
    "# <h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import Requirements\" data-toc-modified-id=\"Import-Requirements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Requirements</a></span></li><li><span><a href=\"#Prepare Training Data\" data-toc-modified-id=\"Prepare-Training-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare Training Data</a></span><ul class=\"toc-item\"></ul></li><li><span><a href=\"#Model Training\" data-toc-modified-id=\"Model Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Model Saving\" data-toc-modified-id=\"Model Saving-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Saving</a></span><ul class=\"toc-item\"></ul></li><li><span><a href=\"#Validation and Results\" data-toc-modified-id=\"Validation and Results-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Validation and Results</a></span><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe6d0d",
   "metadata": {},
   "source": [
    "<a id='Import Requirements'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971b37a",
   "metadata": {},
   "source": [
    "# Import Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca994c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f4b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.naive_bayes import MultinomialNB\n",
    "from river.feature_extraction import BagOfWords,TFIDF\n",
    "\n",
    "# Pipeline\n",
    "from river.compose import Pipeline \n",
    "\n",
    "# Metrics\n",
    "#from river.metrics import Classification_Report,Accuracy\n",
    "from river import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed789023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/online-ml/river --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6586d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from creme import compose\n",
    "#from creme import naive_bayes\n",
    "#from creme import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed6f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from creme import metrics\n",
    "#from creme import preprocessing\n",
    "pipe_mnb=Pipeline(('vect',BagOfWords(lowercase=True)),\n",
    "            ('TFIDF', TFIDF()),\n",
    "            #('mnb', MultinomialNB(alpha= 0.05,fit_prior= False))])\n",
    "            #('clf', RandomForestClassifier())])\n",
    "            ('mnb',MultinomialNB(alpha=1)))\n",
    "model = Pipeline(BagOfWords(),MultinomialNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832babd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_cicd_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3f7329412273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_cicd_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target_new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_cicd_final' is not defined"
     ]
    }
   ],
   "source": [
    "df=data_cicd_final[['processed_text','target_new']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85574f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e715e6d510f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to Tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to Tuple\n",
    "docs=df.to_records(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10798351",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42198a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, label in docs:\n",
    "    pipe_nb = pipe_mnb.learn_one(sentence,label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_one(\"text to predict category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4230c",
   "metadata": {},
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0d0ec",
   "metadata": {},
   "source": [
    "Input data for training consists of both historical data and CICD data( Production run data for which manual agent validation has been done for the ML prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(message):\n",
    "\n",
    "    #stopwords\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    message=message.lower()\n",
    "    message = re.sub(r'-','', message)\n",
    "    #removing the numerical values and working only with text values\n",
    "    message = re.sub('[^a-zA-Z]', \" \", message)\n",
    "    message = re.sub(r'\\\\n','', message)\n",
    "    #lowering and removing punctuation\n",
    "    message = re.sub(r'[^\\w\\s]',' ',message)\n",
    "    message = re.sub(r'\\s+[a-zA-Z]\\s+',' ', message)\n",
    "    #removing the stopwords\n",
    "    message = ' '.join([word for word in message.split() if word not in stpwrd])\n",
    "    #lemmatizing the text\n",
    "    message =  \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation])\n",
    "    #removing hyperlinks\n",
    "    message = re.sub(r'http\\S+', ' ', message)\n",
    "    #removing special characters\n",
    "    message=message.replace('<','').replace('>','').replace('*','').replace('[','').replace(']','').replace('|','')\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inca1=pd.read_csv('../data/retrain_data/INDIRECTTX-1954-03-15.csv', usecols = ['entity_uuid','product_name','product_description','establishment_type','CAT NAME,CAT_TEMP','Integers'])\n",
    "data_inca2=pd.read_csv('../data/retrain_data/INDIRECTTX-1954-02-15.csv', usecols = ['entity_uuid','product_name','product_description','establishment_type','CAT NAME,CAT_TEMP','Integers'])\n",
    "data_inca=pd.concat([data_inca1,data_inca2])\n",
    "data_inca['target_new']=data_inca['CAT NAME,CAT_TEMP']+\":\"+ data_inca['Integers']\n",
    "data_inca.drop(['CAT NAME,CAT_TEMP', 'Integers'],inplace=True,axis=1)\n",
    "data_inca=data_inca.rename(columns={'product_name': 'Item', 'product_description': 'Description','entity_uuid':'UniqueUUID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a09841",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inca.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "data_df = pd.read_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/historical_data_24_11_22.csv', encoding='utf8',engine='python',usecols=['UniqueUUID','Item','Description','establishment_type','target_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from cicd data into dataframe\n",
    "data_cicd=pd.read_csv('../data/retrain_data/TaxML-CICD - Prod_Data_latest.csv', usecols = ['UniqueUUID','Item','Description','establishment_type','Confidence Score','Agent Corrected CAT Name', 'Agent Corrected Integer','CAT NAME_ ValidationScore [0-100]','Integer_ValidationScore[0-100]'])\n",
    "print(data_cicd.shape)\n",
    "#misclassified data                                        \n",
    "data_cicd_misclassification=data_cicd[(data_cicd['CAT NAME_ ValidationScore [0-100]']==0)| (data_cicd['Integer_ValidationScore[0-100]']==0)]\n",
    "data_cicd_latest=data_cicd_misclassification[['UniqueUUID','Item','Description','establishment_type','Agent Corrected CAT Name', 'Agent Corrected Integer']]\n",
    "data_cicd_latest['target_new']=data_cicd_latest['Agent Corrected CAT Name'] + \":\" + data_cicd_latest['Agent Corrected Integer']\n",
    "data_cicd_latest.drop(['Agent Corrected CAT Name', 'Agent Corrected Integer'],inplace=True,axis=1)\n",
    "print(data_cicd_latest.info())\n",
    "data_cicd_final = pd.concat([data_inca, data_cicd_latest], join=\"outer\")\n",
    "final_data=pd.concat([data_df, data_cicd_final], join=\"outer\")\n",
    "final_data.to_csv('training_data.csv',index=False)\n",
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "data_cicd_final['combined_text'] = data_cicd_final[['Item','Description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_cicd_final['processed_text']= data_cicd_final['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "\n",
    "data_cicd_final = data_cicd_final.reset_index(drop=True)\n",
    "# prepare the target column by combining 'Agent Corrected CAT Name' and 'Agent Corrected Integer'\n",
    "\n",
    "data_cicd_final=data_cicd_final.drop_duplicates(subset=['processed_text','target_new'],keep='first')\n",
    "print(data_cicd_final.shape)\n",
    "\n",
    "#remove rows having empty target column\n",
    "data_cicd_final.dropna(subset=['target_new'],inplace=True)\n",
    "\n",
    "\n",
    "X_cicd= data_cicd_final[['Item','Description','establishment_type','processed_text']]\n",
    "y_cicd= data_cicd_final['target_new']\n",
    "\n",
    "# split the cicd data into train and test \n",
    "X_train_cicd, X_test_cicd, y_train_cicd, y_test_cicd = train_test_split(X_cicd, y_cicd,shuffle=True, test_size = .01, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3211e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergedStuff_cicd= data_df.set_index('UniqueUUID').join(data_cicd_latest.set_index('UniqueUUID'))\n",
    "#final_data=pd.concat([data_df, data_cicd_final], join=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa585e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af554b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "data_df['combined_text'] = data_df[['Item','Description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_df['processed_text']= data_df['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0002a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=data_df.drop_duplicates(subset=['processed_text','target_new'],keep='first')\n",
    "print(data_df.shape)\n",
    "#remove rows having empty target column\n",
    "data_df.dropna(subset=['target_new'],inplace=True)\n",
    "\n",
    "X= data_df[['Item','Description','establishment_type','processed_text']]\n",
    "y= data_df['target_new']\n",
    "\n",
    "# split the cicd data into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,shuffle=True, test_size = .20, random_state = 42)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a27ed",
   "metadata": {},
   "source": [
    "We will append the CICD data to the historical data to create the final train and test data.\n",
    "Train set has 80% of all historical data and 90% of all cicd data.\n",
    "Test set consists of 20% of historic data and 10% of all cicd data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceeea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.append(X_train_cicd)\n",
    "X_test_final = X_test.append(X_test_cicd)\n",
    "y_train_final = y_train.append(y_train_cicd)\n",
    "y_test_final = y_test.append(y_test_cicd)\n",
    "\n",
    "#X_train_final = X_train\n",
    "#X_test_final = X_test\n",
    "#y_train_final = y_train\n",
    "#y_test_final = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f453",
   "metadata": {},
   "source": [
    "<a id='Model Training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd540e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data size: {}'.format(len(X_train_final)))\n",
    "print('Test data size: {}'.format(len(X_test_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique labels in train data: {}'.format(len(y_train_final.unique().tolist())))\n",
    "print('Number of unique labels in test data: {}'.format(len(y_test_final.unique().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f664ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = data_df.groupby(['target_new'],sort=False).agg({'target_new':'count'})\n",
    "category_count.rename(columns={'target_new':'count'},inplace=True)\n",
    "category_count.sort_values('count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count.to_csv('category_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6d97a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75af4ab",
   "metadata": {},
   "source": [
    "The Model Pipeline consists of 1. CountVectorizer, 2. Tfidf-Transformer 3. MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(strip_accents='ascii',token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', stop_words='english', max_df=0.85)\n",
    "#X = vectorizer.fit_transform(X_train_final['processed_text'].values)\n",
    "#features = vectorizer.get_feature_names()\n",
    "#len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa506f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a result dataframe to store final results\n",
    "result=X_test_final\n",
    "\n",
    "#create the model pipeline\n",
    "rf = Pipeline([('vect', CountVectorizer(strip_accents='ascii',max_df=0.85)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            #('mnb', MultinomialNB(alpha= 0.05,fit_prior= False))])\n",
    "            #('clf', RandomForestClassifier())])\n",
    "            ('svc',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66314de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform model training\n",
    "rf.fit(X_train_final['processed_text'].values, y_train_final.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a084c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end=time.time()\n",
    "interval=t_end-t_start\n",
    "time_minutes=time.strftime(\"%H:%M:%S\", time.gmtime(interval)\n",
    "print('Total model training time: {}'.format(time_minutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prediction\n",
    "result=X_test_final\n",
    "y_pred = rf.predict(X_test_final['processed_text'].values)\n",
    "\n",
    "result['original_cat']= y_test_final.values\n",
    "result['predicted_cat'] = y_pred\n",
    "#result['prediction_cat_confscore'] = np.round_(np.max(rf.predict_proba(X_test_final['processed_text']), axis=1), decimals=2)\n",
    "result['prediction_cat_confscore'] = np.round_(1/(1+(np.max(rf.decision_function(X_test_final['processed_text'].values), axis=1))),decimals=2)\n",
    "\n",
    "#\n",
    "output = {'accuracy': accuracy_score(y_pred,y_test_final),'precision_score':precision_score(y_pred,y_test_final,average='weighted'),'recall_score':recall_score(y_pred,y_test_final,average='weighted')\n",
    ",'f1_score':f1_score(y_pred,y_test_final,average='weighted')}\n",
    "\n",
    "result['confusion_matrix'] = str(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['confusion_matrix'] [5:6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([\"{'accuracy': 0.7481380408209677, 'precision_score': 0.8104281909828657, 'recall_score': 0.7481380408209677, 'f1_score': 0.772676580987015}\"],\n",
    "      #dtype=object)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f3209",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68dbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "  'mnb__alpha': [0.01,0.05,0.1,0.2,0.3],\n",
    "}\n",
    "clf = GridSearchCV(rf, grid_params,n_jobs=-1,verbose=1)\n",
    "clf.fit(X_train_final['processed_text'].values, y_train_final.values)\n",
    "print(\"Best Score: \", clf.best_score_)\n",
    "print(\"Best Params: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch to determine the value of C\n",
    "param_grid = {'svc__C':np.arange(0.01,100,10)}\n",
    "linearSVC = GridSearchCV(rf,param_grid,cv=2,return_train_score=True)\n",
    "linearSVC.fit(X_train_final['processed_text'].values, y_train_final.values)\n",
    "print(linearSVC.best_params_)\n",
    "#linearSVC.coef_\n",
    "#linearSVC.intercept_\n",
    "\n",
    "bestlinearSVC = linearSVC.best_estimator_\n",
    "bestlinearSVC.fit(X_train,y_train)\n",
    "bestlinearSVC.coef_ = bestlinearSVC.named_steps['SVC'].coef_\n",
    "bestlinearSVC.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report = metrics.classification_report(y_test_final, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4944c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display=pd.DataFrame(classification_report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.to_csv('classification_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa3c79",
   "metadata": {},
   "source": [
    "<a id='Model Saving'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dab90b",
   "metadata": {},
   "source": [
    "# Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import datetime\n",
    "# save the model to disk\n",
    "filename_primary= 'finalized_model.sav'\n",
    "model_dir_taxml='/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/model/latest'\n",
    "# save the model to disk\n",
    "filename_primary= 'finalized_model.sav'\n",
    "model_path = os.path.join(model_dir_taxml, filename_primary) \n",
    "print(model_path)\n",
    "pickle.dump(rf, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0273b",
   "metadata": {},
   "source": [
    "<a id='Validation and Results'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4c028",
   "metadata": {},
   "source": [
    "# Validation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score of the model\n",
    "accuracy = x.score(X_train_final['processed_text'].values, y_train_final)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c11bfb",
   "metadata": {},
   "source": [
    "# Regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "x=joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg=X_train_final['processed_text'][:70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6985a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reg=X_test_final['processed_text'][:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reg=pd.concat([X_train_reg,X_test_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb03bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reg=y_train_final[:70000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd57887",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_reg=y_test_final[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8113a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg=pd.concat([y_train_reg,y_test_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score of the model of regression\n",
    "accuracy = x.score(X_reg, y_reg)\n",
    "print(\"Accuracy = {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_df=pd.DataFrame()\n",
    "#accuracy_df['Accuracy']=[accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.to_csv('old_accuracy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d53df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_accuracy=pd.read_csv('./old_accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_accuracy['Accuracy'].item()>accuracy:\n",
    "    print('No need to update model')\n",
    "else:\n",
    "     print('Need to update model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e978a",
   "metadata": {},
   "source": [
    "# Saving the train and test data for reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_end = int(len(data_df)*train_size)\n",
    "df_train = data_df[:train_end]\n",
    "df_test = data_df[train_end:]\n",
    "train_size_cicd=0.02\n",
    "train_end_cicd = int(len(data_cicd_final)*train_size_cicd)\n",
    "df2_train = data_cicd_final[:train_end_cicd]\n",
    "df2_test = data_cicd_final[train_end_cicd:]\n",
    "df2_train = df2_train[['Item','Description','establishment_type','combined_text','processed_text','target_new']]\n",
    "df2_test = df2_test[['Item','Description','establishment_type','combined_text','processed_text','target_new']]\n",
    "X_train_save = df_train.append(df2_train)\n",
    "X_test_save = df_test.append(df2_test)\n",
    "X_train_save['label'] = 'train'\n",
    "X_test_save['label'] = 'test'\n",
    "X_data = X_train_save.append(X_test_save)\n",
    "X_data.to_csv('df_traintestdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the misclassifications\n",
    "misclassifications= result.loc[result['original_cat']!=result['predicted_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(misclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassifications.to_csv('misclassifications.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c21a4",
   "metadata": {},
   "source": [
    "# model tagging on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c919063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "x=joblib.load('finalized_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "df_1= pd.read_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/before/2023-01-16 - Weekly 711 New Items.csv', encoding='utf8',engine='python')\n",
    "#choose sample data from entire data\n",
    "df_1 = df_1.sample(frac=1, random_state=42)\n",
    "#fill blanks with ''\n",
    "df_1 = df_1.fillna('')\n",
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "df_1['combined_text'] = df_1[['item_name','description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "df_1['processed_text'] = df_1['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "print(df_1.shape)\n",
    "df_1 = df_1.reset_index(drop=True)\n",
    "y_pred = x.predict(df_1['processed_text'].values)\n",
    "df_1['target'] = y_pred\n",
    "df_1[['cat_name','cat_int']] = df_1['target'].str.split(':', expand=True)\n",
    "df_1.drop('target', inplace=True, axis=1)\n",
    "df_1['prediction_cat_confscore'] =np.round_(np.max(x.predict_proba(df_1['processed_text']), axis=1), decimals=2)\n",
    "df_1.to_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/after/2023-01-16 - Weekly 711 New Items_after_tagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "df_2= pd.read_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/before/2023-01-16 - Weekly New GroCo Menu Items.csv', encoding='utf8',engine='python')\n",
    "#choose sample data from entire data\n",
    "df_2 = df_2.sample(frac=1, random_state=42)\n",
    "#fill blanks with ''\n",
    "df_2 = df_2.fillna('')\n",
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "df_2['combined_text'] = df_2[['item_name','description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "df_2['processed_text'] = df_2['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "print(df_2.shape)\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "y_pred = x.predict(df_2['processed_text'].values)\n",
    "df_2['target'] = y_pred\n",
    "df_2[['cat_name','cat_int']] = df_2['target'].str.split(':', expand=True)\n",
    "df_2.drop('target', inplace=True, axis=1)\n",
    "df_2['prediction_cat_confscore'] =np.round_(np.max(x.predict_proba(df_2['processed_text']), axis=1), decimals=2)\n",
    "df_2.to_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/after/2023-01-16 - Weekly New GroCo Menu Items_after_tagging.csv')\n",
    "print(df_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b380a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "df_3= pd.read_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/before/2023-03-16 - INDIRECTTX-2220 - CVS - Sheet1.csv', encoding='latin-1',engine='python')\n",
    "#choose sample data from entire data\n",
    "df_3 = df_3.sample(frac=1, random_state=42)\n",
    "#fill blanks with ''\n",
    "df_3 = df_3.fillna('')\n",
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "df_3['combined_text'] = df_3[['item_name','description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "df_3['processed_text'] = df_3['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "print(df_3.shape)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "y_pred = x.predict(df_3['processed_text'].values)\n",
    "df_3['target'] = y_pred\n",
    "df_3[['cat_name','cat_int']] = df_3['target'].str.split(':', expand=True)\n",
    "df_3.drop('target', inplace=True, axis=1)\n",
    "df_3['prediction_cat_confscore'] =np.round_(np.max(x.predict_proba(df_3['processed_text']), axis=1), decimals=2)\n",
    "df_3.drop('processed_text', inplace=True, axis=1)\n",
    "df_3.drop('combined_text', inplace=True, axis=1)\n",
    "df_3.to_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/ml_tagged_data/after/2023-03-16 - INDIRECTTX-2220 - CVS.csv')\n",
    "print(df_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "df_3= pd.read_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/bf.csv', encoding='latin-1',engine='python')\n",
    "#choose sample data from entire data\n",
    "df_3 = df_3.sample(frac=1, random_state=42)\n",
    "#fill blanks with ''\n",
    "df_3 = df_3.fillna('')\n",
    "# combine the columns Item, Description and establishment_type into one column 'combined_text'\n",
    "df_3['combined_text'] = df_3[['Item','Description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "df_3['processed_text'] = df_3['combined_text'].map(lambda s:preprocess_text(s)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97455808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d629df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_3.shape)\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "y_pred = x.predict(df_3['processed_text'].values)\n",
    "df_3['target'] = y_pred\n",
    "df_3[['ML CAT Name','ML Integer']] = df_3['target'].str.split(':', expand=True)\n",
    "df_3.drop('target', inplace=True, axis=1)\n",
    "df_3['ML Confidence Score'] =np.round_(np.max(x.predict_proba(df_3['processed_text']), axis=1), decimals=2)\n",
    "df_3.drop('processed_text', inplace=True, axis=1)\n",
    "df_3.drop('combined_text', inplace=True, axis=1)\n",
    "df_3.to_csv('/Users/jghosh2/Documents/my-notebook/Tax_ml_poc/data/bf.csv')\n",
    "print(df_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fba8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
