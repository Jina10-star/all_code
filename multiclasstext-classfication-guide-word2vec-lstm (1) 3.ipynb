{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89acce557b54ba7cb0ebf85fb3bf22d9e95868a6"
   },
   "source": [
    "### IMPORTING THE MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "739312197e54166212116baab4af1d81fb4e9ccb",
    "id": "u4Qoy1EB5hmJ",
    "outputId": "9fc57d33-b6cc-45c1-c4ec-24394bb61544"
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cff40d5497523fa8a1815323f6684c66fc1d008e"
   },
   "source": [
    "### LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1abe90648e9f876e84a5ca74717e373d9877c9ab",
    "id": "PxRv4O4w6ELq"
   },
   "outputs": [],
   "source": [
    "#rev_frame=pd.read_csv(r'../input/Reviews.csv')\n",
    "df = pd.read_csv('Taxml_Integers_Verified.csv', index_col=None, header=0,dtype={'item_name': str, 'description': str,'establishment_type': str, 'CAT_Name':str},usecols=['item_name','description','establishment_type','CAT_Name'])\n",
    "#df_duplicates = df_integers[df_integers.duplicated(subset=['item_name','description','establishment_type'],keep=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords=['grocery']\n",
    "stpwrd = nltk.corpus.stopwords.words('english')\n",
    "stpwrd.extend(new_stopwords)\n",
    "le=WordNetLemmatizer()\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stpwrd]\n",
    "    lemma_words=[le.lemmatize(w) for w in filtered_words]\n",
    "    final_text=\" \".join([w for w in lemma_words])\n",
    "    return final_text\n",
    "def cat_name_sorted(text):\n",
    "    text=sorted(text.split(','))\n",
    "    text=','.join([x for x in text])\n",
    "    return text\n",
    "def cat_name_without_temp(text):\n",
    "    if ',TEMP_HEATED' in text:\n",
    "        text=re.sub(',TEMP_HEATED','', text)\n",
    "    if ',TEMP_UNHEATED' in text:\n",
    "        text=re.sub(',TEMP_UNHEATED','', text)\n",
    "    if ',TEMP_COLD' in text:\n",
    "        text=re.sub(',TEMP_COLD','', text)\n",
    "    if 'TEMP_HEATED' in text:\n",
    "        text=re.sub('TEMP_HEATED','', text)\n",
    "    if 'TEMP_UNHEATED' in text:\n",
    "        text=re.sub('TEMP_UNHEATED','', text)\n",
    "    if 'TEMP_COLD' in text:\n",
    "        text=re.sub('TEMP_COLD','', text)\n",
    "    else:\n",
    "        pass\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>description</th>\n",
       "      <th>establishment_type</th>\n",
       "      <th>CAT_Name</th>\n",
       "      <th>input_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokuyo Tororo Kombu</td>\n",
       "      <td>1.1 oz. 33 grams. 1.2 ounces.</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_PREPACKAGED_FOOD</td>\n",
       "      <td>tokuyo tororo kombu oz grams ounces grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rainbow Ice</td>\n",
       "      <td>\\N</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_CANDY</td>\n",
       "      <td>rainbow ice grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Master of Mixes Simple Syrup 375 ml</td>\n",
       "      <td>An essential ingredient in many of the most cl...</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_PREPACKAGED_FOOD</td>\n",
       "      <td>master mies simple syrup ml essential ingredie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bagel with Butter Breakfast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPECIALITY_STORE</td>\n",
       "      <td>CAT_PREPARED_FOOD</td>\n",
       "      <td>bagel butter breakfast speciality_store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 Copas Tequila Anejo, 750 ml (40% ABV)</td>\n",
       "      <td>\\N</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_LIQUOR</td>\n",
       "      <td>copas tequila anejo ml abv grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9535</th>\n",
       "      <td>Peach Bellini®</td>\n",
       "      <td>These all-natural gummies are filled with juic...</td>\n",
       "      <td>SPECIALITY_STORE</td>\n",
       "      <td>CAT_CONFECTIONARY</td>\n",
       "      <td>peach bellini allnatural gummies filled juicy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>E&amp;J BRANDY GRAND BLUE VSOP</td>\n",
       "      <td>750 ML</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_LIQUOR</td>\n",
       "      <td>ej brandy grand blue vsop ml grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>Coors Light, 18pk-12oz can beer (4.2% ABV)\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_BEER</td>\n",
       "      <td>coors light pkoz beer abv grocery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>Patron Anejo Tequila 375mL(40.0%ABV)</td>\n",
       "      <td>A distinctly barrel-aged spirit, Patrón Añejo ...</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_LIQUOR</td>\n",
       "      <td>patron anejo tequila ml abv distinctly barrela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>Beef Steak Burrito</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>CAT_PREPACKAGED_FOOD</td>\n",
       "      <td>beef steak burrito grocery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         item_name  \\\n",
       "0                              Tokuyo Tororo Kombu   \n",
       "1                                      Rainbow Ice   \n",
       "2              Master of Mixes Simple Syrup 375 ml   \n",
       "3                      Bagel with Butter Breakfast   \n",
       "4          4 Copas Tequila Anejo, 750 ml (40% ABV)   \n",
       "...                                            ...   \n",
       "9535                               Peach Bellini®    \n",
       "9536                    E&J BRANDY GRAND BLUE VSOP   \n",
       "9537  Coors Light, 18pk-12oz can beer (4.2% ABV)\\n   \n",
       "9538          Patron Anejo Tequila 375mL(40.0%ABV)   \n",
       "9539                            Beef Steak Burrito   \n",
       "\n",
       "                                            description establishment_type  \\\n",
       "0                         1.1 oz. 33 grams. 1.2 ounces.            GROCERY   \n",
       "1                                                    \\N            GROCERY   \n",
       "2     An essential ingredient in many of the most cl...            GROCERY   \n",
       "3                                                   NaN   SPECIALITY_STORE   \n",
       "4                                                    \\N            GROCERY   \n",
       "...                                                 ...                ...   \n",
       "9535  These all-natural gummies are filled with juic...   SPECIALITY_STORE   \n",
       "9536                                             750 ML            GROCERY   \n",
       "9537                                                NaN            GROCERY   \n",
       "9538  A distinctly barrel-aged spirit, Patrón Añejo ...            GROCERY   \n",
       "9539                                                NaN            GROCERY   \n",
       "\n",
       "                  CAT_Name                                          input_str  \n",
       "0     CAT_PREPACKAGED_FOOD        tokuyo tororo kombu oz grams ounces grocery  \n",
       "1                CAT_CANDY                                rainbow ice grocery  \n",
       "2     CAT_PREPACKAGED_FOOD  master mies simple syrup ml essential ingredie...  \n",
       "3        CAT_PREPARED_FOOD            bagel butter breakfast speciality_store  \n",
       "4               CAT_LIQUOR                 copas tequila anejo ml abv grocery  \n",
       "...                    ...                                                ...  \n",
       "9535     CAT_CONFECTIONARY  peach bellini allnatural gummies filled juicy ...  \n",
       "9536            CAT_LIQUOR               ej brandy grand blue vsop ml grocery  \n",
       "9537              CAT_BEER                  coors light pkoz beer abv grocery  \n",
       "9538            CAT_LIQUOR  patron anejo tequila ml abv distinctly barrela...  \n",
       "9539  CAT_PREPACKAGED_FOOD                         beef steak burrito grocery  \n",
       "\n",
       "[9540 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.sample(n=2000,random_state=42)\n",
    "#df1=df.sample(frac=1, random_state=42)\n",
    "df1['CAT_Name']=df1['CAT_Name'].progress_apply(lambda x: cat_name_sorted(x))\n",
    "df1['CAT_Name']=df1['CAT_Name'].progress_apply(lambda x: cat_name_without_temp(x))\n",
    "df1=df1.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df1 = df1.dropna(subset=['CAT_Name'])\n",
    "df1=df1.drop_duplicates(subset=['item_name','description','establishment_type','CAT_Name'],ignore_index=True,keep=False)\n",
    "df1['input_str'] = df1[['item_name', 'description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "df1['cleanText']=df1['input_str'].map(lambda s:preprocess(s)) \n",
    "df1=df1.drop_duplicates(subset=['CAT_Name','cleanText'],ignore_index=True,keep='first')\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['cat_count'] = df1.groupby('CAT_Name')['CAT_Name'].transform('count')\n",
    "df2=df1[df1['cat_count']<2].reset_index()\n",
    "df3=df1[df1['cat_count']>2].reset_index()\n",
    "X=df3[['item_name','description','establishment_type','cleanText']]\n",
    "Y=df3['CAT_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, y_train, y_val = train_test_split(df1.index.values, \n",
    "                                                  df1.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66f8562443f098c8507486bf3886d3a19dc582e6",
    "id": "hFLpZZSWEr_q"
   },
   "source": [
    "#### Let us now see if any of the column has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y.values)\n",
    "test_y = le.transform(test_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word.capitalize())\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_glove(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        drp = 0.1\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*4 , 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(64, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        *#rint(x.size())*\n",
    "        h_embedding = self.embedding(x)\n",
    "        *#_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))*\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6\n",
    "model = BiLSTM()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "model.cuda()\n",
    "\n",
    "# Load train and test in CUDA Memory\n",
    "x_train = torch.tensor(train_X, dtype=torch.long).cuda()\n",
    "y_train = torch.tensor(train_y, dtype=torch.long).cuda()\n",
    "x_cv = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "y_cv = torch.tensor(test_y, dtype=torch.long).cuda()\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # Set model to train configuration\n",
    "    model.train()\n",
    "    avg_loss = 0.  \n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Predict/Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "    # Set model to validation configuration -Doesn't get trained here\n",
    "    model.eval()        \n",
    "    avg_val_loss = 0.\n",
    "    val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        # keep/store predictions\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
    "\n",
    "    # Check Accuracy\n",
    "    val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
