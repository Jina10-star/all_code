{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** \n",
    "Initialize the configuration \n",
    "*/\n",
    "%%configure -f\n",
    "{\n",
    "  \"kind\": \"spark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_route_analytics\", \n",
    "  \"numExecutors\": 400, \n",
    "  \"executorCores\": 1, \n",
    "  \"driverCores\": 4,\n",
    "  \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.executor.memoryOverhead\": 3072, \n",
    "    \"spark.locality.wait\": \"0\",\n",
    "    \"spark.default.parallelism\":10000\n",
    "  },\n",
    "  \"executorMemory\": \"24g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"phx2/Secure\",\n",
    "    \"X-Drogon-Auth-HDFS-DT\": \"MgAMZGhydXZlbi52b3JhDGRocnV2ZW4udm9yYQCKAYIYec6zigGCPIZSs4w4RLhujgaSFJ9eA_I2f4OlQS1wfHF6EcZDvoIDEldFQkhERlMgZGVsZWdhdGlvbhExMC44MC42Ni4xMzU6ODAyMA\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * This section defines all the objects will be used in the following algorithm.\n",
    " */\n",
    "case class Segment (\n",
    "    segment_uuid: String,\n",
    "    start_junction_uuid: String,\n",
    "    end_junction_uuid: String\n",
    ")\n",
    "\n",
    "case class Location (\n",
    "    latitude: Double,\n",
    "    longitude: Double\n",
    ")\n",
    "\n",
    "case class SegmentTraversalCount (\n",
    "    segment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class NavRouteDivergence (\n",
    "    trip_id: List[String],\n",
    "    pre_div_segment: Segment,\n",
    "    div_segment: Segment,\n",
    "    post_div_suggested_segment: Segment,\n",
    "    post_div_traversed_segment: Segment\n",
    ")\n",
    "\n",
    "case class NavRouteDivergenceCount (\n",
    "    preDivSegment: Segment,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int,\n",
    "    sampleTrips: List[String]\n",
    ")\n",
    "\n",
    "case class TransitionTraversalCount (\n",
    "    firstSegment: Segment,\n",
    "    lastSegment: Segment,\n",
    "    viaSegment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class TransitionDivergenceFeature (\n",
    "    actualTransition: TransitionTraversalCount,\n",
    "    suggestedTransition: TransitionTraversalCount,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int\n",
    ")\n",
    "\n",
    "case class TransitionTraversalCountPair (\n",
    "    actualTransition: TransitionTraversalCount,\n",
    "    suggestedTransition: TransitionTraversalCount\n",
    ")\n",
    "\n",
    "case class TurnRestrictionFeature (\n",
    "    segmentIds: List[String],\n",
    "    actualTraversalsCountOnTransition: Int,\n",
    "    suggestedTraversalsCountOnTransition: Int,\n",
    "    actualTraversalsCountOnSuggestedSegment: Int,\n",
    "    sampleTripsUuids: List[String]\n",
    ")\n",
    "\n",
    "case class UMMIssue (\n",
    "    issueuuid: String,\n",
    "    ummbuilduuid: String,\n",
    "    latitude: Double,\n",
    "    longitude: Double,\n",
    "    sampletripuuids: List[String],\n",
    "    featureuuids: List[String],\n",
    "    numberoftrips: Int,\n",
    "    cityid: Int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Input params for the algorithm\n",
    " */\n",
    "val startDate= \"2022-10-16\"\n",
    "val endDate = \"2022-10-22\"\n",
    "val umm_version = \"9cfbd494-5212-11ed-9455-5c6f6910eaea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Load divergences from maps_intel.navigation_route_divergence\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object NavRouteDivergenceLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadDivergences(utcFromDateStr: String, utcToDateStr: String, cityIds: Array[Int]): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | pre_divergence_traversed_segments,\n",
    "         | divergence_segment,\n",
    "         | post_divergence_suggested_segment,\n",
    "         | post_divergence_suggested_segments,\n",
    "         | post_divergence_traversed_segment,\n",
    "         | count(*) as observations,\n",
    "         | slice(collect_set(distinct job_uuid),1,5) as sample_trips_uuids \n",
    "         | from maps_intel.navigation_route_divergence\n",
    "         | where datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | AND divergence_type = 'validDivergenceFound'\n",
    "         | AND pre_divergence_traversed_segments is not null\n",
    "         | and divergence_segment is not null \n",
    "         | and post_divergence_suggested_segment is not null \n",
    "         | and post_divergence_suggested_segments is not null \n",
    "         | and post_divergence_traversed_segment is not null \n",
    "         | AND divergence_segment.segment_uuid != post_divergence_suggested_segment.segment_uuid\n",
    "         | AND divergence_segment.segment_uuid != post_divergence_traversed_segment.segment_uuid\n",
    "         | AND post_divergence_suggested_segment.segment_uuid != post_divergence_traversed_segment.segment_uuid\n",
    "         | and lineofbusiness in ('rides')\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | GROUP BY 1,2,3,4,5\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    if (!cityIds.isEmpty) {\n",
    "      query = query + s\"\"\" and city_id in (${cityIds.mkString(\",\")})\"\"\"\n",
    "    }\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[NavRouteDivergence] = {\n",
    "\n",
    "    rawDataset.filter(r => {\n",
    "        var preDivSegments = ListBuffer[Segment]()\n",
    "        \n",
    "        r.getAs[Seq[Any]](\"pre_divergence_traversed_segments\").filter(row => row != null).foreach(row => {\n",
    "            val segmentInfo = row.asInstanceOf[Row]\n",
    "            if(segmentInfo.getAs[String](\"segment_uuid\") != null && \n",
    "               segmentInfo.getAs[String](\"start_junction_uuid\") != null &&\n",
    "               segmentInfo.getAs[String](\"end_junction_uuid\") != null) {\n",
    "                preDivSegments += Segment(segmentInfo.getAs[String](\"segment_uuid\"),\n",
    "                                             segmentInfo.getAs[String](\"start_junction_uuid\"),\n",
    "                                             segmentInfo.getAs[String](\"end_junction_uuid\")\n",
    "                                            )\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        !preDivSegments.isEmpty\n",
    "    }).map(r => {\n",
    "        var preDivSegments = ListBuffer[Segment]()\n",
    "        \n",
    "        r.getAs[Seq[Any]](\"pre_divergence_traversed_segments\").foreach(row => {\n",
    "            val segmentInfo = row.asInstanceOf[Row]\n",
    "            preDivSegments += Segment(segmentInfo.getAs[String](\"segment_uuid\"),\n",
    "                                        segmentInfo.getAs[String](\"start_junction_uuid\"),\n",
    "                                        segmentInfo.getAs[String](\"end_junction_uuid\")\n",
    "                                )\n",
    "        })\n",
    "        \n",
    "        var trips = ListBuffer[String]()\n",
    "        \n",
    "        r.getAs[Seq[String]](\"sample_trips_uuids\").foreach(row => trips += row)\n",
    "            \n",
    "        \n",
    "        NavRouteDivergence(\n",
    "            trip_id = trips.toList,\n",
    "            pre_div_segment = preDivSegments.toList.head,\n",
    "            div_segment = Segment(r.getAs[Row](\"divergence_segment\").getAs[String](\"segment_uuid\"),\n",
    "                                 r.getAs[Row](\"divergence_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                                 r.getAs[Row](\"divergence_segment\").getAs[String](\"end_junction_uuid\")),\n",
    "            post_div_suggested_segment = Segment(r.getAs[Row](\"post_divergence_suggested_segment\").getAs[String](\"segment_uuid\"),\n",
    "                                 r.getAs[Row](\"post_divergence_suggested_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                                 r.getAs[Row](\"post_divergence_suggested_segment\").getAs[String](\"end_junction_uuid\")),\n",
    "            post_div_traversed_segment = Segment(r.getAs[Row](\"post_divergence_traversed_segment\").getAs[String](\"segment_uuid\"),\n",
    "                                 r.getAs[Row](\"post_divergence_traversed_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                                 r.getAs[Row](\"post_divergence_traversed_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "          )\n",
    "    })\n",
    "      \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Load and cache divergences from maps_intel.navigation_route_divergence\n",
    "*/\n",
    "val navDivergencesRaw = NavRouteDivergenceLoader.loadDivergences(startDate, endDate, Array())\n",
    "val div = NavRouteDivergenceLoader.makeDataset(navDivergencesRaw).cache\n",
    "div.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "This class loads trasitions from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession, Column}\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import scala.collection.mutable.Map\n",
    "\n",
    "object TransitionTraversalCountsLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadTTC(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select first_segment, last_segment, via_segments, \n",
    "         | sum(suggested_traversals) as suggested_traversals, \n",
    "         | sum(actual_traversals) as actual_traversals, \n",
    "         | sum(overlap_traversals) as overlap_traversals \n",
    "         | from route_corpus_features.transition_traversal_counts\n",
    "         | where datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | AND first_segment is not NULL\n",
    "         | AND last_segment is not NULL\n",
    "         | AND via_segments is not NULL\n",
    "         | AND via_segments[0] is not NULL\n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | group by first_segment, last_segment, via_segments\"\"\".stripMargin.replaceAll(\"\\n\", \" \")\n",
    "      \n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[TransitionTraversalCount] = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        \n",
    "        val firstSegment = Segment(r.getAs[Row](\"first_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        val lastSegment = Segment(r.getAs[Row](\"last_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        var viaSegmentsBuffer = ListBuffer[Segment]()\n",
    "        \n",
    "        r.getAs[Seq[Any]](\"via_segments\").foreach(row => {\n",
    "            val segmentInfo = row.asInstanceOf[Row]\n",
    "            viaSegmentsBuffer += Segment(segmentInfo.getAs[String](\"segment_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"start_junction_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"end_junction_uuid\")\n",
    "                                        )\n",
    "        })\n",
    "        \n",
    "        \n",
    "      TransitionTraversalCount(\n",
    "        firstSegment,\n",
    "        lastSegment,\n",
    "        viaSegmentsBuffer.toList.head,  \n",
    "        r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        r.getAs[Long](\"actual_traversals\").toInt)\n",
    "    })\n",
    "      .filter(T => T.firstSegment.end_junction_uuid == T.viaSegment.start_junction_uuid && \n",
    "            T.viaSegment.end_junction_uuid == T.lastSegment.start_junction_uuid)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Load trasitions from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "val ttcRaw = TransitionTraversalCountsLoader.loadTTC(startDate, endDate)\n",
    "val ttc = TransitionTraversalCountsLoader.makeDataset(ttcRaw).cache\n",
    "ttc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads segments from route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object SegmentTraversalCountLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadSegments(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select segment_uuid, start_junction_uuid, end_junction_uuid, \n",
    "         | sum(suggested_traversals) as suggested_traversals,\n",
    "         | sum(overlap_traversals) as overlap_traversals,\n",
    "         | sum(actual_traversals) as actual_traversals\n",
    "         | from route_corpus_features.segment_traversal_counts\n",
    "         | where segment_uuid is not null \n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | and datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | group by segment_uuid, start_junction_uuid, end_junction_uuid\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[SegmentTraversalCount] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      SegmentTraversalCount(\n",
    "        segment = Segment(r.getAs[String](\"segment_uuid\"), r.getAs[String](\"start_junction_uuid\"), r.getAs[String](\"end_junction_uuid\")),\n",
    "        suggestedCount = r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        overlapCount = r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        actualCount = r.getAs[Long](\"actual_traversals\").toInt\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load segments from route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "val stcRaw = SegmentTraversalCountLoader.loadSegments(startDate, endDate)\n",
    "val stc = SegmentTraversalCountLoader.makeDataset(stcRaw).cache\n",
    "stc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "filter divergences where segments are not joined by common junctions \n",
    "*/\n",
    "val filteredDiv = div.alias(\"DIV\").\n",
    "                    joinWith(stc.alias(\"STC\"), \n",
    "                        col(\"DIV.div_segment.segment_uuid\")===col(\"STC.segment.segment_uuid\")\n",
    "                        &&col(\"DIV.div_segment.start_junction_uuid\")===col(\"STC.segment.start_junction_uuid\")\n",
    "                        &&col(\"DIV.div_segment.end_junction_uuid\")===col(\"STC.segment.end_junction_uuid\")).\n",
    "                    joinWith(stc.alias(\"STC_SUG\"),\n",
    "                        col(\"_1.post_div_suggested_segment.segment_uuid\")===col(\"STC_SUG.segment.segment_uuid\")\n",
    "                        &&col(\"_1.post_div_suggested_segment.start_junction_uuid\")===col(\"STC_SUG.segment.start_junction_uuid\")\n",
    "                        &&col(\"_1.post_div_suggested_segment.end_junction_uuid\")===col(\"STC_SUG.segment.end_junction_uuid\")).\n",
    "                    joinWith(stc.alias(\"STC_ACT\"),\n",
    "                        col(\"_1._1.post_div_traversed_segment.segment_uuid\")===col(\"STC_ACT.segment.segment_uuid\")\n",
    "                        &&col(\"_1._1.post_div_traversed_segment.start_junction_uuid\")===col(\"STC_ACT.segment.start_junction_uuid\")\n",
    "                        &&col(\"_1._1.post_div_traversed_segment.end_junction_uuid\")===col(\"STC_ACT.segment.end_junction_uuid\")).\n",
    "                    map(tuple => tuple._1._1._1).\n",
    "                    filter(d => d.div_segment.end_junction_uuid == d.post_div_suggested_segment.start_junction_uuid \n",
    "                        && d.div_segment.end_junction_uuid == d.post_div_traversed_segment.start_junction_uuid).\n",
    "                    cache\n",
    "filteredDiv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Compute the TTC pair of Suggested transition and actual transition.\n",
    "For testing purpose, pairing is done here only with suggestedTransition where there is no traversal at all.\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val ttcPairs = ttc.alias(\"ACT\").\n",
    "joinWith(ttc.alias(\"SUG\"), \n",
    "         col(\"ACT.firstSegment\")===col(\"SUG.firstSegment\")&&\n",
    "         col(\"ACT.viaSegment\")===col(\"SUG.viaSegment\")&&\n",
    "         col(\"ACT.lastSegment\")=!=col(\"SUG.lastSegment\")\n",
    "        ).\n",
    "map(pair => TransitionTraversalCountPair(pair._1, pair._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttcPairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** \n",
    "Join transition pairs with divergences to compute the feature \n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "val tdf = ttcPairs.alias(\"TTC\").joinWith(filteredDiv.alias(\"DIV\"),\n",
    "                                         col(\"TTC.actualTransition.firstSegment.segment_uuid\")===col(\"DIV.pre_div_segment.segment_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.firstSegment.start_junction_uuid\")===col(\"DIV.pre_div_segment.start_junction_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.firstSegment.end_junction_uuid\")===col(\"DIV.pre_div_segment.end_junction_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.viaSegment.segment_uuid\")===col(\"DIV.div_segment.segment_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.viaSegment.start_junction_uuid\")===col(\"DIV.div_segment.start_junction_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.viaSegment.end_junction_uuid\")===col(\"DIV.div_segment.end_junction_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.lastSegment.segment_uuid\")===col(\"DIV.post_div_traversed_segment.segment_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.lastSegment.start_junction_uuid\")===col(\"DIV.post_div_traversed_segment.start_junction_uuid\")&&\n",
    "                                         col(\"TTC.actualTransition.lastSegment.end_junction_uuid\")===col(\"DIV.post_div_traversed_segment.end_junction_uuid\")&&\n",
    "                                         col(\"TTC.suggestedTransition.lastSegment.segment_uuid\")===col(\"DIV.post_div_suggested_segment.segment_uuid\")&&\n",
    "                                         col(\"TTC.suggestedTransition.lastSegment.start_junction_uuid\")===col(\"DIV.post_div_suggested_segment.start_junction_uuid\")&&\n",
    "                                         col(\"TTC.suggestedTransition.lastSegment.end_junction_uuid\")===col(\"DIV.post_div_suggested_segment.end_junction_uuid\")\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Compute features for turn restrictions.\n",
    "*/\n",
    "val features = tdf.alias(\"TDF\").joinWith(stc.alias(\"STC\"), \n",
    "                          col(\"TDF._2.post_div_suggested_segment.segment_uuid\")===col(\"STC.segment.segment_uuid\")&&\n",
    "                          col(\"TDF._2.post_div_suggested_segment.start_junction_uuid\")===col(\"STC.segment.start_junction_uuid\")&&\n",
    "                          col(\"TDF._2.post_div_suggested_segment.end_junction_uuid\")===col(\"STC.segment.end_junction_uuid\")\n",
    "                         ).cache\n",
    "features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "Map features for turn restrictions to TurnRestrictionFeature.\n",
    "*/\n",
    "val trIssues = features.map(ft => {\n",
    "    \n",
    "    var segments = ListBuffer[String]()\n",
    "    segments += ft._1._1.suggestedTransition.firstSegment.segment_uuid\n",
    "    segments += ft._1._1.suggestedTransition.viaSegment.segment_uuid\n",
    "    segments += ft._1._1.suggestedTransition.lastSegment.segment_uuid\n",
    "    \n",
    "    TurnRestrictionFeature(\n",
    "        segmentIds = segments.toList,\n",
    "        actualTraversalsCountOnTransition = ft._1._1.suggestedTransition.actualCount,\n",
    "        suggestedTraversalsCountOnTransition = ft._1._1.suggestedTransition.suggestedCount,\n",
    "        actualTraversalsCountOnSuggestedSegment = ft._2.actualCount,\n",
    "        sampleTripsUuids = ft._1._2.trip_id\n",
    "    )\n",
    "})\n",
    "\n",
    "trIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "aggregate features for turn restrictions to generate unique issues.\n",
    "*/\n",
    "val aggIssues = trIssues.\n",
    "groupBy(col(\"segmentIds\")).\n",
    "agg(max(col(\"actualTraversalsCountOnTransition\")).alias(\"actualTraversalsCountOnTransition\"),\n",
    "    sum(col(\"suggestedTraversalsCountOnTransition\")).alias(\"suggestedTraversalsCountOnTransition\"),\n",
    "    max(col(\"actualTraversalsCountOnSuggestedSegment\")).alias(\"actualTraversalsCountOnSuggestedSegment\"),\n",
    "    collect_list(col(\"sampleTripsUuids\")).alias(\"sampleTripsUuids\")).\n",
    "map(row => {\n",
    "    \n",
    "    var segments = ListBuffer[String]()\n",
    "    row.getAs[Seq[String]](\"segmentIds\").foreach(value => segments += value)\n",
    "    \n",
    "//     var trips = ListBuffer[String]()\n",
    "//     row.getAs[Seq[String]](\"sampleTripsUuids\").foreach(value => trips += value)\n",
    "    \n",
    "    TurnRestrictionFeature(\n",
    "        segmentIds = segments.toList,\n",
    "        actualTraversalsCountOnTransition = row.getAs[Int](\"actualTraversalsCountOnTransition\").toInt,\n",
    "        suggestedTraversalsCountOnTransition = row.getAs[Long](\"suggestedTraversalsCountOnTransition\").toInt,\n",
    "        actualTraversalsCountOnSuggestedSegment = row.getAs[Int](\"actualTraversalsCountOnSuggestedSegment\").toInt,\n",
    "        sampleTripsUuids = List[String]()\n",
    "    )\n",
    "    \n",
    "})\n",
    "\n",
    "aggIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** Filter features satisfying following criteria\n",
    " 1. Suggested Transition suggested count > 21\n",
    " 2. Suggested Transition actual count = 0\n",
    " 3. Suggested Segment traversal count > 6\n",
    "*/\n",
    "\n",
    "val turnRestrictionIssues = aggIssues.filter(\n",
    "                f => f.suggestedTraversalsCountOnTransition >= 21 &&\n",
    "                f.actualTraversalsCountOnTransition <= 0 &&\n",
    "                f.actualTraversalsCountOnSuggestedSegment >= 6).cache\n",
    "\n",
    "turnRestrictionIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val uniqueIssues = turnRestrictionIssues.\n",
    "map(tr => (tr._1._1.suggestedTransition, tr)).\n",
    "dropDuplicates(\"_1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "publish features to HDFS in csv format\n",
    "*/\n",
    "features.\n",
    "orderBy(col(\"_1._2.preDivSegment\"), col(\"_1._2.divSegment\"), col(\"_1._2.postDivSuggestedSegment\")).\n",
    "map(row => {\n",
    "    \n",
    "    val feature = row._1._2\n",
    "    \n",
    "    Output(\n",
    "        divergence_segment_uuid = feature.divSegment.segment_uuid,\n",
    "        divergence_segment_start_junction_uuid = feature.divSegment.start_junction_uuid,\n",
    "        divergence_segment_end_junction_uuid = feature.divSegment.end_junction_uuid,\n",
    "        \n",
    "        post_divergence_segment_uuid = feature.postDivSuggestedSegment.segment_uuid,\n",
    "        post_divergence_segment_start_junction_uuid = feature.postDivSuggestedSegment.start_junction_uuid,\n",
    "        post_divergence_segment_end_junction_uuid = feature.postDivSuggestedSegment.end_junction_uuid,\n",
    "        \n",
    "        pre_divergence_segment_uuid = feature.postDivTraversedSegment.segment_uuid,\n",
    "        pre_divergence_segment_start_junction_uuid = feature.postDivTraversedSegment.start_junction_uuid,\n",
    "        pre_divergence_segment_end_junction_uuid = feature.postDivTraversedSegment.end_junction_uuid,\n",
    "        \n",
    "        observations = feature.observations,\n",
    "        \n",
    "        sampleTrips = feature.sampleTrips.mkString(\",\")\n",
    "    )\n",
    "    \n",
    "}).\n",
    "distinct.\n",
    "limit(100).\n",
    "write.\n",
    "mode(SaveMode.Overwrite).\n",
    "option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\").\n",
    "option(\"header\",\"true\").\n",
    "csv(\"turn_restriction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads turn restriction issues from UMM issues table\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object UmmIssueLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadUmmIssues(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | issueuuid,\n",
    "         | ummbuilduuid,\n",
    "         | latitude,\n",
    "         | longitude,\n",
    "         | sampletripuuids,\n",
    "         | featureuuids,\n",
    "         | numberoftrips,\n",
    "         | cityid\n",
    "         | from map_creation.meds_umm_issues\n",
    "         | where createddate between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | and productionrun = false\n",
    "         | and detectorname = 'TurnRestrictionDetector'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[UMMIssue] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        var segments = ListBuffer[String]()\n",
    "        r.getAs[Seq[String]](\"featureuuids\").foreach(row => segments += row)\n",
    "        \n",
    "        var trips = ListBuffer[String]()\n",
    "        r.getAs[Seq[String]](\"sampletripuuids\").foreach(row => trips += row)\n",
    "            \n",
    "        \n",
    "        UMMIssue(\n",
    "            issueuuid = r.getAs[String](\"issueuuid\"),\n",
    "            ummbuilduuid = r.getAs[String](\"ummbuilduuid\"),\n",
    "            latitude = r.getAs[Double](\"latitude\"),\n",
    "            longitude = r.getAs[Double](\"longitude\"),\n",
    "            sampletripuuids = trips.toList,\n",
    "            featureuuids = segments.toList,\n",
    "            numberoftrips = r.getAs[Int](\"numberoftrips\"),\n",
    "            cityid = r.getAs[Int](\"cityid\")\n",
    "          )\n",
    "    })\n",
    "      \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load turn restriction issues from UMM issues table\n",
    "*/\n",
    "val ummIssuesRaw = UmmIssueLoader.loadUmmIssues(\"2022-07-07\", \"2022-07-08\")\n",
    "val ummIssues = UmmIssueLoader.makeDataset(ummIssuesRaw)\n",
    "ummIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "find common issues from previous run and current run\n",
    "*/\n",
    "val commonIssues = turnRestrictionIssues.map(issue => {\n",
    "    val segments = ListBuffer[String]()\n",
    "    segments += issue._1._1.suggestedTransition.firstSegment.segment_uuid\n",
    "    segments += issue._1._1.suggestedTransition.viaSegment.segment_uuid\n",
    "    segments += issue._1._1.suggestedTransition.lastSegment.segment_uuid\n",
    "    \n",
    "    (segments, issue)\n",
    "}).alias(\"T\").joinWith(ummIssues.alias(\"U\"), \n",
    "                       col(\"T._1\")===col(\"U.featureuuids\"),\n",
    "                       \"left\")\n",
    "\n",
    "commonIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "display top common issues\n",
    "*/\n",
    "commonIssues.filter(row => row._2 == null).limit(10).collect.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
