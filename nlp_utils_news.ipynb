{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import itertools, string, operator, re, unicodedata, nltk\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "new_stopwords=['Hi','Hello',' ','Team','Thanks','Hey','regards','regard',\n",
    "                     'please','jira','ext','com','image.png',\n",
    "                     'Capture','PNG','thumbnail','thank','could','pm','jan',\n",
    "                     'feb','mar','apr','may','june','jul','aug','sep','oct','nov','dec','--','d_','_xd_']\n",
    "for i in range(len(new_stopwords)):\n",
    "    new_stopwords[i] = new_stopwords[i].lower()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(new_stopwords)\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\" \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "def casual_tokenizer(text): #Splits words on white spaces (leaves contractions intact) and splits out trailing punctuation\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "#Function to replace the nltk pos tags with the corresponding wordnet pos tag to use the wordnet lemmatizer\n",
    "def get_word_net_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemma_wordnet(tagged_text):\n",
    "    final = []\n",
    "    for word, tag in tagged_text:\n",
    "        wordnet_tag = get_word_net_pos(tag)\n",
    "        if wordnet_tag is None:\n",
    "            final.append(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            final.append(lemmatizer.lemmatize(word, pos=wordnet_tag))\n",
    "    return final\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    bracket_del = re.sub(r'\\[.*?\\]', '  ', text)\n",
    "    apostrphe = re.sub('â€™', \"'\", bracket_del)\n",
    "    string = apostrphe.replace('\\r','  ')\n",
    "    string = string.replace('\\n','  ')\n",
    "    extra_space = re.sub(' +',' ', string)\n",
    "    return extra_space\n",
    "\n",
    "def process_text(text):\n",
    "    text=remove_html(text.lower())\n",
    "    token=casual_tokenizer(text)\n",
    "    lower = [item.lower() for item in token]\n",
    "    tagged = nltk.pos_tag(lower)\n",
    "    lemma = lemma_wordnet(tagged)\n",
    "    no_num = [re.sub('[0-9]+', '', each) for each in lemma]\n",
    "    no_punc = [w for w in no_num if w not in punc]\n",
    "    no_stop = [w for w in no_punc if w not in stop_words and len(w)>2]\n",
    "    return no_stop\n",
    "\n",
    "def word_count(text):\n",
    "    return len(str(text).split(' '))\n",
    "\n",
    "def word_freq(clean_text_list, top_n):\n",
    "    \"\"\"\n",
    "    Word Frequency\n",
    "    \"\"\"\n",
    "    flat = [item for sublist in clean_text_list for item in sublist]\n",
    "    with_counts = Counter(flat)\n",
    "    top = with_counts.most_common(top_n)\n",
    "    word = [each[0] for each in top]\n",
    "    num = [each[1] for each in top]\n",
    "    return pd.DataFrame([word, num]).T\n",
    "\n",
    "def word_freq_bigrams(clean_text_list, top_n):\n",
    "    \"\"\"\n",
    "    Word Frequency With Bigrams\n",
    "    \"\"\"\n",
    "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
    "    w_bigrams = list(bigram_model[clean_text_list])\n",
    "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
    "    with_counts = Counter(flat_w_bigrams)\n",
    "    top = with_counts.most_common(top_n)\n",
    "    word = [each[0] for each in top]\n",
    "    num = [each[1] for each in top]\n",
    "    return pd.DataFrame([word, num]).T\n",
    "\n",
    "\n",
    "def bigram_freq(clean_text_list, top_n):\n",
    "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
    "    w_bigrams = list(bigram_model[clean_text_list])\n",
    "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
    "    bigrams = []\n",
    "    for each in flat_w_bigrams:\n",
    "        if '_' in each:\n",
    "            bigrams.append(each)\n",
    "    counts = Counter(bigrams)\n",
    "    top = counts.most_common(top_n)\n",
    "    word = [each[0] for each in top]\n",
    "    num = [each[1] for each in top]\n",
    "    return pd.DataFrame([word, num]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
