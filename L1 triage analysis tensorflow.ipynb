{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eaac7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import json\n",
    "#import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb4fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.layers import InputLayer, Activation, merge, Concatenate,Input\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import numpy as np\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8245471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(message):\n",
    "\n",
    "    #stopwords\n",
    "    new_stopwords=['Hi','Hello','Team','Thanks','Hey','regards','please','jira','@uber.com','@ext.uber.com',' !image.png|thumbnail!','!Capture.PNG|thumbnail!']\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    stpwrd.extend(new_stopwords)\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #removing the numerical values and working only with text values\n",
    "    message = re.sub('[^a-zA-Z]', \" \", message )\n",
    "    #lowering and removing punctuation\n",
    "    message = re.sub(r'[^\\w\\s]','', message.lower())\n",
    "    #removing the stopwords\n",
    "    message = ' '.join([word for word in message.split() if word not in stpwrd and len(word)>1])\n",
    "    #lemmatizing the text\n",
    "    message =  \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation])\n",
    "    #message = [lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation]\n",
    "    #tagged_sentence = nltk.tag.pos_tag(message.split())\n",
    "    #edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' or tag != 'NNPS']\n",
    "    #message =  \" \".join(edited_sentence)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d59a70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44513, 5)\n",
      "(40025, 5)\n"
     ]
    }
   ],
   "source": [
    "#read input from historical data into dataframe\n",
    "data_df = pd.read_excel('/Users/jghosh2/Documents/my-notebook/L1 triage POC/data/Updated-JIRA_DUMP_Mar_2020_March_2022_Sourabh.xlsx',usecols=['Summary','Description','Component'])\n",
    "data_df['Summary']=data_df['Summary'].astype(str)\n",
    "data_df['Description']=data_df['Description'].astype(str)\n",
    "data_df.dropna()\n",
    "#choose sample data from entire data\n",
    "data_df = data_df.sample(frac=1, random_state=42)\n",
    "data_df['combined_text'] = data_df[['Summary','Description']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_df['processed_text'] = data_df['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_df.dropna()\n",
    "print(data_df.shape)\n",
    "data_df=data_df.drop_duplicates(subset=['processed_text','Component'],keep='first')\n",
    "#data_df_duplicated=data_df[data_df.duplicated(subset=['processed_text','target'],keep=False)]\n",
    "#print(data_df_duplicated.shape)\n",
    "print(data_df.shape)\n",
    "data_df = data_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1191e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data_df, test_size=0.2, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d61416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = list(compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(data_df['Component']),\n",
    "                                        y = data_df['Component']                                                    \n",
    "                                    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1be8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights.sort()\n",
    "weights={}\n",
    "for index, weight in enumerate(class_weights) :\n",
    "    weights[index]=weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2885b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train['processed_text'].values, X_train['Component'].values))\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test['processed_text'].values, X_test['Component'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bd7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,len(data_df['Component'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4fb1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tf.lookup.StaticHashTable(\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=tf.constant(data_df['Component'].unique()),\n",
    "        values=tf.constant(list(range(0,len(data_df['Component'].unique())))),\n",
    "    ),\n",
    "    default_value=tf.constant(-1),\n",
    "    name=\"target_encoding\"\n",
    ")\n",
    "\n",
    "@tf.function\n",
    "def target(x):\n",
    "    return table.lookup(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2738215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset, size=5):\n",
    "      for batch, label in dataset.take(size):\n",
    "          print(batch.numpy())\n",
    "          print(target(label).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a87ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34e3d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(text, labels):\n",
    "    return text, tf.one_hot(target(labels),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e7f9010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_f=dataset_train.map(fetch)\n",
    "test_data_f=dataset_test.map(fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02645847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'need help split invoice hi team need help splitting invoice prepare mass addition',\n",
       "        b'request create po another entity po creator nangulo uber com manager detailed justification benefit manager latam responsible vendor management benefit granted employee costa rica creation po entity require access entity portier costa rica srl',\n",
       "        b'access oracle apex hi recently joined strategic finance team london would possible receive access oracle apex fp permission thank help best shaked',\n",
       "        b'clone delegate request aj greulich hi fintech team set michael huaco delegate aj greulich coupa oracle approval date range start date end date reason paternity leave let know question thanks blaine http uber box com shared static xdf wm te bi pior dc gif blaine milner finops procure pay gpo bmilner uber com mailto bmilner uber com uber com http www uber com twitter http twitter com uber facebook http www facebook com uber blog http blog uber com instagram http www instagram com uber supplier resource found http www uber com info supplier confidentiality notice mail message including attachment sole use intended recipient may contain privileged confidential information unauthorized review use disclosure distribution prohibited intended recipient contact sender reply mail destroy copy original message',\n",
       "        b'unable access coupa hey trying access coupa onelogin getting error page login errorunable locate user record sso id melody kuo uber com http uberinternal com servicedesk customershim image icon mail small gif fromissue mailto melody kuo uber com valid sso id assigned user able access service whober concur workday etc fine idea thanks melody'],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(5, 6), dtype=float32, numpy=\n",
       " array([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, train_labels = next(iter(train_data_f.batch(5)))\n",
    "train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29bdf826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[ 0.14832334,  0.11642308, -0.00259464, -0.2741825 , -0.03419008,\n",
       "         0.05448474,  0.09951627,  0.13680217, -0.21613999,  0.45808873,\n",
       "        -0.10294521, -0.11740037, -0.03200293,  0.08250536, -0.05801883,\n",
       "        -0.20927465, -0.0221511 ,  0.09924161, -0.09119239,  0.0615944 ,\n",
       "         0.13973267,  0.16179985, -0.08059154, -0.08594684,  0.0673902 ,\n",
       "        -0.00145557, -0.00130037,  0.11433198, -0.23935963, -0.00845843,\n",
       "        -0.05642676, -0.01102808,  0.10869243,  0.01076306, -0.06180575,\n",
       "         0.00575203, -0.19492295,  0.08626429,  0.04200781,  0.01668094,\n",
       "         0.08677208, -0.13424514,  0.04854997, -0.10535169, -0.09532416,\n",
       "         0.05975296, -0.10966925,  0.21322154,  0.14673808, -0.08014761,\n",
       "        -0.00512229, -0.16650414, -0.02921075,  0.08152414, -0.18004051,\n",
       "         0.16847077, -0.02556576, -0.30985662, -0.05359058,  0.14351867,\n",
       "         0.10968975,  0.2667437 ,  0.01079381, -0.16184346, -0.15252608,\n",
       "         0.33485833, -0.18355441, -0.15465418, -0.1767907 ,  0.05760702,\n",
       "        -0.00303852, -0.00916222,  0.06596314, -0.04850843,  0.07315758,\n",
       "         0.05555885,  0.1242366 , -0.11820403,  0.00952304, -0.13448174,\n",
       "         0.22442801,  0.0781167 , -0.07080161, -0.17793061,  0.25694665,\n",
       "         0.141633  ,  0.04885754, -0.00645167,  0.3063877 ,  0.25064746,\n",
       "        -0.05089708, -0.07296604, -0.10931824, -0.12802964,  0.07215   ,\n",
       "        -0.19113162, -0.25385243, -0.01959343, -0.17271605,  0.00639918,\n",
       "         0.12255625,  0.06904496,  0.0090955 , -0.07386574, -0.04699721,\n",
       "         0.14155184,  0.116033  ,  0.1852015 ,  0.13416424,  0.1260025 ,\n",
       "        -0.162079  ,  0.04108867, -0.27624035, -0.0513684 ,  0.05885069,\n",
       "         0.1081138 , -0.03346517, -0.0488877 ,  0.09956218,  0.13959822,\n",
       "        -0.08125585, -0.14201838, -0.22590266,  0.04869226,  0.06685221,\n",
       "        -0.03320739,  0.06051927,  0.08628503]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, output_shape=[128], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)\n",
    "hub_layer(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e55975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 128)               124642688 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124,686,246\n",
      "Trainable params: 124,686,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "for units in [128, 128, 64 , 32]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8148a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "802629f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_f=train_data_f.shuffle(70000).batch(512)\n",
    "test_data_f=test_data_f.batch(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c123bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 77s 1s/step - loss: 2.5733 - accuracy: 0.1537 - val_loss: 6.8391 - val_accuracy: 0.0586\n",
      "Epoch 2/4\n",
      "63/63 [==============================] - 76s 1s/step - loss: 1014.4958 - accuracy: 0.1072 - val_loss: 6157.3218 - val_accuracy: 0.0643\n",
      "Epoch 3/4\n",
      "63/63 [==============================] - 76s 1s/step - loss: 35850.4180 - accuracy: 0.1046 - val_loss: 130183.4688 - val_accuracy: 0.0643\n",
      "Epoch 4/4\n",
      "63/63 [==============================] - 77s 1s/step - loss: 358917.6250 - accuracy: 0.1081 - val_loss: 497689.4375 - val_accuracy: 0.0586\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data_f,\n",
    "                    epochs=4,\n",
    "                    validation_data=test_data_f,\n",
    "                    verbose=1,\n",
    "                    class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2fe1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 811630.8125 - accuracy: 0.1026 - 414ms/epoch - 414ms/step\n",
      "[811630.8125, 0.10256090015172958]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(dataset_test.map(fetch).batch(11491), verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94df8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = next(iter(dataset_test.map(fetch).batch(45963)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bdffe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67bde187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecf4b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      5837\n",
      "           1       0.00      0.00      0.00        95\n",
      "           2       0.00      0.00      0.00       515\n",
      "           3       0.00      0.00      0.00       469\n",
      "           4       0.00      0.00      0.00       268\n",
      "           5       0.10      1.00      0.19       821\n",
      "\n",
      "    accuracy                           0.10      8005\n",
      "   macro avg       0.02      0.17      0.03      8005\n",
      "weighted avg       0.01      0.10      0.02      8005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels.numpy().argmax(axis=1), y_pred.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4481c1d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shallow_rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b7b5adbdd1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilename_primary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'finalized_model.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshallow_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_primary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shallow_rf' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import datetime\n",
    "# save the model to disk\n",
    "filename_primary= 'finalized_model.sav'\n",
    "pickle.dump(shallow_rf, open(filename_primary, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
