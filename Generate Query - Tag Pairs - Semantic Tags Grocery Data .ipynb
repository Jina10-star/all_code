{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Thanks for using Drogon for your interactive Spark application. We update Drogon/SparkMagic as often as possible to make it easier, faster and more reliable for you. Have a question or feedback? Ping us on [uChat](https://uchat.uberinternal.com/uber/channels/spark).</span>\n",
    "\n",
    "What's New\n",
    "- Now you can use `%%configure` and `%%spark` magics to configure and start a Spark session (deprecating hard-to-use `%load_ext sparkmagic.magics` and `manage_spark` magics). Check out [this example](https://workbench.uberinternal.com/explore/knowledge/localfile/cwang/sparkmagic_python2_example.ipynb) for more details.\n",
    "- Improved `%%configure` magic. You now can use it to make all Spark and Drogon configurations from within notebook itself. Check out our [latest documentation & examples](https://docs.google.com/document/d/1mkYtDHquh4FjqTeA0Fxii8lyV-P6qzmoABhmmRwm_00/edit#heading=h.xn14pmoorsn0) for more details.\n",
    "- Bug fixes and performance updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs:\n",
       "<pre>{\n",
       "  \"pyFiles\": [], \n",
       "  \"kind\": \"spark\", \n",
       "  \"proxyUser\": \"radhesh\", \n",
       "  \"sparkEnv\": \"SPARK_24\", \n",
       "  \"driverMemory\": \"8g\", \n",
       "  \"queue\": \"uber_eats_ml\", \n",
       "  \"conf\": {\n",
       "    \"spark.dynamicAllocation.enabled\": \"true\", \n",
       "    \"spark.driver.memory\": \"12g\", \n",
       "    \"spark.dynamicAllocation.maxExecutors\": 200, \n",
       "    \"spark.driver.memoryOverhead\": \"4g\", \n",
       "    \"spark.executor.memoryOverhead\": \"4g\", \n",
       "    \"spark.hadoop.hadoop.security.authentication\": \"simple\", \n",
       "    \"spark.dynamicAllocation.minExecutors\": 100, \n",
       "    \"spark.executor.memory\": \"12g\", \n",
       "    \"spark.dynamicAllocation.initialExecutors\": 100, \n",
       "    \"spark.shuffle.service.enabled\": true, \n",
       "    \"spark.sql.shuffle.partitions\": 500\n",
       "  }, \n",
       "  \"executorCores\": 2, \n",
       "  \"driverCores\": 2, \n",
       "  \"jars\": [\n",
       "    \"/user/radhesh/spark-corenlp-0.4.0-spark2.4-scala2.11.jar\", \n",
       "    \"/user/radhesh/stanford-corenlp-3.9.1-models.jar\"\n",
       "  ], \n",
       "  \"executorMemory\": \"12g\", \n",
       "  \"drogonHeaders\": {\n",
       "    \"X-DROGON-CLUSTER\": \"phx2/secure\"\n",
       "  }\n",
       "}</pre><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure \n",
    "{\n",
    "  \"pyFiles\": [],\n",
    "  \"kind\": \"spark\",\n",
    "  \"proxyUser\": \"radhesh\",\n",
    "  \"sparkEnv\": \"SPARK_24\",\n",
    "  \"driverMemory\": \"8g\",\n",
    "  \"queue\": \"uber_eats_ml\",\n",
    "  \"conf\": { \n",
    "      \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "      \"spark.dynamicAllocation.initialExecutors\":100,\n",
    "      \"spark.dynamicAllocation.minExecutors\":100,\n",
    "      \"spark.dynamicAllocation.maxExecutors\" : 200,\n",
    "      \"spark.executor.memory\": \"12g\",\n",
    "      \"spark.executor.memoryOverhead\": \"4g\",\n",
    "      \"spark.driver.memory\": \"12g\",\n",
    "      \"spark.driver.memoryOverhead\" : \"4g\",\n",
    "      \"spark.hadoop.hadoop.security.authentication\": \"simple\",\n",
    "      \"spark.shuffle.service.enabled\" : true,\n",
    "      \"spark.sql.shuffle.partitions\" : 500\n",
    "},\n",
    "  \"executorCores\": 2,\n",
    "  \"driverCores\": 2,\n",
    "  \"executorMemory\": \"12g\",\n",
    "    \"jars\": [\"/user/radhesh/spark-corenlp-0.4.0-spark2.4-scala2.11.jar\",\n",
    "            \"/user/radhesh/stanford-corenlp-3.9.1-models.jar\"],\n",
    "   \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"phx2/secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application (can take 60s or more)...\n",
      "Starting heartbeat thread...done.\n",
      "Waiting for Drogon session to be ready...................................................\n",
      "Drogon session is ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Drogon Session ID</th><th>Spark Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>570369339</td><td>application_1674066407532_2036689</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://hadoop-ui.uberinternal.com/gateway/prodsec_phx2/yarn/nodemanager/node/containerlogs/container_e249_1674066407532_2036689_01_000001/radhesh?scheme=http&host=phx3-9r4.prod.uber.internal&port=8042\">Link</a></td><td><a target=\"_blank\" href=\"https://shs-phx2.uberinternal.com/proxy/application_1674066407532_2036689\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "\n",
      "\n",
      "Cell execution took 103 seconds.\n",
      "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@611052b8"
     ]
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_sql_query: String = with data_stage0 AS ( SELECT normalized_search_term as query,item_name,COUNT(item_name) as item_freq FROM uber_eats.michelangelo_basis_eats_in_store_search_ranking_v1 where datestr >= '2022-01-01' and location_type = 'GROCERY_STORE' and merchant_type_analytics not in ('MERCHANT_TYPE_RESTAURANT','MERCHANT_TYPE_UNKNOWN') and language_code like 'en%' and length(normalized_search_term) >= 4 and length(item_name) >= 3 and country_iso3 IN ('USA', 'GBA', 'CAN') GROUP BY normalized_search_term, item_name) SELECT query, item_freq, semantic_tags from data_stage0 INNER JOIN uber_eats.semantic_tags_grocery_data ON uber_eats.semantic_tags_grocery_data.item_name = data_stage0.item_name ORDER by item_freq DESC"
     ]
    }
   ],
   "source": [
    "var input_sql_query = \"with data_stage0 AS ( SELECT normalized_search_term as query,item_name,COUNT(item_name) as item_freq FROM uber_eats.michelangelo_basis_eats_in_store_search_ranking_v1 where datestr >= '2022-01-01' and location_type = 'GROCERY_STORE' and merchant_type_analytics not in ('MERCHANT_TYPE_RESTAURANT','MERCHANT_TYPE_UNKNOWN') and language_code like 'en%' and length(normalized_search_term) >= 4 and length(item_name) >= 3 and country_iso3 IN ('USA', 'GBA', 'CAN') GROUP BY normalized_search_term, item_name) SELECT query, item_freq, semantic_tags from data_stage0 INNER JOIN uber_eats.semantic_tags_grocery_data ON uber_eats.semantic_tags_grocery_data.item_name = data_stage0.item_name ORDER by item_freq DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [query: string, item_freq: bigint ... 1 more field]"
     ]
    }
   ],
   "source": [
    "var data = spark.sql(input_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+\n",
      "|    query|item_freq|       semantic_tags|\n",
      "+---------+---------+--------------------+\n",
      "|chocolate|   139654|ice cream:pre pac...|\n",
      "|ice cream|   129425|ice cream:ice cre...|\n",
      "|ice cream|   125503|frozen dessert:be...|\n",
      "|    pizza|   123267|bakery:frozen:bre...|\n",
      "|     milk|   105665|premium drink:bev...|\n",
      "|ice cream|    93450|ice cream popsicl...|\n",
      "|ice cream|    92837|cal dairy ice cre...|\n",
      "|ice cream|    87953|ice cream:healthi...|\n",
      "|ice cream|    87749|ice cream:cal dai...|\n",
      "|ice cream|    87064|cal dairy:pint:ca...|\n",
      "|ice cream|    85898|healthier:frozen:...|\n",
      "|ice cream|    84994|ice cream popsicl...|\n",
      "|     milk|    83544|snack mixer:milk:...|\n",
      "|    water|    80112|         drink:water|\n",
      "|ice cream|    79997|halo ice cream:ca...|\n",
      "|ice cream|    79616|pint:ice cream po...|\n",
      "|ice cream|    78800|yogurt smoothie k...|\n",
      "|     milk|    78691|healthier drink:d...|\n",
      "|ice cream|    77348|           ice cream|\n",
      "|    fruit|    77071|frozen:healthier ...|\n",
      "+---------+---------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [query: string, item_freq: bigint ... 2 more fields]"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"tag\",explode(split(col(\"semantic_tags\"),\":\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+--------------------+\n",
      "|      query|item_freq|       semantic_tags|                 tag|\n",
      "+-----------+---------+--------------------+--------------------+\n",
      "| roast beef|       17|beef pork:meat:co...|           beef pork|\n",
      "| roast beef|       17|beef pork:meat:co...|                meat|\n",
      "| roast beef|       17|beef pork:meat:co...|                cook|\n",
      "| roast beef|       17|beef pork:meat:co...|meat seafood plan...|\n",
      "| roast beef|       17|beef pork:meat:co...|        meat seafood|\n",
      "|coffee mate|       17|coffee:drink:iced...|              coffee|\n",
      "|coffee mate|       17|coffee:drink:iced...|               drink|\n",
      "|coffee mate|       17|coffee:drink:iced...|       iced beverage|\n",
      "|     hot sa|       17|consumable:cookin...|          consumable|\n",
      "|     hot sa|       17|consumable:cookin...|             cooking|\n",
      "|     hot sa|       17|consumable:cookin...|           condiment|\n",
      "|     hot sa|       17|consumable:cookin...|noodle rice grain...|\n",
      "|     hot sa|       17|consumable:cookin...|               sauce|\n",
      "|     hot sa|       17|consumable:cookin...|     condiment sauce|\n",
      "|     hot sa|       17|consumable:cookin...|             grocery|\n",
      "|     hot sa|       17|consumable:cookin...|    condiment spread|\n",
      "|     hot sa|       17|consumable:cookin...|grocery condiment...|\n",
      "|     hot sa|       17|consumable:cookin...|     condiment syrup|\n",
      "|     hot sa|       17|consumable:cookin...| meat marinade sauce|\n",
      "|     hot sa|       17|consumable:cookin...|      sauce marinade|\n",
      "+-----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: Long = 271894236"
     ]
    }
   ],
   "source": [
    "data.drop(\"semantic_tags\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [query: string, item_freq: bigint ... 1 more field]"
     ]
    }
   ],
   "source": [
    "data = data.drop(\"semantic_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res8: Long = 271894236"
     ]
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "| query|item_freq|                 tag|\n",
      "+------+---------+--------------------+\n",
      "|drinks|        8|      pop cold drink|\n",
      "|drinks|        8|             dog toy|\n",
      "|drinks|        8|vitamin mineral s...|\n",
      "|drinks|        8|               mandy|\n",
      "|drinks|        8|     essential drink|\n",
      "|drinks|        8|  protein supplement|\n",
      "|drinks|        8|                soda|\n",
      "|drinks|        8|              powder|\n",
      "|drinks|        8|        milk product|\n",
      "|drinks|        8|       confectionery|\n",
      "|drinks|        8|             chicken|\n",
      "|drinks|        8|    tea energy drink|\n",
      "|drinks|        8|chill drink water...|\n",
      "|drinks|        8|             alcohol|\n",
      "|drinks|        8|        energy drink|\n",
      "|drinks|        8|ready drink cocktail|\n",
      "|drinks|        8|              health|\n",
      "|drinks|        8|       internacional|\n",
      "|drinks|        8|   tawny drink snack|\n",
      "|drinks|        8|neighbour hood store|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_query_tag: org.apache.spark.sql.DataFrame = [query: string, tag: string ... 1 more field]"
     ]
    }
   ],
   "source": [
    "var final_query_tag = data.groupBy(\"query\",\"tag\").agg(sum(\"item_freq\").as(\"tag_freq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|               query|                 tag|tag_freq|\n",
      "+--------------------+--------------------+--------+\n",
      "|            fig bars|               casey|      80|\n",
      "|   popcorn seasoning|         snack candy|     551|\n",
      "|     pantene shampoo|     harry newsagent|     195|\n",
      "|              chiken|              frozen|    3539|\n",
      "|              redbul|           soda shop|     824|\n",
      "|  vitamin water zero|    carbonated drink|     400|\n",
      "|      raspberry leaf|             alcohol|      72|\n",
      "|             tequila|            beverage|  191633|\n",
      "|               garba|     household paper|     254|\n",
      "|               ramen|             instant|   27373|\n",
      "|            box wine|               booze|    9264|\n",
      "|ice cream + froze...|               water|    1578|\n",
      "|   motrin children's|                baby|    1410|\n",
      "|             hot dog|        chip popcorn|   12414|\n",
      "|         diet sprite|           celebrate|    3407|\n",
      "|      detergent pods|           household|    3243|\n",
      "|         crown royal|liquor ready drin...|     315|\n",
      "|       snapple apple|          soda juice|      95|\n",
      "|                soda|convinience groce...|  427433|\n",
      "|        banana chips|                chip|    4051|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "final_query_tag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [query: string, tag: string ... 1 more field]"
     ]
    }
   ],
   "source": [
    "var df = final_query_tag.orderBy(\"tag_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<console>:26: error: value query is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "       df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))\n",
      "                    ^\n",
      "<console>:26: error: value tag is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "       df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))\n",
      "                             ^\n",
      "<console>:26: error: type mismatch;\n",
      " found   : Char('-')\n",
      " required: String\n",
      "       df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))\n",
      "                                           ^\n",
      "<console>:26: error: value tag is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "       df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))\n",
      "                                                  ^\n",
      "<console>:26: error: value tag_freq is not a member of org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\n",
      "       df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))\n",
      "                                                          ^\n"
     ]
    }
   ],
   "source": [
    "df.select(df.query,df.tag,concat_ws('-',df.tag, df.tag_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkException: Job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:202)\n",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:220)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:105)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:103)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:106)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:106)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:88)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:135)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:334)\n",
      "  at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:320)\n",
      "  ... 47 elided\n",
      "Caused by: org.apache.spark.SparkException: Job 30 cancelled\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2017)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1952)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2222)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2205)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2194)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "  ... 67 more\n"
     ]
    }
   ],
   "source": [
    "final_query_tag.write.mode(\"overwrite\").insertInto(\"tmp.eats_query_tag_freq_grocery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
