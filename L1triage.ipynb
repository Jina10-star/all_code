{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!install_package_python37.sh add xlrd\n",
    "#!install_package_python37.sh add nltk==3.7\n",
    "#!install_package_python37.sh add python-dotenv\n",
    "#!install_package_python37.sh add scikit-learn==0.21.1\n",
    "#!install_package_python37.sh add pyjwt\n",
    "#!install_package_python37.sh add boxsdk\n",
    "#!install_package_python37.sh add openpyxl\n",
    "#!install_package_python37.sh add sendgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/irpa3/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime, timezone,timedelta\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "from boxsdk import Client, JWTAuth\n",
    "import chardet\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import sklearn.metrics as metrics\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "import sendgrid\n",
    "from sendgrid import SendGridAPIClient\n",
    "load_dotenv(\".env\")\n",
    "sg = sendgrid.SendGridAPIClient(os.environ.get('api_key'))\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "\n",
    "LOGFILE = \"logs/job1.log\"\n",
    "\n",
    "logger = logging.getLogger(\"dailylogs\")\n",
    "\n",
    "# set logging level : INFO, DEBUG, WARNING or ERROR\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# Create TimedRotatingFileHandler with log file name\n",
    "# It will create a new log file each day at midnight\n",
    "handler = TimedRotatingFileHandler(LOGFILE, when=\"midnight\", interval=1)\n",
    "\n",
    "# This is the format in which logs will be displayed in log file\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# assign the formatter and suffix to file_handler object\n",
    "# suffix will be added to each file\n",
    "handler.setFormatter(formatter)\n",
    "handler.suffix = \"%Y%m%d\"\n",
    "\n",
    "\n",
    "# add the handler to logger\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "logger.debug('New Job has started ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(message):\n",
    "\n",
    "    #stopwords\n",
    "    new_stopwords=['hi','hello','team','thanks','hey','regards','please','jira','com','ext','image','capture']\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    stpwrd.extend(new_stopwords)\n",
    "    # 1. Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    message=message.lower()\n",
    "    #removing the numerical values and working only with text values\n",
    "    message = re.sub('[^a-zA-Z]', \" \", message)\n",
    "    #lowering and removing punctuation\n",
    "    message = re.sub(r'[^\\w\\s]','',message)\n",
    "    #removing the stopwords\n",
    "    message = ' '.join([word for word in message.split() if word not in stpwrd and len(word)>1])\n",
    "    #lemmatizing the text\n",
    "    message =  \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation])\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated email notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_exception_1 = {\n",
    "    'personalizations': [\n",
    "        {\n",
    "    \"to\": [{\n",
    "            \"email\": \"amoham117@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"nbandi@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "         {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vbanap@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        }],\n",
    "            'subject': 'L1Triaging_MLModelAutoLearning Failure Notification'\n",
    "        }\n",
    "    ],\n",
    "    'from': {\n",
    "        'email': 'ia-coe-support-group@uber.com'\n",
    "    },\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>L1Triaging_MLModelAutoLearning automation has encountered an Error And the team Is currently analyzing the issue. We will follow up shortly With our Next steps For resolution And Let you know When the issue Is triaged.Please reach Out To ia-coe-support-group@uber.com For any questions Or concerns.<br><br>Error Message: Please provide corresponding Report3.xlsx with proper columns <br><br>Kindly check And rerun the process.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "  \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "message_exception_2 = {\n",
    "    'personalizations': [\n",
    "        {\n",
    "    \"to\": [{\n",
    "            \"email\": \"amoham117@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"nbandi@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "         {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vbanap@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        }],\n",
    "            'subject': 'L1Triaging_MLModelAutoLearning Failure Notification'\n",
    "        }\n",
    "    ],\n",
    "    'from': {\n",
    "        'email': 'ia-coe-support-group@uber.com'\n",
    "    },\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>L1Triaging_MLModelAutoLearning automation has encountered an Error And the team Is currently analyzing the issue. We will follow up shortly With our Next steps For resolution And Let you know When the issue Is triaged.Please reach Out To ia-coe-support-group@uber.com For any questions Or concerns.<br><br>Error Message: No file present in input location <br><br>Kindly check And rerun the process.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "\n",
    "                \n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cicd data import from box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "INPUT_FOLDER_ID = '170368566025'\n",
    "ARCHIVE_FOLDER_ID = '170364762149'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('12_12_2022_Report3.xlsx', '1088505411021')]\n"
     ]
    }
   ],
   "source": [
    "logger.debug('Read the file from BOX location... ')\n",
    "folder = client.folder(folder_id=INPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "# make a list of iten name and item id of all items\n",
    "files = [(item.name, item.id) for item in items]\n",
    "datetime_str = datetime.now().strftime(\"%d_%m_%Y\")\n",
    "#print(datetime_str)\n",
    "FILENAME = 'Report3.xlsx'\n",
    "FILENAME = datetime_str+\"_\"+FILENAME\n",
    "print(files)\n",
    "# sort it by filename\n",
    "files.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "try:\n",
    "    if len(files)>=1:\n",
    "        for i, f in enumerate(files):\n",
    "            # get id of the latest file (based on timestamp)\n",
    "            file_id = f[1]   \n",
    "           # print(f[0])\n",
    "            if f[0]==FILENAME:\n",
    "                # read the content and convert it into Pandas DataFrame\n",
    "                with open(FILENAME, 'wb') as open_file:\n",
    "                    client.file(file_id).download_to(open_file)\n",
    "                    open_file.close()\n",
    "                #data_cicd = pd.read_excel(io.StringIO(file_info.decode('utf-16')),usecols=['TicketSummary','TicketDescription','Corrected Component','Corrected Tier Type 1','Corrected Tier Type 2'])\n",
    "                data_cicd_latest = pd.read_excel(open(FILENAME, 'rb'),engine='openpyxl',usecols=['TicketSummary','TicketDescription','Corrected Component','Corrected Tier Type 1','Corrected Tier Type 2'])\n",
    "                logger.debug('CICD File {} has been downloaded from BOX with shape {}...'.format( f[0],data_cicd_latest.shape))\n",
    "            else:\n",
    "                logger.debug('Please provide {} with columns: {}'.format(FILENAME,data_cicd.columns))\n",
    "                response=sg.client.mail.send.post(request_body=message_exception_1)\n",
    "                \n",
    "    else:\n",
    "        logger.error('No file present in input location...')\n",
    "        response=sg.client.mail.send.post(request_body=message_exception_2)\n",
    "        \n",
    "except Exception as e:\n",
    "    print('kk')\n",
    "    logger.error('Exception details::{}'.format(e), exc_info=True)\n",
    "   ## response=sg.client.mail.send.post(request_body=message_exception_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cicd=pd.read_csv('./data/data_cicd_latest.csv',index_col=0)\n",
    "data_cicd=data_cicd.append(data_cicd_latest)\n",
    "logger.debug('Incremental CICD data has been added with shape {}'.format(data_cicd.shape))\n",
    "data_cicd.to_csv('./data/data_cicd_latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cicd_component=data_cicd.dropna(subset=['Corrected Component'])\n",
    "data_cicd_component['combined_text'] = data_cicd_component[['TicketSummary','TicketDescription']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_cicd_component['processed_text'] = data_cicd_component['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_cicd_component=data_cicd_component.drop_duplicates(subset=['processed_text','Corrected Component'],keep='first')\n",
    "data_cicd_component = data_cicd_component.reset_index(drop=True)\n",
    "X_cicd_component= data_cicd_component[['TicketSummary','TicketDescription','processed_text']]\n",
    "y_cicd_component= data_cicd_component['Corrected Component']\n",
    "logger.debug('Input data size of component: {}'.format(len(X_cicd_component)))\n",
    "data_cicd_tier_type_1=data_cicd.dropna(subset=['Corrected Tier Type 1'])\n",
    "data_cicd_tier_type_1['combined_text'] = data_cicd_tier_type_1[['TicketSummary','TicketDescription']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_cicd_tier_type_1['processed_text'] = data_cicd_tier_type_1['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_cicd_tier_type_1=data_cicd_tier_type_1.drop_duplicates(subset=['processed_text','Corrected Tier Type 1'],keep='first')\n",
    "data_cicd_tier_type_1 = data_cicd_tier_type_1.reset_index(drop=True)\n",
    "X_cicd_tier_type_1= data_cicd_tier_type_1[['TicketSummary','TicketDescription','processed_text']]\n",
    "y_cicd_tier_type_1= data_cicd_tier_type_1['Corrected Tier Type 1']\n",
    "logger.debug('Input data size of tier_type_1: {}'.format(len(X_cicd_tier_type_1)))\n",
    "data_cicd_tier_type_2=data_cicd.dropna(subset=['Corrected Tier Type 2'])\n",
    "data_cicd_tier_type_2['combined_text'] = data_cicd_tier_type_2[['TicketSummary','TicketDescription']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_cicd_tier_type_2['processed_text'] = data_cicd_tier_type_2['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_cicd_tier_type_2=data_cicd_tier_type_2.drop_duplicates(subset=['processed_text','Corrected Tier Type 2'],keep='first')\n",
    "data_cicd_tier_type_2 = data_cicd_tier_type_2.reset_index(drop=True)\n",
    "X_cicd_tier_type_2= data_cicd_tier_type_2[['TicketSummary','TicketDescription','processed_text']]\n",
    "y_cicd_tier_type_2= data_cicd_tier_type_2['Corrected Tier Type 2']\n",
    "logger.debug('Input data size of tier_type_2: {}'.format(len(X_cicd_tier_type_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input from historical data into dataframe\n",
    "df_1 = pd.read_csv('./data/historical_data.csv')\n",
    "df_1['TicketSummary']=df_1['TicketSummary'].astype(str)\n",
    "df_1['TicketDescription']=df_1['TicketDescription'].astype(str)\n",
    "#df_1.drop(['Unnamed: 0','Unnamed: 5','combined_text','processed_text'],axis=1)\n",
    "#choose sample data from entire data\n",
    "data_df = df_1.sample(frac=1, random_state=42)\n",
    "data_df['combined_text'] = data_df[['TicketSummary','TicketDescription']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "# apply data preprocessing steps on the prepared column\n",
    "data_df['processed_text'] = data_df['combined_text'].map(lambda s:preprocess_text(s)) \n",
    "data_df=data_df.drop_duplicates(subset=['processed_text','Corrected Component','Corrected Tier Type 1','Corrected Tier Type 2'],keep='first')\n",
    "#data_df.rename(columns={'Summary': 'TicketSummary', 'Description': 'TicketDescription','Updated Component': 'Corrected Component', 'Tier Type 1': 'Corrected Tier Type 1', 'Tier Type 2': 'Corrected Tier Type 2'}, inplace=True)\n",
    "#data_df.to_csv('./data/historical_data.csv')\n",
    "logger.debug('Input data size of historical data: {}'.format(data_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TicketSummary</th>\n",
       "      <th>TicketDescription</th>\n",
       "      <th>Corrected Component</th>\n",
       "      <th>Corrected Tier Type 1</th>\n",
       "      <th>Corrected Tier Type 2</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>13148</td>\n",
       "      <td>Difference in Historical Balance</td>\n",
       "      <td>Hi Team,\\n\\n\\n\\n\\nWe have observed that there ...</td>\n",
       "      <td>Oracle Payables</td>\n",
       "      <td>Oracle_ap</td>\n",
       "      <td>BLANK</td>\n",
       "      <td>Difference in Historical Balance Hi Team,\\n\\n\\...</td>\n",
       "      <td>difference historical balance observed change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13066</th>\n",
       "      <td>12042</td>\n",
       "      <td>Coupa Access</td>\n",
       "      <td>Hi Finance Support Team,\\n\\n\\nI work on the ch...</td>\n",
       "      <td>Access Requests</td>\n",
       "      <td>Coupa_access</td>\n",
       "      <td>Coupa_access_request</td>\n",
       "      <td>Coupa Access Hi Finance Support Team,\\n\\n\\nI w...</td>\n",
       "      <td>coupa access finance support work channel mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12354</th>\n",
       "      <td>15917</td>\n",
       "      <td>[OTM Oracle] Please add Uber Finance role to b...</td>\n",
       "      <td>Dears,\\n\\n\\nPlease add the UBER_FINANCE role t...</td>\n",
       "      <td>Oracle Transport Management</td>\n",
       "      <td>OTM</td>\n",
       "      <td>OTM</td>\n",
       "      <td>[OTM Oracle] Please add Uber Finance role to b...</td>\n",
       "      <td>otm oracle add uber finance role user dear add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37411</th>\n",
       "      <td>27649</td>\n",
       "      <td>Access for HSBC &amp; ICICI Bank statement</td>\n",
       "      <td>Hi Team,\\n\\n\\n\\nPlease could you provide acces...</td>\n",
       "      <td>Kyriba</td>\n",
       "      <td>Kyriba</td>\n",
       "      <td>Kyriba_report_access</td>\n",
       "      <td>Access for HSBC &amp; ICICI Bank statement Hi Team...</td>\n",
       "      <td>access hsbc icici bank statement could provide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24280</th>\n",
       "      <td>25266</td>\n",
       "      <td>Unable to find the approver name in Oralce.</td>\n",
       "      <td>Hi Team,\\n\\n\\nHope you're doing well!\\n\\n\\n\\nW...</td>\n",
       "      <td>Access Requests</td>\n",
       "      <td>Oracle_sysadmin</td>\n",
       "      <td>BLANK</td>\n",
       "      <td>Unable to find the approver name in Oralce. Hi...</td>\n",
       "      <td>unable find approver name oralce hope well abl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                      TicketSummary  \\\n",
       "216         13148                   Difference in Historical Balance   \n",
       "13066       12042                                       Coupa Access   \n",
       "12354       15917  [OTM Oracle] Please add Uber Finance role to b...   \n",
       "37411       27649             Access for HSBC & ICICI Bank statement   \n",
       "24280       25266        Unable to find the approver name in Oralce.   \n",
       "\n",
       "                                       TicketDescription  \\\n",
       "216    Hi Team,\\n\\n\\n\\n\\nWe have observed that there ...   \n",
       "13066  Hi Finance Support Team,\\n\\n\\nI work on the ch...   \n",
       "12354  Dears,\\n\\n\\nPlease add the UBER_FINANCE role t...   \n",
       "37411  Hi Team,\\n\\n\\n\\nPlease could you provide acces...   \n",
       "24280  Hi Team,\\n\\n\\nHope you're doing well!\\n\\n\\n\\nW...   \n",
       "\n",
       "               Corrected Component Corrected Tier Type 1  \\\n",
       "216                Oracle Payables             Oracle_ap   \n",
       "13066              Access Requests          Coupa_access   \n",
       "12354  Oracle Transport Management                   OTM   \n",
       "37411                       Kyriba                Kyriba   \n",
       "24280              Access Requests       Oracle_sysadmin   \n",
       "\n",
       "        Corrected Tier Type 2  \\\n",
       "216                     BLANK   \n",
       "13066  Coupa_access_request     \n",
       "12354                   OTM     \n",
       "37411  Kyriba_report_access     \n",
       "24280                   BLANK   \n",
       "\n",
       "                                           combined_text  \\\n",
       "216    Difference in Historical Balance Hi Team,\\n\\n\\...   \n",
       "13066  Coupa Access Hi Finance Support Team,\\n\\n\\nI w...   \n",
       "12354  [OTM Oracle] Please add Uber Finance role to b...   \n",
       "37411  Access for HSBC & ICICI Bank statement Hi Team...   \n",
       "24280  Unable to find the approver name in Oralce. Hi...   \n",
       "\n",
       "                                          processed_text  \n",
       "216    difference historical balance observed change ...  \n",
       "13066  coupa access finance support work channel mark...  \n",
       "12354  otm oracle add uber finance role user dear add...  \n",
       "37411  access hsbc icici bank statement could provide...  \n",
       "24280  unable find approver name oralce hope well abl...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df[['TicketSummary','TicketDescription','processed_text']]\n",
    "y_component= data_df['Corrected Component']\n",
    "y_tier_type_1= data_df['Corrected Tier Type 1']\n",
    "y_tier_type_2= data_df['Corrected Tier Type 2']\n",
    "# split the cicd data into train and test \n",
    "X_train_component, X_test_component, y_train_component, y_test_component = train_test_split(X, y_component, test_size = .20, random_state = 42)\n",
    "X_train_tier_type_1, X_test_tier_type_1, y_train_tier_type_1, y_test_tier_type_1 = train_test_split(X, y_tier_type_1, test_size = .20, random_state = 42)\n",
    "X_train_tier_type_2, X_test_tier_type_2, y_train_tier_type_2, y_test_tier_type_2 = train_test_split(X, y_tier_type_2, test_size = .20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# component training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final_component = X_train_component.append(X_cicd_component)\n",
    "X_test_final_component = X_test_component\n",
    "y_train_final_component = y_train_component.append(y_cicd_component)\n",
    "y_test_final_component = y_test_component\n",
    "logger.debug('Training data size of component: {} {}'.format(len(X_train_final_component),len(y_train_final_component)))\n",
    "logger.debug('Test data size of component: {} {}'.format(len(X_test_final_component),len(y_test_final_component)))\n",
    "model_component= Pipeline([('vect', CountVectorizer(max_df=0.80)),\n",
    "       ('tfidf', TfidfTransformer()),\n",
    "       #('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "       ('mnb', MultinomialNB(fit_prior=False,alpha=0.05))])\n",
    "# perform model training\n",
    "model_component.fit(X_train_final_component['processed_text'].values, y_train_final_component.values)\n",
    "logger.debug('Model fit successfully completed for component model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{'accuracy': 0.7516113630938172, 'precision_score': 0.7774055232286262, 'recall_score': 0.7516113630938172, 'f1_score': 0.7605012122257254}\"]\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "result1=X_test_final_component\n",
    "y_pred = model_component.predict(X_test_final_component['processed_text'].values)\n",
    "\n",
    "result1['original_cat_component']= y_test_final_component\n",
    "result1['predicted_cat_component'] = y_pred\n",
    "\n",
    "result1['prediction_cat_confscore_component'] = np.round_(np.max(model_component.predict_proba(X_test_final_component['processed_text'].values), axis=1), decimals=2)\n",
    "\n",
    "#\n",
    "output = {'accuracy': accuracy_score(y_pred,y_test_final_component),'precision_score':precision_score(y_pred,y_test_final_component,average='weighted'),'recall_score':recall_score(y_pred,y_test_final_component,average='weighted')\n",
    ",'f1_score':f1_score(y_pred,y_test_final_component,average='weighted')}\n",
    "\n",
    "result1['confusion_matrix'] = str(output)\n",
    "print(result1['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33801, 3), (8378, 7))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final_component.shape,X_test_final_component.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('confusion_matrix for component:'+result1['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 1 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final_tier_type_1 = X_train_tier_type_1.append(X_cicd_tier_type_1)\n",
    "X_test_final_tier_type_1 = X_test_tier_type_1\n",
    "y_train_final_tier_type_1 = y_train_tier_type_1.append(y_cicd_tier_type_1)\n",
    "y_test_final_tier_type_1 = y_test_tier_type_1\n",
    "logger.debug('Training data size of tier type 1: {} {}'.format(len(X_train_final_tier_type_1),len(y_train_final_tier_type_1)))\n",
    "logger.debug('Test data size of tier type 1: {} {}'.format(len(X_test_final_tier_type_1),len(y_test_final_tier_type_1)))\n",
    "model_tier_type_1= Pipeline([('vect', CountVectorizer(max_df=0.80)),\n",
    "       ('tfidf', TfidfTransformer()),\n",
    "       #('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "       ('mnb', MultinomialNB(fit_prior=False,alpha=0.05))])\n",
    "# perform model training\n",
    "model_tier_type_1.fit(X_train_final_tier_type_1['processed_text'].values, y_train_final_tier_type_1.values)\n",
    "logger.debug('Model fit successfully completed for tier type 1 model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{'accuracy': 0.7346622105514442, 'precision_score': 0.7752703923594014, 'recall_score': 0.7346622105514442, 'f1_score': 0.7493648837351858}\"]\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "result2=X_test_final_tier_type_1\n",
    "y_pred = model_tier_type_1.predict(X_test_final_tier_type_1['processed_text'].values)\n",
    "\n",
    "result2['original_cat_tier_type_1']= y_test_final_tier_type_1\n",
    "result2['predicted_cat_tier_type_1'] = y_pred\n",
    "\n",
    "result2['prediction_cat_confscore_tier_type_1'] = np.round_(np.max(model_tier_type_1.predict_proba(X_test_final_tier_type_1['processed_text'].values), axis=1), decimals=2)\n",
    "\n",
    "#\n",
    "output = {'accuracy': accuracy_score(y_pred,y_test_final_tier_type_1),'precision_score':precision_score(y_pred,y_test_final_tier_type_1,average='weighted'),'recall_score':recall_score(y_pred,y_test_final_tier_type_1,average='weighted')\n",
    ",'f1_score':f1_score(y_pred,y_test_final_tier_type_1,average='weighted')}\n",
    "\n",
    "result2['confusion_matrix'] = str(output)\n",
    "print(result2['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('confusion_matrix for tier type 1:'+result2['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 2 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final_tier_type_2 = X_train_tier_type_2.append(X_cicd_tier_type_2)\n",
    "X_test_final_tier_type_2 = X_test_tier_type_2\n",
    "y_train_final_tier_type_2 = y_train_tier_type_2.append(y_cicd_tier_type_2)\n",
    "y_test_final_tier_type_2 = y_test_tier_type_2\n",
    "logger.debug('Training data size of tier type 2: {} {}'.format(len(X_train_final_tier_type_2),len(y_train_final_tier_type_2)))\n",
    "logger.debug('Test data size of tier type 2: {} {}'.format(len(X_test_final_tier_type_2),len(y_test_final_tier_type_2)))\n",
    "model_tier_type_2= Pipeline([('vect', CountVectorizer(max_df=0.80)),\n",
    "       ('tfidf', TfidfTransformer()),\n",
    "       #('clf', RandomForestClassifier(class_weight='balanced'))])\n",
    "       ('mnb', MultinomialNB(fit_prior=False,alpha=0.05))])\n",
    "# perform model training\n",
    "model_tier_type_2.fit(X_train_final_tier_type_2['processed_text'].values, y_train_final_tier_type_2.values)\n",
    "logger.debug('Model fit successfully completed for tier type 2 model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{'accuracy': 0.6008593936500358, 'precision_score': 0.6955837668608098, 'recall_score': 0.6008593936500358, 'f2_score': 0.6336917346533826}\"]\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "result3=X_test_final_tier_type_2\n",
    "y_pred = model_tier_type_2.predict(X_test_final_tier_type_2['processed_text'].values)\n",
    "\n",
    "result3['original_cat_tier_type_2']= y_test_final_tier_type_2\n",
    "result3['predicted_cat_tier_type_2'] = y_pred\n",
    "\n",
    "result3['prediction_cat_confscore_tier_type_2'] = np.round_(np.max(model_tier_type_2.predict_proba(X_test_final_tier_type_2['processed_text'].values), axis=1), decimals=2)\n",
    "\n",
    "#\n",
    "output = {'accuracy': accuracy_score(y_pred,y_test_final_tier_type_2),'precision_score':precision_score(y_pred,y_test_final_tier_type_2,average='weighted'),'recall_score':recall_score(y_pred,y_test_final_tier_type_2,average='weighted')\n",
    ",'f2_score':f1_score(y_pred,y_test_final_tier_type_2,average='weighted')}\n",
    "\n",
    "result3['confusion_matrix'] = str(output)\n",
    "print(result3['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('confusion_matrix for tier type 2:'+result3['confusion_matrix'] [5:6].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import datetime\n",
    "model_path_component='./model/component'\n",
    "model_path_tier_type_1='./model/tier_type_1'\n",
    "model_path_tier_type_2='./model/tier_type_2'\n",
    "# save the model to disk\n",
    "filename_primary= 'finalized_model.sav'\n",
    "component_path = os.path.join(model_path_component, filename_primary) \n",
    "tier_type_1_path = os.path.join(model_path_tier_type_1, filename_primary) \n",
    "tier_type_2_path = os.path.join(model_path_tier_type_2, filename_primary) \n",
    "pickle.dump(model_component, open(component_path, 'wb'))\n",
    "pickle.dump(model_tier_type_1, open(tier_type_1_path, 'wb'))\n",
    "pickle.dump(model_tier_type_2, open(tier_type_2_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33801, 3) (8378, 7)\n",
      "(33848, 2) (8378, 2)\n",
      "(33930, 2) (8378, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train_component_data=X_train_final_component.drop(['processed_text'],axis=1)\n",
    "X_test_component_data=X_test_final_component.drop(['processed_text','original_cat_component','predicted_cat_component','prediction_cat_confscore_component','confusion_matrix'],axis=1)\n",
    "print(X_train_final_component.shape,X_test_final_component.shape)\n",
    "X_train_component_data['component']=y_train_final_component\n",
    "X_train_component_data['label']='train'\n",
    "X_test_component_data['component']=y_test_final_component\n",
    "X_test_component_data['label']='test'\n",
    "component_train_test=X_train_component_data.append(X_test_component_data)\n",
    "\n",
    "X_train_tier_type_1_data=X_train_final_tier_type_1.drop(['processed_text'],axis=1)\n",
    "X_test_tier_type_1_data=X_test_final_tier_type_1.drop(['processed_text','original_cat_tier_type_1','predicted_cat_tier_type_1','prediction_cat_confscore_tier_type_1','confusion_matrix'],axis=1)\n",
    "print(X_train_tier_type_1_data.shape,X_test_tier_type_1_data.shape)\n",
    "X_train_tier_type_1_data['component']=y_train_final_tier_type_1\n",
    "X_train_tier_type_1_data['label']='train'\n",
    "X_test_tier_type_1_data['component']=y_test_final_tier_type_1\n",
    "X_test_tier_type_1_data['label']='test'\n",
    "tier_type_1_train_test=X_train_tier_type_1_data.append(X_test_tier_type_1_data)\n",
    "\n",
    "\n",
    "X_train_tier_type_2_data=X_train_final_tier_type_2.drop(['processed_text'],axis=1)\n",
    "X_test_tier_type_2_data=X_test_final_tier_type_2.drop(['processed_text','original_cat_tier_type_2','predicted_cat_tier_type_2','prediction_cat_confscore_tier_type_2','confusion_matrix'],axis=1)\n",
    "print(X_train_tier_type_2_data.shape,X_test_tier_type_2_data.shape)\n",
    "X_train_tier_type_2_data['component']=y_train_final_tier_type_2\n",
    "X_train_tier_type_2_data['label']='train'\n",
    "X_test_tier_type_2_data['component']=y_test_final_tier_type_2\n",
    "X_test_tier_type_2_data['label']='test'\n",
    "tier_type_2_train_test=X_train_tier_type_2_data.append(X_test_tier_type_2_data)\n",
    "\n",
    "data_path_component='./data/component'\n",
    "data_path_tier_type_1='./data/tier_type_1'\n",
    "data_path_tier_type_2='./data/tier_type_2'\n",
    "filename_primary_data= 'dftraintest.csv'\n",
    "\n",
    "component_path_data = os.path.join(data_path_component, filename_primary_data) \n",
    "tier_type_1_path_data = os.path.join(data_path_tier_type_1, filename_primary_data) \n",
    "tier_type_2_path_data = os.path.join(data_path_tier_type_2, filename_primary_data) \n",
    "\n",
    "component_train_test.to_csv(component_path_data)\n",
    "tier_type_1_train_test.to_csv(tier_type_1_path_data)\n",
    "tier_type_2_train_test.to_csv(tier_type_2_path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# component model upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"finalized_model.sav\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "#FOLDER_ID = '164616436488'\n",
    "OUTPUT_FOLDER_ID = '170542616761'\n",
    "\n",
    "# Create Filename based on timestamp\n",
    "FILENAME = 'finalized_model.sav'\n",
    "#datetime_str = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
    "# print(datetime_str)\n",
    "\n",
    "#FILENAME = FILENAME + \"_\" + datetime_str+\".csv\"\n",
    "\n",
    "#print(FILENAME)\n",
    "\n",
    "\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading component model in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(component_path)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        print(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(component_path)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# component data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"dftraintest.csv\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "OUTPUT_FOLDER_ID = '170542616761'\n",
    "# Create Filename based on timestamp\n",
    "FILENAME ='dftraintest.csv'\n",
    "\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading component data in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(component_path_data)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        print(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(component_path_data)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 1 model upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"finalized_model.sav\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "#FOLDER_ID = '164616436488'\n",
    "OUTPUT_FOLDER_ID = '172902610098'\n",
    "# Create Filename based on timestamp\n",
    "FILENAME = 'finalized_model.sav'\n",
    "#datetime_str = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
    "# print(datetime_str)\n",
    "#FILENAME = FILENAME + \"_\" + datetime_str+\".csv\"\n",
    "#print(FILENAME)\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading tier type 1 model in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(tier_type_1_path)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        print(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(tier_type_1_path)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 1 data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"dftraintest.csv\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "OUTPUT_FOLDER_ID = '172902610098'\n",
    "# Create Filename based on timestamp\n",
    "FILENAME ='dftraintest.csv'\n",
    "\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading tier type 1 data in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(tier_type_1_path_data)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(tier_type_1_path_data)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 2 model upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"finalized_model.sav\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "#FOLDER_ID = '164616436488'\n",
    "OUTPUT_FOLDER_ID = '172905132682'\n",
    "# Create Filename based on timestamp\n",
    "FILENAME = 'finalized_model.sav'\n",
    "#datetime_str = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n",
    "# print(datetime_str)\n",
    "#FILENAME = FILENAME + \"_\" + datetime_str+\".csv\"\n",
    "#print(FILENAME)\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading tier type 2 model in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(tier_type_2_path)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        print(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(tier_type_2_path)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tier type 2 data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"dftraintest.csv\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "OUTPUT_FOLDER_ID = '172905132682'\n",
    "# Create Filename based on timestamp\n",
    "FILENAME ='dftraintest.csv'\n",
    "\n",
    "# establish a connection for BOX\n",
    "config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "client = Client(config)\n",
    "# fetch all the items from the folder\n",
    "folder = client.folder(folder_id=OUTPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "flag = False\n",
    "\n",
    "logger.debug('Uploading tier type 2 data in BOX....')\n",
    "\n",
    "for item in items:\n",
    "    # if file name exists, update the content\n",
    "    if item.name == FILENAME:\n",
    "        chunked_uploader = client.file(item.id).get_chunked_uploader(tier_type_2_path_data)\n",
    "        updated_file = chunked_uploader.start()\n",
    "        print(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        logger.debug(f'File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "        flag = True\n",
    "        break\n",
    "\n",
    "# if file doesn not exists upload the file\n",
    "if not flag:\n",
    "    uploaded_file = folder.upload(tier_type_2_path_data)\n",
    "    print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "    logger.debug(f'File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_params = {\n",
    "  'mnb__alpha': [0.01,0.05,0.1,0.2,0.3,0.4,0.5],\n",
    "}\n",
    "clf = GridSearchCV(model_component, grid_params,n_jobs=-1,verbose=1)\n",
    "clf.fit(X_train['processed_text'].values, y_train.values)\n",
    "print(\"Best Score: \", clf.best_score_)\n",
    "print(\"Best Params: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cumulative logging report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER_ID='172905132682'\n",
    "logger.debug('Read the logging file from BOX location... ')\n",
    "\n",
    "folder = client.folder(folder_id=INPUT_FOLDER_ID)\n",
    "items = folder.get_items()\n",
    "# make a list of iten name and item id of all items\n",
    "files = [(item.name, item.id) for item in items]\n",
    "\n",
    "# sort it by filename\n",
    "files.sort(reverse=True, key=lambda x: x[0])\n",
    "#print(files)\n",
    "try:\n",
    "    for i, f in enumerate(files):\n",
    "        # get id of the latest file (based on timestamp)\n",
    "        if f[0]=='l1triage_model_classifier_loggging.csv':\n",
    "            file_id = f[1]\n",
    "            file_info = client.file(file_id).content()\n",
    "            report = pd.read_csv(io.StringIO(file_info.decode('utf-8')),index_col=0)\n",
    "        else:\n",
    "            report=pd.DataFrame()\n",
    "            \n",
    "    logger.debug('logging file  present...')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    logger.error('Getting no new logging file....', exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_component={'Model_info':'component',\n",
    "                   'Model_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model\\final_model',\n",
    "                   'Lob':'L1_triage',\n",
    "                   'Hyperparameters_used':model_component.steps,\n",
    "                   'Validation_accuracy':result1['confusion_matrix'] [5:6].values,\n",
    "                   'Model_status_flag':'active',\n",
    "                   'Train_test_data_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model\\final_model\\Data\\df_traintestdata.csv',\n",
    "                   'Deployed_date':pd.Timestamp(\"today\").strftime(\"%d-%b-%Y\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_tier_type_1={'Model_info':'tier type 1',\n",
    "                     'Model_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model2\\final_model',\n",
    "                     'Lob':'L1_triage',\n",
    "                     'Hyperparameters_used':model_tier_type_1.steps,\n",
    "                     'Validation_accuracy':result2['confusion_matrix'] [5:6].values,\n",
    "                     'Model_status_flag':'active',\n",
    "                     'Train_test_data_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model2\\final_model\\Data\\df_traintestdata.csv',\n",
    "                     'Deployed_date':pd.Timestamp(\"today\").strftime(\"%d-%b-%Y\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_tier_type_2={'Model_info':'tier type 2',\n",
    "                     'Model_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model3\\final_model',\n",
    "                     'Lob':'L1_triage',\n",
    "                     'Hyperparameters_used':model_tier_type_2.steps,\n",
    "                     'Validation_accuracy':result3['confusion_matrix'] [5:6].values,\n",
    "                     'Model_status_flag':'active',\n",
    "                     'Train_test_data_path':r'\\\\corp.uber.com\\rpa\\UiPath\\Dev\\IACOE OCR\\models\\L1Triage\\Model3\\final_model\\Data\\df_traintestdata.csv',\n",
    "                     'Deployed_date':pd.Timestamp(\"today\").strftime(\"%d-%b-%Y\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append row to the dataframe\n",
    "report = report.append(new_row_component, ignore_index=True)\n",
    "report = report.append(new_row_tier_type_1, ignore_index=True)\n",
    "report = report.append(new_row_tier_type_2, ignore_index=True)\n",
    "report=report.loc[report.astype(str).drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"l1triage_model_classifier_loggging.csv\" has been uploaded\n",
      "File \"l1triage_model_classifier_loggging.csv\" has been uploaded\n",
      "File \"l1triage_model_classifier_loggging.csv\" has been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Folder ID to upload the data\n",
    "OUTPUT_FOLDER_ID=['170542616761','172902610098','172905132682']\n",
    "# Create Filename based on timestamp\n",
    "FILENAME ='l1triage_model_classifier_loggging.csv'\n",
    "logging_path='./data'\n",
    "logging_path_data = os.path.join(logging_path, FILENAME) \n",
    "report.to_csv(logging_path_data)\n",
    "for i in OUTPUT_FOLDER_ID:\n",
    "    config = JWTAuth.from_settings_file('Tax_Auto_Config.json')\n",
    "    client = Client(config)\n",
    "    # fetch all the items from the folder\n",
    "    folder = client.folder(folder_id=i)\n",
    "    items = folder.get_items()\n",
    "    flag = False\n",
    "\n",
    "    for item in items:\n",
    "        # if file name exists, update the content\n",
    "        if item.name == FILENAME:\n",
    "\n",
    "            updated_file = client.file(item.id).update_contents(logging_path_data)\n",
    "            print('File \"{0}\" has been updated'.format(updated_file.name))\n",
    "            logger.debug('File \"{0}\" has been updated in BOX...'.format(updated_file.name))\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    # if file doesn not exists upload the file\n",
    "    if not flag:\n",
    "        uploaded_file = folder.upload(logging_path_data)\n",
    "        print('File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "        logger.debug('File \"{0}\" has been uploaded in BOX...'.format(uploaded_file.name))\n",
    "logger.debug('Retraining completed sucessfully...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated email notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "email sent\n"
     ]
    }
   ],
   "source": [
    "# datetime object containing current date and time\n",
    "from datetime import datetime    \n",
    "import pytz    \n",
    "tz_NY = pytz.timezone('Asia/Kolkata')   \n",
    "datetime_NY = datetime.now(tz_NY)  \n",
    "ist=datetime_NY.strftime(\"%Y-%m-%d %H:%M:%S\") \n",
    "message_success = {\"personalizations\": [\n",
    "    {\n",
    "    \"to\": [{\n",
    "            \"email\": \"amoham117@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"nbandi@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "         {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vbanap@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        }],\n",
    "    \"subject\": \"L1Triaging ML CICD automation successfull\"\n",
    "    }\n",
    "],\n",
    "\"from\": {\n",
    "    \"email\": \"ia-coe-support-group@uber.com\"\n",
    "},\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>The L1triage Automation has been run successfully at \"+str(ist)+\" IST.<br><br>The output reports are ready for use https://uber.app.box.com/folder/170368651516.<br><br>You can reach out to ia-coe-support-group@uber.com for any questions or concerns.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "\n",
    "        }\n",
    "]\n",
    "}\n",
    "try:\n",
    "    response = sg.client.mail.send.post(request_body=message_success)\n",
    "    print(response.status_code)\n",
    "    logger.debug('L1 traige automated retraining email notification has been sent successfully...')\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    logger.debug('Exception details::{}'.format(e))\n",
    "if response.status_code == 202:\n",
    "    print (\"email sent\")\n",
    "else:\n",
    "    print(\"Checking body\",response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "interval=time.strftime('%H:%M:%S', time.gmtime(end_time-start_time))\n",
    "logger.debug('Total program execution time: {}\\n\\n\\n\\n'.format(interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
