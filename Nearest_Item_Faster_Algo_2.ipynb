{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: faiss-gpu in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (1.7.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: sentence-transformers in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (3.6.7)\n",
      "Requirement already satisfied: scipy in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (4.18.0)\n",
      "Requirement already satisfied: tqdm in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: torchvision in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.11.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: numpy in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: sentencepiece in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: pyyaml in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: dataclasses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
      "Requirement already satisfied: importlib-resources in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tqdm->sentence-transformers) (5.4.0)\n",
      "Requirement already satisfied: click in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from nltk->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from torchvision->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: six in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Collecting scikit-learn==0.20.3\n",
      "  Using cached https://pypi.uberinternal.com/packages/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from scikit-learn==0.20.3) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from scikit-learn==0.20.3) (1.5.3)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.0\n",
      "    Uninstalling scikit-learn-0.21.0:\n",
      "      Successfully uninstalled scikit-learn-0.21.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyenv3 0.1.0 requires scikit-learn==0.18.1, but you have scikit-learn 0.20.3 which is incompatible.\n",
      "pyenv3 0.1.0 requires torch==1.2.0, but you have torch 1.10.1 which is incompatible.\n",
      "pyenv3 0.1.0 requires torchvision==0.4.0, but you have torchvision 0.11.2 which is incompatible.\u001B[0m\n",
      "Successfully installed scikit-learn-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import exists\n",
    "import pickle\n",
    "\n",
    "filePathPickle = \"../itemNames_embedding.pickle\" #Either This comes as an input, or\n",
    "filePathItemNames = '../Items_Jan_Apr2022_English_68000000_Lemmatized_Names.csv' # this comes as input\n",
    "\n",
    "file_exists = exists(filePathPickle)\n",
    "\n",
    "if (file_exists):\n",
    "    with open('../itemNames_embedding.pickle', 'rb') as pkl:\n",
    "        item_embeddings = pickle.load(pkl)\n",
    "else:\n",
    "    with open(filePathItemNames) as f:\n",
    "        itemNames = [line.rstrip() for line in f]\n",
    "    itemModel = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    item_embeddings = itemModel.encode(itemNames)\n",
    "    pickle.dump (item_embeddings, open('itemNames_embedding.pickle', 'wb'))\n",
    "\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Items_Jan_Apr2022_English_68000000_Lemmatized_Names.csv') as f:\n",
    "    itemNames = [line.rstrip() for line in f]\n",
    "\n",
    "itemModel = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../itemNames_embedding.pickle', 'rb') as pkl:\n",
    "    item_embeddings = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2214494, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = item_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214494"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_index.is_trained\n",
    "\n",
    "item_index.add(item_embeddings)\n",
    "\n",
    "item_index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearestItems (random_sample, k, simThreshold, simCountThreshold, output_file) :\n",
    "    covered_items = set()\n",
    "    for i in random_sample:\n",
    "        xq = item_embeddings[i]\n",
    "        xq = np.reshape (xq, (1, 768))\n",
    "        D, I = item_index.search(xq, k)\n",
    "        #Change to getNearestItemsSimScore2 to run the new code.. Remove 2 for the old code..\n",
    "        itemsCovered = getNearestItemsSimScore2 (I, simThreshold, simCountThreshold, output_file)\n",
    "        covered_items = covered_items | itemsCovered\n",
    "        \n",
    "    return covered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#unlike the first part, we produce only one line \n",
    "id_output_file = open(\"cluster_id_item\",\"w\")\n",
    "used_clusers = set()\n",
    "start = 0\n",
    "\n",
    "\n",
    "def getNearestItemsSimScore2 (I, simThreshold, simCountThreshold, output_file):\n",
    "    counter = 0\n",
    "    counter_Neighbors = 0\n",
    "    sim_items_set = set()\n",
    "    sim_items_index = set()\n",
    "    for i in I[0]:\n",
    "        #print (\"counter_Neighbors=\" + str (counter_Neighbors) + \", Sim Items set size:\" + str (len (sim_items_set)))\n",
    "        counter_Neighbors += 1\n",
    "        first_item = item_embeddings[i]\n",
    "        #first_item = np.reshape (first_item, (1, 768))\n",
    "        counter = 0\n",
    "        for j in I[0]:\n",
    "            second_item = item_embeddings[j]\n",
    "            #second_item = np.reshape (second_item, (1, 768))\n",
    "            if cosine (first_item, second_item) >= simThreshold:\n",
    "                counter += 1\n",
    "                #sim_items_set.add(itemNames[j])\n",
    "                if counter >= simCountThreshold :\n",
    "                    sim_items_set.add(itemNames[i])\n",
    "                    sim_items_index.add(i)\n",
    "                    break\n",
    "\n",
    "    itemsAsStr = \"###\".join(sim_items_set)\n",
    "    output_file.write(itemsAsStr + \"&&&&&\" + str (len(sim_items_set)))\n",
    "    output_file.write (\"\\n\")\n",
    "    #itemsCovered.add(i)\n",
    "    while start in used_clusers:\n",
    "        start = start + 1\n",
    "    id_output_file.write(str(start)+ \"---->\"+itemAsStr)\n",
    "\n",
    "    print (\"Sim items set size:\" + str (len (sim_items_set)))\n",
    "            \n",
    "    return sim_items_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "num = math.floor(0.00005*item_embeddings.shape[0]) ##We pick 0.5% of items\n",
    "\n",
    "uncovered_items_set = set (range(0,len (itemNames))) ##All items\n",
    "\n",
    "covered_items_set = set() ##Initialized to be an empty set\n",
    "\n",
    "output_file = open(\"nearestItems_using_neighbors_full_new_3.txt\", \"a\")\n",
    "\n",
    "k = 300\n",
    "simThreshold = 0.8\n",
    "simCountThreshold = 20\n",
    "\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug\n",
    "counter = 0\n",
    "while len (covered_items_set) < math.floor (0.8*len(itemNames)) : # len(uncovered_items_set) > math.floor (0.05*len(itemNames)): ##till 95% of all items are covered\n",
    "    random_sample = getRandomSample (uncovered_items_set, num)\n",
    "    random_sample = random_sample - covered_items_set\n",
    "    #print (random_sample)\n",
    "    covered_items_set = covered_items_set | random_sample\n",
    "    print (\"Covered item set before:\" + str (len (covered_items_set)))\n",
    "    print (\"Uncovered item set size before:\" + str (len (uncovered_items_set)))\n",
    "    covered_items_set = covered_items_set | getNearestItems (random_sample, k, simThreshold, simCountThreshold, output_file)\n",
    "    uncovered_items_set = uncovered_items_set - covered_items_set #= [i for i in uncovered_items_list if i not in covered_items_list]\n",
    "    print (\"Covered item set after:\" + str (len (covered_items_set)))\n",
    "    print (\"Uncovered item set size after:\" + str (len (uncovered_items_set)))\n",
    "    random_sample.clear\n",
    "    counter += 1\n",
    "    print (\"############################# Counter:\" + str (counter))\n",
    "\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
