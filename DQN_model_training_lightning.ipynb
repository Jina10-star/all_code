{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install dataclasses\n",
    "# %pip install matching-ds-tools\n",
    "# %pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from queryrunner_client import Client\n",
    "USER_EMAIL = 'ssadeghi@uber.com'\n",
    "qclient = Client(user_email=USER_EMAIL)\n",
    "CONSUMER_NAME = 'intelligentdispatch'\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#num_cores = multiprocessing.cpu_count()  -- 48\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from queryrunner_client import Client as QRClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import mdstk\n",
    "from mdstk.data_fetcher.data_fetcher import DataFetcher\n",
    "from mdstk.data_fetcher.cached_data_fetcher import CachedDataFetcher\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import h3\n",
    "import h3pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCASE 1: San Francisco\\ncity_id = 1\\nvvid = 8\\nMAX_LAT = 38.19\\nMIN_LAT = 37.09\\nMAX_LNG = -121.55\\nMIN_LNG = -122.60\\nLAT_CENTER = 37.6\\nLON_CENTER = -122.2\\n\\n\\nCASE 2: Detroit\\ncity_id = 50\\nvvid = 425\\nMAX_LAT = 42.89\\nMIN_LAT = 42.01\\nMAX_LNG = -82.68\\nMIN_LNG = -83.93\\nLAT_CENTER = 42.420149389121406\\nLON_CENTER = -83.15996619595755\\n\\n\\nCase 3: Philadelphia\\ncity_id = 20\\nvvid = 663\\nMAX_LAT = 40.22\\nMIN_LAT = 39.74\\nMAX_LNG = -75.46\\nMIN_LNG = -74.86\\nLAT_CENTER = 39.95201837418434\\nLON_CENTER = -75.15727285438611\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CASE 1: San Francisco\n",
    "city_id = 1\n",
    "vvid = 8\n",
    "MAX_LAT = 38.19\n",
    "MIN_LAT = 37.09\n",
    "MAX_LNG = -121.55\n",
    "MIN_LNG = -122.60\n",
    "LAT_CENTER = 37.6\n",
    "LON_CENTER = -122.2\n",
    "\n",
    "\n",
    "CASE 2: Detroit\n",
    "city_id = 50\n",
    "vvid = 425\n",
    "MAX_LAT = 42.89\n",
    "MIN_LAT = 42.01\n",
    "MAX_LNG = -82.68\n",
    "MIN_LNG = -83.93\n",
    "LAT_CENTER = 42.420149389121406\n",
    "LON_CENTER = -83.15996619595755\n",
    "\n",
    "\n",
    "Case 3: Philadelphia\n",
    "city_id = 20\n",
    "vvid = 663\n",
    "MAX_LAT = 40.22\n",
    "MIN_LAT = 39.74\n",
    "MAX_LNG = -75.46\n",
    "MIN_LNG = -74.86\n",
    "LAT_CENTER = 39.95201837418434\n",
    "LON_CENTER = -75.15727285438611\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS\n",
    "\n",
    "prefix = 'unifiedQ'\n",
    "\n",
    "city_id = 20\n",
    "vvid = 663\n",
    "MAX_LAT = 40.22\n",
    "MIN_LAT = 39.74\n",
    "MAX_LNG = -74.86\n",
    "MIN_LNG = -75.46\n",
    "LAT_CENTER = 39.95201837418434\n",
    "LON_CENTER = -75.15727285438611\n",
    "        \n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "    '2022-11-18',\n",
    "    '2022-11-20',\n",
    "    '2022-11-21',\n",
    "    '2022-11-22',\n",
    "    '2022-11-23',\n",
    "    '2022-11-24',\n",
    "    '2022-11-25',\n",
    "    '2022-11-26',\n",
    "    '2022-11-27',\n",
    "    '2022-11-28',\n",
    "    '2022-11-29',\n",
    "    '2022-11-30',\n",
    "    '2022-12-01',\n",
    "    '2022-12-02',\n",
    "    '2022-12-03',\n",
    "    '2022-12-04',\n",
    "    '2022-12-05',\n",
    "    '2022-12-06',\n",
    "    '2022-12-07',\n",
    "    '2022-12-08',\n",
    "]\n",
    "\n",
    "\n",
    "## ML TRAINING\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "ITERATIONS = 500000\n",
    "DISCOUNT = 0.995\n",
    "VALUE_DEGRADE_LEVEL = 0.85\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_duration_seconds = np.log(VALUE_DEGRADE_LEVEL) / np.log(DISCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "QUERY = \"\"\"\n",
    "WITH\n",
    "completed as (\n",
    "SELECT\n",
    "  ft.datestr as datestr,\n",
    "  ft.city_id as city_id, \n",
    "  ft.trip_uuid as trip_uuid,\n",
    "  ft.session_uuid as session_uuid,\n",
    "  ft.driver_uuid as driver_uuid,\n",
    "  ft.request_timestamp_local as local_time,\n",
    "  ft.eta as eta,\n",
    "  ft.fare as fare,\n",
    "  ft.duration_min as duration_min,\n",
    "  --   Driver origin information\n",
    "  mez.driver_origin_lat as driver_origin_lat,\n",
    "  mez.driver_origin_lng as driver_origin_lng,\n",
    "  mez.pickup_lat as pickup_lat,\n",
    "  mez.pickup_lng as pickup_lng,\n",
    "  mez.dropoff_lat as dropoff_lat,\n",
    "  mez.dropoff_lng as dropoff_lng\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      base.uuid as uuid,\n",
    "      base.accepted_lat as driver_origin_lat,\n",
    "      base.accepted_lng as driver_origin_lng,\n",
    "      base.begintrip_lat as pickup_lat,\n",
    "      base.begintrip_lng as pickup_lng,\n",
    "      base.dropoff_lat as dropoff_lat,\n",
    "      base.dropoff_lng as dropoff_lng\n",
    "    FROM\n",
    "      rawdata.schemaless_mezzanine_trips_rows\n",
    "    WHERE\n",
    "      datestr = '{datestr}'\n",
    "      and base.city_id = {city_id}\n",
    "      and LOWER(base.status) = 'completed'\n",
    "  ) mez\n",
    "  JOIN (\n",
    "    SELECT\n",
    "      --Request Information\n",
    "      datestr,\n",
    "      city_id,\n",
    "      uuid as trip_uuid,\n",
    "      session_id as session_uuid,\n",
    "      driver_uuid,\n",
    "      --Time Information,\n",
    "      request_timestamp_local,\n",
    "      request_timestamp_utc,\n",
    "      eta,\n",
    "      client_upfront_fare_usd as fare,\n",
    "      --Distances and duration of the request,\n",
    "      trip_duration_seconds / 60 as duration_min\n",
    "    FROM\n",
    "      restricted_dwh.fact_trip\n",
    "    WHERE\n",
    "      lower(global_product_name) = 'uberx'\n",
    "      and lower(status) = 'completed'\n",
    "      and city_id = {city_id}\n",
    "      and datestr = '{datestr}'\n",
    "--      and dispatch_type in (NULL, 'fifo')\n",
    "  ) ft \n",
    "ON mez.uuid = ft.trip_uuid\n",
    "),\n",
    "\n",
    "idle as (\n",
    "SELECT\n",
    "sp.datestr as datestr,\n",
    "sp.city_id as city_id,\n",
    "'NA' as trip_uuid,\n",
    "'NA' as session_uuid,\n",
    "sp.earner_uuid as driver_uuid,\n",
    "sp.start_timestamp.`local` as local_time,\n",
    "CAST(0 as DOUBLE) as eta,\n",
    "CAST(0 as DOUBLE) as fare,\n",
    "CAST(0.5 as DOUBLE) as duration_min,\n",
    "sp.location.lat as driver_origin_lat,\n",
    "sp.location.lng as driver_origin_lng,\n",
    "sp.location.lat as pickup_lat,\n",
    "sp.location.lng as pickup_lng,\n",
    "sp.location.lat as dropoff_lat,\n",
    "sp.location.lng as dropoff_lng\n",
    "FROM\n",
    "driver.fact_earner_supply_minute as sp\n",
    "WHERE\n",
    "sp.datestr = '{datestr}'\n",
    "and LOWER(sp.flow_type) IN ('uberx', 'p2p')\n",
    "and sp.city_id = {city_id}\n",
    "and LOWER(sp.earner_state)='open'\n",
    "and substr(sp.earner_uuid, 1, length('3')) = '3'\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    datestr,\n",
    "    city_id, \n",
    "    trip_uuid,\n",
    "    session_uuid,\n",
    "    driver_uuid,\n",
    "    local_time,\n",
    "    eta,\n",
    "    fare,\n",
    "    duration_min,\n",
    "    --   Driver origin information\n",
    "    driver_origin_lat,\n",
    "    driver_origin_lng,\n",
    "    pickup_lat,\n",
    "    pickup_lng,\n",
    "    dropoff_lat,\n",
    "    dropoff_lng\n",
    "FROM\n",
    "    completed\n",
    "UNION\n",
    "SELECT\n",
    "    datestr,\n",
    "    city_id,\n",
    "    trip_uuid,\n",
    "    session_uuid,\n",
    "    driver_uuid,\n",
    "    local_time,\n",
    "    eta,\n",
    "    fare,\n",
    "    duration_min,\n",
    "    driver_origin_lat,\n",
    "    driver_origin_lng,\n",
    "    pickup_lat,\n",
    "    pickup_lng,\n",
    "    dropoff_lat,\n",
    "    dropoff_lng\n",
    "FROM\n",
    "    idle\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_id, num_days, datestr\n",
    "\n",
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    city_id: int\n",
    "    datestr: str\n",
    "    num_days: int\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.datestr}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, datestr=self.datestr)\n",
    "        \n",
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25/25 dataframes from cache!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = [Query(prefix=prefix, city_id=city_id, datestr=datestr, num_days=1) for datestr in datestrs]\n",
    "\n",
    "cache_qry_map = {q.name: q.qry for q in queries}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='phx2',\n",
    "    datasource='hive-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = scans.columns\n",
    "cols = [i.split('.')[1] for i in cols]\n",
    "scans.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans['duration_min'][scans['eta'] == 0.0] = idle_duration_seconds / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new objective function\n",
    "def clean_df(df):\n",
    "    df = df[df['fare'].notnull()]\n",
    "    df['driver_origin_geohash8'] = scans.h3.geo_to_h3(8, lat_col = 'driver_origin_lat', lng_col = 'driver_origin_lng').index\n",
    "    df['pickup_geohash8'] = scans.h3.geo_to_h3(8, lat_col = 'pickup_lat', lng_col = 'pickup_lng').index\n",
    "    df['dropoff_geohash8'] = scans.h3.geo_to_h3(8, lat_col = 'dropoff_lat', lng_col = 'dropoff_lng').index\n",
    "    df['driver_origin_geohash7'] = scans.h3.geo_to_h3(7, lat_col = 'driver_origin_lat', lng_col = 'driver_origin_lng').index\n",
    "    df['pickup_geohash7'] = scans.h3.geo_to_h3(7, lat_col = 'pickup_lat', lng_col = 'pickup_lng').index\n",
    "    df['dropoff_geohash7'] = scans.h3.geo_to_h3(7, lat_col = 'dropoff_lat', lng_col = 'dropoff_lng').index    \n",
    "    df = df[df['driver_origin_lng'] < MAX_LNG]\n",
    "    df = df[df['driver_origin_lng'] > MIN_LNG]\n",
    "    df = df[df['driver_origin_lat'] < MAX_LAT]\n",
    "    df = df[df['driver_origin_lat'] > MIN_LAT]\n",
    "    df['local_time'] = pd.to_datetime(df.local_time)\n",
    "    df['weekday_origin'] = df.local_time.dt.dayofweek\n",
    "    df['weekday_dropoff'] = df.local_time.dt.dayofweek\n",
    "    df['second_in_day'] = df.local_time.dt.hour * 3600 + \\\n",
    "                          df.local_time.dt.minute * 60 + \\\n",
    "                          df.local_time.dt.second\n",
    "    df['trip_duration_seconds'] = df.duration_min * 60\n",
    "    df['total_driver_trip_time'] = df.trip_duration_seconds + df.eta\n",
    "    df['destination_arrival_time'] = df.total_driver_trip_time + df.second_in_day\n",
    "    df['destination_arrival_time'][df['eta'] == 0] = df['second_in_day']\n",
    "    mask = df['destination_arrival_time'] > 24 * 3600\n",
    "    df['destination_arrival_time'] = df['destination_arrival_time'].mod(24 * 3600)\n",
    "    df['weekday_dropoff'][mask] = (df['weekday_dropoff'][mask] + 1) % 7\n",
    "#     df['trip_length'][df['trip_length'] <= 100] = 100\n",
    "#     df = df.drop_duplicates(subset=['job_uuid', 'supply_uuid'])\n",
    "#     df = df.dropna()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jaeger_tracing:Tracing sampler started with sampling refresh interval 60 sec\n"
     ]
    }
   ],
   "source": [
    "df = clean_df(scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique geohash8: 10057\n"
     ]
    }
   ],
   "source": [
    "geohashes8 = list(set(pd.concat([df['pickup_geohash8'], df['dropoff_geohash8']])))\n",
    "geohashes8 += ['UNK']\n",
    "\n",
    "geohash8_to_int = {geohashes8[i]: i for i in range(len(geohashes8))}\n",
    "\n",
    "df['driver_origin_geohash8'] = df['driver_origin_geohash8'].apply(lambda x: geohash8_to_int.get(x, len(geohashes8) - 1))\n",
    "df['pickup_geohash8'] = df['pickup_geohash8'].apply(lambda x: geohash8_to_int.get(x, len(geohashes8) - 1))\n",
    "df['dropoff_geohash8'] = df['dropoff_geohash8'].apply(lambda x: geohash8_to_int.get(x, len(geohashes8) - 1))\n",
    "\n",
    "print(f'number of unique geohash8: {len(geohashes8)}')\n",
    "\n",
    "with open(f'geohash8Map_{city_id}.pkl', 'wb') as file:\n",
    "    pickle.dump(geohash8_to_int, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique geohash7: 3119\n"
     ]
    }
   ],
   "source": [
    "geohashes7 = list(set(pd.concat([df['pickup_geohash7'], df['dropoff_geohash7']])))\n",
    "geohashes7 += ['UNK']\n",
    "\n",
    "geohash7_to_int = {geohashes7[i]: i for i in range(len(geohashes7))}\n",
    "\n",
    "df['driver_origin_geohash7'] = df['driver_origin_geohash7'].apply(lambda x: geohash7_to_int.get(x, len(geohashes7) - 1))\n",
    "df['pickup_geohash7'] = df['pickup_geohash7'].apply(lambda x: geohash7_to_int.get(x, len(geohashes7) - 1))\n",
    "df['dropoff_geohash7'] = df['dropoff_geohash7'].apply(lambda x: geohash7_to_int.get(x, len(geohashes7) - 1))\n",
    "\n",
    "print(f'number of unique geohash7: {len(geohashes7)}')\n",
    "\n",
    "with open(f'geohash7Map_{city_id}.pkl', 'wb') as file:\n",
    "    pickle.dump(geohash7_to_int, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datestr</th>\n",
       "      <th>city_id</th>\n",
       "      <th>trip_uuid</th>\n",
       "      <th>session_uuid</th>\n",
       "      <th>driver_uuid</th>\n",
       "      <th>local_time</th>\n",
       "      <th>eta</th>\n",
       "      <th>fare</th>\n",
       "      <th>duration_min</th>\n",
       "      <th>driver_origin_lat</th>\n",
       "      <th>driver_origin_lng</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_lng</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_lng</th>\n",
       "      <th>driver_origin_geohash8</th>\n",
       "      <th>pickup_geohash8</th>\n",
       "      <th>dropoff_geohash8</th>\n",
       "      <th>driver_origin_geohash7</th>\n",
       "      <th>pickup_geohash7</th>\n",
       "      <th>dropoff_geohash7</th>\n",
       "      <th>weekday_origin</th>\n",
       "      <th>weekday_dropoff</th>\n",
       "      <th>second_in_day</th>\n",
       "      <th>trip_duration_seconds</th>\n",
       "      <th>total_driver_trip_time</th>\n",
       "      <th>destination_arrival_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3d5dd9b0-a590-4c7a-96d6-772a0459279d</td>\n",
       "      <td>2022-11-13 08:31:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>39.947678</td>\n",
       "      <td>-75.150772</td>\n",
       "      <td>39.947678</td>\n",
       "      <td>-75.150772</td>\n",
       "      <td>39.947678</td>\n",
       "      <td>-75.150772</td>\n",
       "      <td>7192</td>\n",
       "      <td>7192</td>\n",
       "      <td>7192</td>\n",
       "      <td>1679</td>\n",
       "      <td>1679</td>\n",
       "      <td>1679</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>30660</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>30660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3c2f73fb-2f96-43ce-ba11-0e092f5d9040</td>\n",
       "      <td>2022-11-12 20:54:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>39.960620</td>\n",
       "      <td>-75.137718</td>\n",
       "      <td>39.960620</td>\n",
       "      <td>-75.137718</td>\n",
       "      <td>39.960620</td>\n",
       "      <td>-75.137718</td>\n",
       "      <td>2986</td>\n",
       "      <td>2986</td>\n",
       "      <td>2986</td>\n",
       "      <td>1609</td>\n",
       "      <td>1609</td>\n",
       "      <td>1609</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>75240</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>75240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>20</td>\n",
       "      <td>9f0163ce-e7d0-4ad4-a78d-6bf69a993788</td>\n",
       "      <td>5cf77049-e3a3-42c5-817a-7dc9826cd313</td>\n",
       "      <td>59e76dd3-4d22-4ca3-ae8d-8939f66cd655</td>\n",
       "      <td>2022-11-13 16:33:55</td>\n",
       "      <td>316.0</td>\n",
       "      <td>12.99</td>\n",
       "      <td>11.116667</td>\n",
       "      <td>39.974910</td>\n",
       "      <td>-75.248350</td>\n",
       "      <td>39.973790</td>\n",
       "      <td>-75.272300</td>\n",
       "      <td>39.995560</td>\n",
       "      <td>-75.234240</td>\n",
       "      <td>5130</td>\n",
       "      <td>2304</td>\n",
       "      <td>5480</td>\n",
       "      <td>1586</td>\n",
       "      <td>2804</td>\n",
       "      <td>1050</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>59635</td>\n",
       "      <td>667.000000</td>\n",
       "      <td>983.000000</td>\n",
       "      <td>60618.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30f459d2-2247-4b74-b806-f3844c73bd58</td>\n",
       "      <td>2022-11-13 06:56:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>39.903767</td>\n",
       "      <td>-75.197037</td>\n",
       "      <td>39.903767</td>\n",
       "      <td>-75.197037</td>\n",
       "      <td>39.903767</td>\n",
       "      <td>-75.197037</td>\n",
       "      <td>8066</td>\n",
       "      <td>8066</td>\n",
       "      <td>8066</td>\n",
       "      <td>1687</td>\n",
       "      <td>1687</td>\n",
       "      <td>1687</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24960</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>24960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35a64076-da8e-40fe-861f-18a44899faab</td>\n",
       "      <td>2022-11-13 11:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.540374</td>\n",
       "      <td>39.885044</td>\n",
       "      <td>-75.242429</td>\n",
       "      <td>39.885044</td>\n",
       "      <td>-75.242429</td>\n",
       "      <td>39.885044</td>\n",
       "      <td>-75.242429</td>\n",
       "      <td>7210</td>\n",
       "      <td>7210</td>\n",
       "      <td>7210</td>\n",
       "      <td>653</td>\n",
       "      <td>653</td>\n",
       "      <td>653</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>41400</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>32.422459</td>\n",
       "      <td>41400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      datestr  city_id                             trip_uuid  \\\n",
       "0  2022-11-13       20                                   NaN   \n",
       "1  2022-11-13       20                                   NaN   \n",
       "2  2022-11-13       20  9f0163ce-e7d0-4ad4-a78d-6bf69a993788   \n",
       "3  2022-11-13       20                                   NaN   \n",
       "4  2022-11-13       20                                   NaN   \n",
       "\n",
       "                           session_uuid                           driver_uuid  \\\n",
       "0                                   NaN  3d5dd9b0-a590-4c7a-96d6-772a0459279d   \n",
       "1                                   NaN  3c2f73fb-2f96-43ce-ba11-0e092f5d9040   \n",
       "2  5cf77049-e3a3-42c5-817a-7dc9826cd313  59e76dd3-4d22-4ca3-ae8d-8939f66cd655   \n",
       "3                                   NaN  30f459d2-2247-4b74-b806-f3844c73bd58   \n",
       "4                                   NaN  35a64076-da8e-40fe-861f-18a44899faab   \n",
       "\n",
       "           local_time    eta   fare  duration_min  driver_origin_lat  \\\n",
       "0 2022-11-13 08:31:00    0.0   0.00      0.540374          39.947678   \n",
       "1 2022-11-12 20:54:00    0.0   0.00      0.540374          39.960620   \n",
       "2 2022-11-13 16:33:55  316.0  12.99     11.116667          39.974910   \n",
       "3 2022-11-13 06:56:00    0.0   0.00      0.540374          39.903767   \n",
       "4 2022-11-13 11:30:00    0.0   0.00      0.540374          39.885044   \n",
       "\n",
       "   driver_origin_lng  pickup_lat  pickup_lng  dropoff_lat  dropoff_lng  \\\n",
       "0         -75.150772   39.947678  -75.150772    39.947678   -75.150772   \n",
       "1         -75.137718   39.960620  -75.137718    39.960620   -75.137718   \n",
       "2         -75.248350   39.973790  -75.272300    39.995560   -75.234240   \n",
       "3         -75.197037   39.903767  -75.197037    39.903767   -75.197037   \n",
       "4         -75.242429   39.885044  -75.242429    39.885044   -75.242429   \n",
       "\n",
       "   driver_origin_geohash8  pickup_geohash8  dropoff_geohash8  \\\n",
       "0                    7192             7192              7192   \n",
       "1                    2986             2986              2986   \n",
       "2                    5130             2304              5480   \n",
       "3                    8066             8066              8066   \n",
       "4                    7210             7210              7210   \n",
       "\n",
       "   driver_origin_geohash7  pickup_geohash7  dropoff_geohash7  weekday_origin  \\\n",
       "0                    1679             1679              1679               6   \n",
       "1                    1609             1609              1609               5   \n",
       "2                    1586             2804              1050               6   \n",
       "3                    1687             1687              1687               6   \n",
       "4                     653              653               653               6   \n",
       "\n",
       "   weekday_dropoff  second_in_day  trip_duration_seconds  \\\n",
       "0                6          30660              32.422459   \n",
       "1                5          75240              32.422459   \n",
       "2                6          59635             667.000000   \n",
       "3                6          24960              32.422459   \n",
       "4                6          41400              32.422459   \n",
       "\n",
       "   total_driver_trip_time  destination_arrival_time  \n",
       "0               32.422459                   30660.0  \n",
       "1               32.422459                   75240.0  \n",
       "2              983.000000                   60618.0  \n",
       "3               32.422459                   24960.0  \n",
       "4               32.422459                   41400.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22143"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.weekday_dropoff != df.weekday_origin])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training offline DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_sim.util import *\n",
    "from mini_sim.DQN_offlineData import *\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict, deque\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from typing import Iterator, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchMaker:\n",
    "    def __init__(self, table) -> None:\n",
    "        self.table = table\n",
    "        self.lenDF = len(table)\n",
    "    \n",
    "    def __len__(self) -> None:\n",
    "        return len(self.table)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        sample = self.table.iloc[np.random.choice(self.lenDF, batch_size)]\n",
    "        batch_v1 = torch.zeros(batch_size, 6)\n",
    "        batch_v2 = torch.zeros(batch_size, 6)\n",
    "        batch_eta = torch.zeros(batch_size, 1)\n",
    "        batch_tripDuration = torch.zeros(batch_size, 1)\n",
    "        batch_fares = torch.zeros(batch_size, 1)\n",
    "\n",
    "        ### make samples based on en routes\n",
    "        batch_v1[:, 0] = torch.tensor(sample.weekday_origin.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:, 1] = torch.tensor(sample.second_in_day.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:, 2] = torch.tensor(sample.driver_origin_geohash7.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:, 3] = torch.tensor(sample.driver_origin_geohash8.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:, 4] = torch.tensor(sample.driver_origin_lat.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:, 5] = torch.tensor(sample.driver_origin_lng.to_numpy(), dtype=torch.float)\n",
    "        batch_eta[:,0] = torch.tensor(sample.eta.to_numpy(), dtype=torch.float)\n",
    "        batch_tripDuration[:,0] = torch.tensor(sample.trip_duration_seconds.to_numpy(), dtype=torch.float)\n",
    "        batch_fares[:,0] = torch.tensor(sample.fare.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:, 0] = torch.tensor(sample.weekday_dropoff.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:, 1] = torch.tensor(sample.destination_arrival_time.to_numpy() , dtype=torch.float)\n",
    "        batch_v2[:, 2] = torch.tensor(sample.dropoff_geohash7.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:, 3] = torch.tensor(sample.dropoff_geohash8.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:, 4] = torch.tensor(sample.dropoff_lat.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:, 5] = torch.tensor(sample.dropoff_lng.to_numpy(), dtype=torch.float)\n",
    "\n",
    "        return (batch_v1, batch_v2, batch_eta, batch_tripDuration, batch_fares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_maker, sample_size: int = 200) -> None:\n",
    "        self.sample_size = sample_size\n",
    "        self.batch_maker = batch_maker\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple]:\n",
    "        batch_v1, batch_v2, batch_eta, batch_tripDuration, batch_fares = self.batch_maker.sample(self.sample_size)\n",
    "        for i in range(len(batch_eta)):\n",
    "            yield batch_v1[i], batch_v2[i], batch_eta[i], batch_tripDuration[i], batch_fares[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = DISCOUNT,\n",
    "        sync_rate: int = 200,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_maker = batchMaker(df)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.sync_rate = sync_rate\n",
    "        \n",
    "        self.net = Net(len(geohashes7), len(geohashes8))\n",
    "        self.target_net = Net(len(geohashes7), len(geohashes8))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        states, next_states, batch_eta, batch_tripDuration, batch_fares = batch        \n",
    "\n",
    "        state_values, variance = self.net(states)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values, _ = self.target_net(next_states)\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        timeNextState = batch_eta + batch_tripDuration\n",
    "        discountedNextState = torch.pow(self.gamma, timeNextState) * next_state_values\n",
    "        timeFareCollected = batch_eta\n",
    "        discountedFare = torch.pow(self.gamma, timeFareCollected) * batch_fares\n",
    "        expected_state_values = discountedFare + discountedNextState            \n",
    "\n",
    "        return nn.GaussianNLLLoss()(state_values, expected_state_values, variance)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "        based on the minibatch recieved.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            nb_batch: batch number\n",
    "\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_loss(batch)\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            }\n",
    "        )\n",
    "        self.log(\"steps\", self.global_step, logger=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = torch.optim.RMSprop(self.net.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.batch_maker, self.batch_size)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name       | Type | Params\n",
      "------------------------------------\n",
      "0 | net        | Net  | 1.6 M \n",
      "1 | target_net | Net  | 1.6 M \n",
      "------------------------------------\n",
      "3.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.654    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f5904f3ed14e8fa6815646e408634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n",
    "    max_epochs=100000,\n",
    "    val_check_interval=100,\n",
    "    logger=CSVLogger(save_dir=f\"logs/city_id{city_id}\"),\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls logs/city_id20/lightning_logs/version_9/checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'logs/city_id20/lightning_logs/version_9/checkpoints/epoch=99999-step=100000.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_dict(path):\n",
    "    trained_dict = torch.load(path)['state_dict']\n",
    "    target_dict = {}\n",
    "    for k,v in trained_dict.items():\n",
    "        if k.startswith('target'):\n",
    "            target_dict[k[11:]] = v\n",
    "    return target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "tr1 = DQNLightning()\n",
    "LTSV = Net(len(geohashes7), len(geohashes8))\n",
    "target_dict = get_target_dict(PATH)\n",
    "LTSV.load_state_dict(target_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotMap(tr, n = 5000, weekday = 2, stat = 'mean'):\n",
    "    LTSV.eval()\n",
    "    data = np.zeros((24 * n, 4))\n",
    "    batch_v1, _, _, _, _ = tr.batch_maker.sample(n)\n",
    "    batch_v1[:, 0] = weekday\n",
    "    data[:, 1:3] = batch_v1[:, -2:].repeat(24, 1)\n",
    "    with torch.no_grad():\n",
    "        for idx, selectedTime in enumerate(range(0, 24 * 3600, 3600)):\n",
    "            data[idx * n: (idx + 1) * n, 0] = idx + 1\n",
    "            batch_v1[:, 1] = selectedTime\n",
    "            if stat == 'mean':\n",
    "                out, _ = LTSV(batch_v1)\n",
    "            elif stat == 'zscore':\n",
    "                mean, out = LTSV(batch_v1)\n",
    "                out = mean / torch.sqrt(out)\n",
    "            elif stat == 'cov':\n",
    "                mean, out = LTSV(batch_v1)\n",
    "                out = torch.sqrt(out) / mean\n",
    "                out = out\n",
    "            else:\n",
    "                raise Exception('stat invalid')\n",
    "            data[idx * n: (idx + 1) * n, -1] = out[:,0]\n",
    "\n",
    "    hours_df = pd.DataFrame(data = data, columns = ['hour', 'lat', 'lng', 'value'])\n",
    "\n",
    "    fig = ff.create_hexbin_mapbox(\n",
    "        data_frame=hours_df, \n",
    "        lat=\"lat\",\n",
    "        lon=\"lng\",\n",
    "        color='value',\n",
    "        animation_frame='hour',\n",
    "        nx_hexagon=100, opacity=0.8,\n",
    "        min_count=20\n",
    "    )    \n",
    "\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=8, mapbox_center = {\"lat\": LAT_CENTER, \"lon\": LON_CENTER},)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "    fig.update_layout(autosize=False,width=700,height=700)\n",
    "    fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 600\n",
    "    fig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 600\n",
    "    fig.layout.coloraxis.showscale = True   \n",
    "    fig.layout.sliders[0].pad.t = 10\n",
    "    fig.layout.updatemenus[0].pad.t= 10             \n",
    "\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotMap(tr1, n = 100000, weekday = 4, stat = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotMap(tr1, n = 100000, weekday = 4, stat = 'cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1am, x1, y1) --> (2am, x2, y2) w/ $10\n",
    "# (4am, x1, y1) --> (4:30am, x3, y3) w/ $8\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Python 3.7 (General DS)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
