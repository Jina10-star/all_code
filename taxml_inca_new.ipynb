{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec569de-7deb-49e1-b4df-ca9cd0962b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import date, datetime, timezone,timedelta\n",
    "import logging\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "import io\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from dotenv import load_dotenv\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "from boxsdk import Client, JWTAuth\n",
    "\n",
    "from queryrunner_client import Client as qr_client\n",
    "import config\n",
    "import sendgrid\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "sg = sendgrid.SendGridAPIClient(config.API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "814e3847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 10:49:42,799 main DEBUG New Job has started ....\n"
     ]
    }
   ],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger(\"main\")\n",
    "\n",
    "# set logging level : INFO, DEBUG, WARNING or ERROR\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create TimedRotatingFileHandler with log file name\n",
    "# It will create a new log file each day at midnight\n",
    "handler = TimedRotatingFileHandler(config.LOGFILE_INCA, when=\"midnight\", interval=1)\n",
    "\n",
    "# This is the format in which logs will be displayed in log file\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# assign the formatter and suffix to file_handler object\n",
    "# suffix will be added to each file\n",
    "handler.setFormatter(formatter)\n",
    "handler.suffix = \"%Y%m%d\"\n",
    "\n",
    "\n",
    "# add the handler to logger\n",
    "logger.addHandler(handler)\n",
    "start_time = time.time()\n",
    "logger.debug('New Job has started ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3ce924",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f70e74-3c95-4489-912a-d8019fcc69ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIVE_QUERY = f\"\"\"\n",
    "with target_merchants as (\n",
    "    with us_states as (\n",
    "      select\n",
    "        geofence_uuid,\n",
    "        name as state_name,\n",
    "        simplified_shape as simplified_shape_wkt,\n",
    "        areasqmeters,\n",
    "        countrycode\n",
    "      from\n",
    "        map_geofences.geofences_mbi_admin\n",
    "      where\n",
    "        namespace = 'canonical-mbi-admin-1'\n",
    "        and (\n",
    "          countrycode = 'US'\n",
    "          or countrycode = 'CA'\n",
    "        )\n",
    "    ),\n",
    "    us_counties as (\n",
    "      select\n",
    "        geofence_uuid,\n",
    "        name as county_name,\n",
    "        simplified_shape as simplified_shape_wkt,\n",
    "        areasqmeters,\n",
    "        countrycode\n",
    "      from\n",
    "        map_geofences.geofences_mbi_admin\n",
    "      where\n",
    "        (\n",
    "          namespace = 'canonical-mbi-admin-4'\n",
    "          and countrycode = 'US'\n",
    "        )\n",
    "        or (\n",
    "          namespace = 'canonical-mbi-admin-5'\n",
    "          and countrycode = 'CA'\n",
    "        )\n",
    "    )\n",
    "    ----------------------------\n",
    "    ----------------------------\n",
    "    select\n",
    "      dm.uuid\n",
    "    from\n",
    "      eds.dim_merchant dm\n",
    "      left join us_states st on st_contains(\n",
    "        st.simplified_shape_wkt,\n",
    "        st_point(dm.longitude, dm.latitude)\n",
    "      )\n",
    "      left join us_counties co on st_contains(\n",
    "        co.simplified_shape_wkt,\n",
    "        st_point(dm.longitude, dm.latitude)\n",
    "      )\n",
    "      where\n",
    "        st.geofence_uuid is not null or\n",
    "        co.geofence_uuid is not null\n",
    "    group by\n",
    "      1\n",
    ")\n",
    "\n",
    ", allinfo as (\n",
    "select\n",
    "  ext.entity_uuid,\n",
    "  ent.external_id,\n",
    "  ext.catalog_uuid,\n",
    "  coalesce(ext.product.name, raw_ext.product.name) as product_name,\n",
    "  coalesce(ext.product.description, raw_ext.product.description) as product_description,\n",
    "  ext.product.merchant_category_path as published_merchant_category_path,\n",
    "  raw_ext.product.merchant_category_path as raw_merchant_category_path,\n",
    "  ext.tax_labels,\n",
    "  dm.uuid as store_uuid,\n",
    "  dm.store_name,\n",
    "  dm.establishment_type as establishment_type,\n",
    "  dm.uber_merchant_type as uber_merchant_type,\n",
    "  FROM_ISO8601_TIMESTAMP(ext.updated_at) as updated_at\n",
    "from\n",
    "  inca.dim_catalog c\n",
    "  cross join unnest(used_by_stores) as t(store_uuid)\n",
    "  join eds.dim_merchant dm on dm.uuid = t.store_uuid\n",
    "  join target_merchants tm on tm.uuid = dm.uuid\n",
    "  and dm.parent_chain_uuid not in (\n",
    "    '99db4bf9-629f-4b75-a34c-e6628dc66f40' --Exclude Walmart Canada eff 4/18/2023. Discussion that CS mapping more accurate than ML; immediately stop bleeding for ML incorrect tagging for 4/20 launch.\n",
    "  )\n",
    "  join inca.extension_main ext on\n",
    "    ext.catalog_uuid = c.catalog_uuid\n",
    "    and ext.source = 'published'\n",
    "    \n",
    "    and ext.tax_labels is null\n",
    "    \n",
    "    and ext.entity_type = 'PRODUCT'\n",
    "  join inca.extension_main raw_ext on\n",
    "    raw_ext.catalog_uuid = c.catalog_uuid\n",
    "    and raw_ext.entity_uuid = ext.entity_uuid\n",
    "    and raw_ext.source = 'raw'\n",
    "    \n",
    "    and raw_ext.tax_labels is null\n",
    "    \n",
    "    and raw_ext.entity_type = 'PRODUCT'\n",
    "  join inca.entity_main ent on\n",
    "    ext.catalog_uuid = ent.catalog_uuid\n",
    "    and ent.entity_uuid = ext.entity_uuid\n",
    "    and ent.source = 'published'\n",
    "WHERE\n",
    "    1 = 1\n",
    "    and FROM_ISO8601_TIMESTAMP(ext.updated_at) > now() - interval '{config.DAYS_BACK}' day\n",
    "    \n",
    ")\n",
    "\n",
    "SELECT\n",
    "    store_uuid as merchant_uuid\n",
    "    , store_name\n",
    "    , catalog_uuid\n",
    "    , entity_uuid\n",
    "    , product_name\n",
    "    , product_description\n",
    "    , establishment_type\n",
    "    , uber_merchant_type\n",
    "    , published_merchant_category_path\n",
    "    , raw_merchant_category_path\n",
    "    , tax_labels\n",
    "    , updated_at\n",
    "FROM allinfo limit 900000\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2bdc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(message):\n",
    "\n",
    "    # stopwords\n",
    "    stpwrd = nltk.corpus.stopwords.words('english')\n",
    "    # initialize lemmatizing \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lowering input\n",
    "    message = message.lower()\n",
    "    # removing the numerical values and working only with text values\n",
    "    message = re.sub('[^a-zA-Z]', \" \", message)\n",
    "    # removing the stopwords\n",
    "    message = ' '.join([word for word in message.split() if word not in stpwrd])\n",
    "    # lemmatizing the text\n",
    "    message = \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(message) if w not in string.punctuation])\n",
    "    # removing hyperlinks\n",
    "    message = re.sub(r'http\\S+', ' ', message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b63b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LinearSVC from version 0.24.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(config.MODEL_PATH)\n",
    "# establish a connection for BOX\n",
    "boxconfig = JWTAuth.from_settings_file(config.TAX_CONFIG)\n",
    "client = Client(boxconfig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb985528",
   "metadata": {},
   "source": [
    "# dedup version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bde75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(df, sub_folder):\n",
    "    # combining input features to a single column \n",
    "    df['combined_text'] = df[['product_name','product_description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "    # apply data preprocessing steps on the combined_text column\n",
    "    df['processed_text'] = df['combined_text'].map(lambda s:preprocess_text(s))\n",
    "    df['product_name_lower'] = df['product_name'].str.lower()\n",
    "    df['product_description_lower'] = df['product_description'].str.lower()\n",
    "    df['establishment_type_lower'] = df['establishment_type'].str.lower()\n",
    "    # removing duplicates\n",
    "    dedup_df = df.drop_duplicates(subset=['product_name_lower','product_description_lower','establishment_type_lower'],ignore_index=True)\n",
    "    logger.info(\"Incoming dedup data size: {}\".format(dedup_df.shape))\n",
    "    print(dedup_df.shape)\n",
    "    # getting ml predictions\n",
    "    dedup_df['ml_prediction'] = model.predict(dedup_df['processed_text'])\n",
    "    \n",
    "    # combining cat name and integer column to get a single output column\n",
    "    dedup_df[['ml_predicted_cat_name','ml_predicted_integer']] = dedup_df['ml_prediction'].str.split(':', expand=True)\n",
    "    \n",
    "    # getting confidence score\n",
    "    dedup_df['ml_predicted_conf_score'] = np.round_(1/(1+np.exp(-np.round_(np.max(model.decision_function(dedup_df['processed_text'].values), axis=1), decimals=2))),decimals=2)\n",
    "    # dropping rest of the columns\n",
    "    # dedup_df.drop(['merchant_uuid','entity_uuid','source','merchant_type_analytics','external_id','time_stamp','tax_labels','tax_label_ts','processed_text','combined_text','ml_prediction'], inplace=True, axis=1)\n",
    "    dedup_df.drop(['merchant_uuid','store_name','entity_uuid','uber_merchant_type','published_merchant_category_path',\\\n",
    "                   'raw_merchant_category_path','tax_labels','updated_at','processed_text','combined_text','ml_prediction','product_name_lower','product_description_lower','establishment_type_lower'], inplace=True, axis=1)\n",
    "    \n",
    "    datetime_str = datetime.now().strftime('%d%b%Y_%H%M%S')\n",
    "    filename_inca_data = config.FILENAME_DEDUP+\"_\"+datetime_str+\".csv\"\n",
    "    \n",
    "    print(filename_inca_data)\n",
    "    inca_path_data_dedup = os.path.join(config.DATA_PATH_INCA_DEDUP, filename_inca_data)\n",
    "    \n",
    "    # saving dedup file \n",
    "    dedup_df.to_csv(inca_path_data_dedup, index=False)\n",
    "    FILENAME_DEDUP = config.FILENAME_DEDUP+\"_\"+datetime_str+\".csv\"\n",
    "    items = sub_folder.get_items()\n",
    "    flag = False\n",
    "    logger.debug('Uploading dedup file with ml prediction data in BOX....')\n",
    "\n",
    "    for item in items:\n",
    "        # if file name exists, update the content\n",
    "        if item.name == FILENAME_DEDUP:\n",
    "            chunked_uploader = client.file(item.id).get_chunked_uploader(inca_path_data_dedup)\n",
    "            updated_file = chunked_uploader.start()\n",
    "            print(f'Dedup File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "            logger.debug(f'Dedup File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    # if file doesn not exists upload the file\n",
    "    if not flag:\n",
    "        uploaded_file = sub_folder.upload(inca_path_data_dedup)\n",
    "        print('Dedup File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "        logger.debug(f'Dedup File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aeee94",
   "metadata": {},
   "source": [
    "# Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b6d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw(df,sub_folder):\n",
    "    logger.info(\"Incoming raw data size: {}\".format(df.shape))\n",
    "    # combining input features to a single column \n",
    "    df['combined_text'] = df[['product_name','product_description','establishment_type']].apply(lambda x: ' '.join(x[x.notnull()]), axis = 1)\n",
    "    # apply data preprocessing steps on the combined_text column\n",
    "    df['processed_text'] = df['combined_text'].map(lambda s:preprocess_text(s))\n",
    "    # getting the prediction\n",
    "    df['ml_prediction'] = model.predict(df['processed_text'])\n",
    "    # combining cat name and integer column to get a single output column\n",
    "    df[['ml_predicted_cat_name','ml_predicted_integer']] = df['ml_prediction'].str.split(':', expand=True)\n",
    "    # getting confidence score\n",
    "    df['ml_predicted_conf_score'] = np.round_(1/(1+np.exp(-np.round_(np.max(model.decision_function(df['processed_text'].values), axis=1), decimals=2))),decimals=2)\n",
    "    # dropping irrelevent columns\n",
    "    df.drop(['combined_text','processed_text','ml_prediction'], inplace=True, axis=1)\n",
    "    datetime_str = datetime.now().strftime('%d%b%Y_%H%M%S')\n",
    "    filename_inca_data = config.FILENAME_RAW+\"_\"+datetime_str+\".csv\"\n",
    "    print(filename_inca_data)\n",
    "    inca_path_data_raw = os.path.join(config.DATA_PATH_INCA_RAW, filename_inca_data) \n",
    "    # saving output in a csv\n",
    "    df.to_csv(inca_path_data_raw, index=False)\n",
    "    FILENAME_RAW = config.FILENAME_RAW+\"_\"+datetime_str+\".csv\"\n",
    "    items = sub_folder.get_items()\n",
    "    flag = False\n",
    "    logger.debug('Uploading Raw file with ml prediction data in BOX....')\n",
    "\n",
    "    for item in items:\n",
    "        # if file name exists, update the content\n",
    "        if item.name == FILENAME_RAW:\n",
    "            chunked_uploader = client.file(item.id).get_chunked_uploader(inca_path_data_raw)\n",
    "            updated_file = chunked_uploader.start()\n",
    "            print(f'Raw File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "            logger.debug(f'Raw File \"{updated_file.name}\" updated to Box with file ID {updated_file.id}')\n",
    "            flag = True\n",
    "            break\n",
    "\n",
    "    # if file doesn not exists upload the file\n",
    "    if not flag:\n",
    "        uploaded_file = sub_folder.upload(inca_path_data_raw)\n",
    "        print('Raw File \"{0}\" has been uploaded'.format(uploaded_file.name))\n",
    "        logger.debug(f'Raw File \"{uploaded_file.name}\" uploaded to Box with file ID {uploaded_file.id}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adefacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/22/2023 10:49:43 AM Send empty tier_metadata {} to Queryrunner V2.\n",
      "08/22/2023 10:49:43 AM \u001b[93m [Polling] fd0d8d26-0d26-41f8-9f7a-5624623ff556 \u001b[0m\n",
      "08/22/2023 10:49:43 AM \u001b[93m [Status] created \u001b[0m\n",
      "08/22/2023 10:49:44 AM \u001b[93m [Status] finished auth check \u001b[0m\n",
      "08/22/2023 10:49:45 AM \u001b[93m [Status] started waiting to execute \u001b[0m\n",
      "08/22/2023 10:50:51 AM \u001b[93m [Status] started execution \u001b[0m\n",
      "08/22/2023 10:59:47 AM \u001b[93m [Status] completed success \u001b[0m\n",
      "08/22/2023 10:59:47 AM \u001b[92m [Query Success] completed success \u001b[0m\n",
      "2023-08-22 10:59:47,403 main DEBUG Hive data load into dataframe 00:10:04\n",
      "2023-08-22 11:08:01,153 main INFO Incoming dedup data size: (87858, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87858, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/dsw/snapshots/e23922be-439f-4b4e-96ec-f6ffe45a3a7a/python37/lib/python3.7/site-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_Deduped_22Aug2023_110804.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 11:08:05,031 main DEBUG Uploading dedup file with ml prediction data in BOX....\n",
      "2023-08-22 11:08:10,683 main DEBUG Dedup File \"Input_Deduped_22Aug2023_110804.csv\" uploaded to Box with file ID 1286872556898\n",
      "2023-08-22 11:08:10,692 main INFO Incoming raw data size: (900000, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedup File \"Input_Deduped_22Aug2023_110804.csv\" has been uploaded\n",
      "Input_RawData_22Aug2023_111642.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 11:16:56,059 main DEBUG Uploading Raw file with ml prediction data in BOX....\n",
      "2023-08-22 11:17:57,196 main DEBUG Raw File \"Input_RawData_22Aug2023_111642.csv\" uploaded to Box with file ID 1286890780380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw File \"Input_RawData_22Aug2023_111642.csv\" has been uploaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 11:17:57,526 main DEBUG Taxml automated prediction email notification has been sent successfully...\n",
      "2023-08-22 11:17:57,527 main DEBUG Total program execution time for ml prediction TAXML INCA 00:28:14\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    qr = qr_client(user_email=config.USER_EMAIL)\n",
    "    cursor = qr.execute('presto', query=HIVE_QUERY, pii=False, timeout=10800)\n",
    "    end_time = time.time()\n",
    "    interval = time.strftime('%H:%M:%S', time.gmtime(end_time-start_time))\n",
    "    logger.debug('Hive data load into dataframe {}'.format(interval))\n",
    "    df = cursor.to_pandas()\n",
    "    datetime_str_const = datetime.now().strftime('%d%b%Y_%H%M%S')\n",
    "    folder = client.folder(folder_id=config.OUTPUT_FOLDER_ID_INCA)\n",
    "    subfolder_naming_format = 'INCA_QB_'+datetime_str_const\n",
    "    sub_folder = folder.create_subfolder(name=subfolder_naming_format)\n",
    "    if len(df) > 0:\n",
    "        dedup(df,sub_folder)\n",
    "        raw(df,sub_folder) \n",
    "        message_success = {\"personalizations\": [{\n",
    "    \"to\": [{\n",
    "            \"email\": \"phegde1@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "           {\n",
    "            \"email\": \" amoham117@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" vbanap@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" iacoe-email-notification-service@uber.pagerduty.com\"\n",
    "        }],\n",
    "    \"cc\": [ {\n",
    "            \"email\": \"sshaik26@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"spothi1@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"dampol@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vvemav@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"anevre@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"jghosh2@ext.uber.com\"\n",
    "        }],\n",
    "\n",
    "            'subject': 'TaxML_INCA Daily Run ML Job Sucess Notification'\n",
    "        }\n",
    "    ],\n",
    "                           \n",
    "    \"from\": {\n",
    "    \"email\": \"ia-coe-support-group@uber.com\"\n",
    "},\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>TaxML INCA ML Job  uploaded file in this location(https://uber.app.box.com/folder/\"+sub_folder.id+\") successfully.<br><br>You can reach out to ia-coe-support-group@uber.com for any questions or concerns.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "\n",
    "        }\n",
    "]\n",
    "}\n",
    "\n",
    "        response = sg.client.mail.send.post(request_body=message_success)\n",
    "        logger.debug('Taxml automated prediction email notification has been sent successfully...')\n",
    "    else:\n",
    "        logger.error('No Data Fetched from hive db...')\n",
    "        message_bussiness_exception = {\n",
    "    'personalizations': [\n",
    "        {\n",
    "    \"to\": [{\n",
    "            \"email\": \"phegde1@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "           {\n",
    "            \"email\": \" amoham117@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" vbanap@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" iacoe-email-notification-service@uber.pagerduty.com\"\n",
    "        }],\n",
    "     \"cc\": [ {\n",
    "            \"email\": \"sshaik26@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"spothi1@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"dampol@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vvemav@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"anevre@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"jghosh2@ext.uber.com\"\n",
    "        }],\n",
    "            'subject': 'TaxML_INCA Daily Run ML Job Failure Notification :: BussinessException'\n",
    "        }\n",
    "    ],\n",
    "    'from': {\n",
    "        'email': 'ia-coe-support-group@uber.com'\n",
    "    },\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>TaxML_INCA ML Job has been failed with the below error: <br><br> No Data Present in Hive DB.<br><br>Please reach Out To ia-coe-support-group@uber.com For any questions Or concerns.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "  \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "        response=sg.client.mail.send.post(request_body=message_bussiness_exception)\n",
    "except Exception as e:\n",
    "    logger.error('Exception details::{}'.format(e), exc_info=True)\n",
    "    message_technical_exception = {\n",
    "    'personalizations': [\n",
    "        {\n",
    "    \"to\": [{\n",
    "            \"email\": \"phegde1@ext.uber.com\"\n",
    "        }, {\n",
    "            \"email\": \"ssethu@ext.uber.com\"\n",
    "        },\n",
    "           {\n",
    "            \"email\": \" amoham117@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"pyagat@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rsujay@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" vbanap@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \"rkunta@ext.uber.com\"\n",
    "        },\n",
    "            {\n",
    "            \"email\": \" iacoe-email-notification-service@uber.pagerduty.com\"\n",
    "        }],\n",
    "     \"cc\": [ {\n",
    "            \"email\": \"sshaik26@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"spothi1@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"dampol@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"vvemav@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"anevre@ext.uber.com\"\n",
    "        },\n",
    "        {\n",
    "            \"email\": \"jghosh2@ext.uber.com\"\n",
    "        }],\n",
    "            'subject': 'TaxML_INCA Daily Run ML Job Failure Notification :: TechnicalException'\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    'from': {\n",
    "        'email': 'ia-coe-support-group@uber.com'\n",
    "    },\n",
    "    'content': [\n",
    "        {\n",
    "            'type': 'text/html',\n",
    "            \"value\": \"<html><head></head><body>Hello!<br><br>TaxML_INCA ML Job has been failed with the below error: <br><br>Error Message: \"+str(e)+\".<br><br>Please reach Out To ia-coe-support-group@uber.com For any questions Or concerns.<br><br><br>Thanks,<br><b>Intelligent Automation COE Support Group<b></body></html>\"\n",
    "  \n",
    "        }\n",
    "    ]\n",
    "}\n",
    "    response = sg.client.mail.send.post(request_body=message_technical_exception)\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    interval = time.strftime('%H:%M:%S', time.gmtime(end_time-start_time))\n",
    "    logger.debug('Total program execution time for ml prediction TAXML INCA {}\\n\\n\\n\\n'.format(interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315b169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02. Python 3.7 (General DS)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
