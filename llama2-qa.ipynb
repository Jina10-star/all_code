{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run Llama 2 Locally with Python (Document QA)\n",
    "\n",
    "This Jupyter Notebook is part of a Blog Post on https://swharden.com\n",
    "\n",
    "https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume information in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\scott\\Documents\\temp\\ai\\DocuLlama2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define what documents to load\n",
    "loader = DirectoryLoader(\"./\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# interpret information in the documents\n",
    "documents = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "texts = splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# create and save the local database\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a LLM that knows about our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "config = {'max_new_tokens': 256, 'temperature': 0.01}\n",
    "llm = CTransformers(model='../models/llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "                    model_type='llama', config=config)\n",
    "\n",
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)\n",
    "\n",
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])\n",
    "\n",
    "QA_LLM = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions About our Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model, question):\n",
    "    model_path = model.combine_documents_chain.llm_chain.llm.model\n",
    "    model_name = pathlib.Path(model_path).name\n",
    "    time_start = time.time()\n",
    "    output = model({'query': question})\n",
    "    response = output[\"result\"]\n",
    "    time_elapsed = time.time() - time_start\n",
    "    display(HTML(f'<code>{model_name} response time: {time_elapsed:.02f} sec</code>'))\n",
    "    display(HTML(f'<strong>Question:</strong> {question}'))\n",
    "    display(HTML(f'<strong>Answer:</strong> {response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<code>llama-2-7b-chat.ggmlv3.q8_0.bin response time: 70.81 sec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> Who is the author of FftSharp? What is their favorite color?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> Scott William Harden is the author of FftSharp. According to Scott, his favorite color is dark blue despite being colorblind."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(QA_LLM, \"Who is the author of FftSharp? What is their favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<code>llama-2-7b-chat.ggmlv3.q8_0.bin response time: 71.04 sec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Question:</strong> Why is JupyterGoBoom obsolete?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong>Answer:</strong> JupyterGoBoom is considered obsolete because modern software developers have come to realize that Jupyter notebooks become unmaintainable all by themselves."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query(QA_LLM, \"Why is JupyterGoBoom obsolete?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
