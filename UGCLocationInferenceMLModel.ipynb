{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from queryrunner_client import Client\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "qr = Client(user_email='abhishek.sharma@uber.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First go through this presentation to get an overview of what's happening here\n",
    "# https://docs.google.com/presentation/d/1HM1wt3vnSfcT6tW3tYr8hqXAucc9tG9E3zyV91O4AU8/edit?usp=sharing\n",
    "# Some terminology differences between the notebook and the presentation:\n",
    "# - candidate segments in the code is equivalent to prefiltered segments in the presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_filter = \"in (20, 8, 14, 198, 5, 12, 134, 26, 23, 25, 24, 208, 27, 1541, 7, 6, 45, 227, 4, 35)\" # cities to consider\n",
    "builduuid = '6dc8e44c-be20-11ea-bb97-000af7f88c50' # umm build uuid, it should be very close to when tickets were reported\n",
    "db = 'maps_automation' # where the tables computed by spark are\n",
    "email = 'abhishek.sharma@uber.com'\n",
    "\n",
    "# Week 1\n",
    "experiment = '_23_07_29_07_20c' # experiment name, refers to one week worth of tickets\n",
    "date_filter = \"between '2020-07-23' and '2020-07-29'\" # looking at tickets reported within these dates\n",
    "date_filter_streaks1 = \"between '2020-07-16' and '2020-07-22'\" # looking at aggregated streaks within these dates\n",
    "date_filter_streaks2 = \"between '2020-07-09' and '2020-07-15'\" # and these dates\n",
    "\n",
    "config1 = {'experiment': experiment, 'date_filter': date_filter, 'date_filter_streaks1': date_filter_streaks1, \n",
    "          'date_filter_streaks2': date_filter_streaks2, 'city_filter': city_filter, 'builduuid': builduuid, 'db': db}\n",
    "\n",
    "\n",
    "# Week 2\n",
    "experiment = '_30_07_05_08_20c2' # experiment name, refers to one week worth of tickets\n",
    "date_filter = \"between '2020-07-30' and '2020-08-05'\" # looking at tickets reported within these dates\n",
    "date_filter_streaks1 = \"between '2020-07-23' and '2020-07-29'\" # looking at aggregated streaks within these dates\n",
    "date_filter_streaks2 = \"between '2020-07-16' and '2020-07-22'\" # and these dates\n",
    "\n",
    "config2 = {'experiment': experiment, 'date_filter': date_filter, 'date_filter_streaks1': date_filter_streaks1, \n",
    "          'date_filter_streaks2': date_filter_streaks2, 'city_filter': city_filter, 'builduuid': builduuid, 'db': db}\n",
    "\n",
    "# Week 3\n",
    "experiment = '_06_08_12_08_20c' # experiment name, refers to one week worth of tickets\n",
    "date_filter = \"between '2020-08-06' and '2020-08-12'\" # looking at tickets reported within these dates\n",
    "date_filter_streaks1 = \"between '2020-07-30' and '2020-08-05'\" # looking at aggregated streaks within these dates\n",
    "date_filter_streaks2 = \"between '2020-07-23' and '2020-07-29'\" # and these dates\n",
    "\n",
    "config3 = {'experiment': experiment, 'date_filter': date_filter, 'date_filter_streaks1': date_filter_streaks1, \n",
    "          'date_filter_streaks2': date_filter_streaks2, 'city_filter': city_filter, 'builduuid': builduuid, 'db': db}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary (in format name: query_template) contains all the queries templates to calculate features\n",
    "# (and training label) for a particular experiment. You could probably put this dictionary in another file and import it.\n",
    "\n",
    "features_query_templates = {\n",
    "# query to load candidate segments from table\n",
    "'candidate_segments': '''\n",
    "  select\n",
    "    map_ticket_id,\n",
    "    trip_id,\n",
    "    city_id,\n",
    "    report_lat,\n",
    "    report_long,\n",
    "    report_date,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "''',\n",
    "    \n",
    "# query to find distance between candidate segment and report location\n",
    "'report_location_dist': '''\n",
    "with candidate_segments as (\n",
    "  select distinct\n",
    "    map_ticket_id,\n",
    "    report_lat,\n",
    "    report_long,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    uuid as segment_id,\n",
    "    geometry.polyline.points as polyline_points\n",
    "  FROM\n",
    "    umm.map_feature_segments_tomtom t\n",
    "    inner JOIN dwh.dim_city c on ST_Contains(\n",
    "      ST_GeometryFromText(c.simplified_shape),\n",
    "      ST_Point(\n",
    "        geometry.polyline.points[1].lnge7 / 1e7,\n",
    "        geometry.polyline.points[1].late7 / 1e7\n",
    "      )\n",
    "    )\n",
    "  where\n",
    "    builduuid = '{builduuid}'\n",
    "    and c.city_id {city_filter}\n",
    ")\n",
    "select\n",
    "  t.map_ticket_id,\n",
    "  t.candidate_segment,\n",
    "  ST_DISTANCE(\n",
    "    ST_LineString(\n",
    "      transform(\n",
    "        s.polyline_points,\n",
    "        x -> ST_POINT(x.lnge7 / 1e7, x.late7 / 1e7)\n",
    "      )\n",
    "    ),\n",
    "    ST_POINT(t.report_long, t.report_lat)\n",
    "  ) * 111321 * cos(radians(t.report_lat)) as report_location_dist\n",
    "from\n",
    "  subset_segments s\n",
    "  join candidate_segments t on s.segment_id = t.candidate_segment\n",
    "''',\n",
    "    \n",
    "# query to find if the candidate segment is suggested anywhere in the trip\n",
    "'segment_suggested': '''SET session join_distribution_type = BROADCAST;\n",
    "  \n",
    "with candidate_segments as (\n",
    "  select\n",
    "    distinct trip_id,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "suggested_segments as (\n",
    "  select\n",
    "    trip_id,\n",
    "    flatten(array_agg(transform(msg.segments, x -> x.segmentuuid))) as suggested_segments\n",
    "  from\n",
    "    rawdata_user.kafka_hp_gurafu_route_logs_nodedup s\n",
    "    join {db}.candidate_segments{experiment} t on s.msg.tripuuid = t.trip_id\n",
    "  where\n",
    "    s.datestr {date_filter}\n",
    "    and s.msg.cityid {city_filter}\n",
    "  group by\n",
    "    1\n",
    ")\n",
    "select\n",
    "  c.trip_id,\n",
    "  c.candidate_segment,\n",
    "  contains(suggested_segments, candidate_segment) as segment_suggested\n",
    "from\n",
    "  suggested_segments s\n",
    "  join candidate_segments c on s.trip_id = c.trip_id''',\n",
    "    \n",
    "# query to check if the candidate_segment is in the path\n",
    "'segment_traversed': '''SET session join_distribution_type = BROADCAST;\n",
    "\n",
    "with candidate_segments as (\n",
    "  select\n",
    "    distinct trip_id,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "traversed_segments as (\n",
    "  select\n",
    "    trip_id,\n",
    "    array_distinct(flatten(array_agg(transform(msg.fittedlocations, x -> x.segmentuuid)))) as traversed_segments\n",
    "  from\n",
    "    rawdata_user.kafka_hp_gmatching_map_matched_trips_nodedup s\n",
    "    join {db}.candidate_segments{experiment} t on s.msg.tripuuid = t.trip_id\n",
    "  where\n",
    "    s.datestr {date_filter}\n",
    "    and s.msg.cityid {city_filter}\n",
    "  group by\n",
    "    1\n",
    ")\n",
    "select\n",
    "  c.trip_id,\n",
    "  c.candidate_segment,\n",
    "  contains(traversed_segments, candidate_segment) as segment_traversed\n",
    "from\n",
    "  traversed_segments s\n",
    "  join candidate_segments c on s.trip_id = c.trip_id'''\n",
    ",\n",
    "    \n",
    "# Calculates average speed on segments traversed in the trip\n",
    "'avg_trip_speed' : '''SET session join_distribution_type = BROADCAST;\n",
    "\n",
    "with candidate_segments as (\n",
    "  select\n",
    "    distinct trip_id,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    uuid as segment_id,\n",
    "    data.segment.startjunctionuuid,\n",
    "    data.segment.endjunctionuuid\n",
    "  from\n",
    "    umm.map_feature_segments_tomtom s\n",
    "    join candidate_segments t on s.uuid = t.candidate_segment\n",
    "  where\n",
    "    builduuid = '{builduuid}'\n",
    "),\n",
    "trip_streaks as (\n",
    "  select\n",
    "    trip_id,\n",
    "    msg.graphsegment.segmentuuid as segment_id,\n",
    "    msg.graphsegment.startjunctionuuid,\n",
    "    msg.graphsegment.endjunctionuuid,\n",
    "    count(uuid) as streaks,\n",
    "    avg(msg.speedkmph) as avg_speed\n",
    "  from\n",
    "    rawdata_user.kafka_hp_maps_historical_streaks_tomtom_nodedup s\n",
    "    join maps_automation.candidate_segments{experiment} c on s.msg.jobuuid = c.trip_id\n",
    "  where\n",
    "    datestr {date_filter}\n",
    "  group by\n",
    "    1, 2, 3, 4\n",
    ")\n",
    "select\n",
    "  t.trip_id,\n",
    "  t.segment_id,\n",
    "  sum(case when (t.startjunctionuuid = s.startjunctionuuid and t.endjunctionuuid = s.endjunctionuuid) then t.streaks else 0 end) as trip_streaks_forward,\n",
    "  sum(case when (t.startjunctionuuid = s.endjunctionuuid and t.endjunctionuuid = s.startjunctionuuid) then t.streaks else 0 end) as trip_streaks_backward,\n",
    "  sum(case when (t.startjunctionuuid = s.startjunctionuuid and t.endjunctionuuid = s.endjunctionuuid) then t.avg_speed else 0 end) as trip_avg_speed_forward,\n",
    "  sum(case when (t.startjunctionuuid = s.endjunctionuuid and t.endjunctionuuid = s.startjunctionuuid) then t.avg_speed else 0 end) as trip_avg_speed_backward\n",
    "from\n",
    "  subset_segments s\n",
    "  join trip_streaks t on s.segment_id = t.segment_id\n",
    "group by\n",
    "  1, 2\n",
    "''',\n",
    "    \n",
    "# query to find segment features\n",
    "'segment_features': '''\n",
    "with candidate_segments as (\n",
    "  select distinct\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    s.uuid as segment_id,\n",
    "    s.data.segment.carriageway,\n",
    "    s.data.segment.controlledaccessroad,\n",
    "    s.data.segment.defaultspeednegkph,\n",
    "    s.data.segment.defaultspeedposkph,\n",
    "    s.data.segment.drivingside,\n",
    "    s.data.segment.groundlevel,\n",
    "    s.data.segment.hovroad,\n",
    "    s.data.segment.inintersection,\n",
    "    s.data.segment.lanecount,\n",
    "    s.data.segment.privateroad,\n",
    "    s.data.segment.roadclass,\n",
    "    s.data.segment.roadusage,\n",
    "    s.data.segment.segmentattributes.maxspeednegkph,\n",
    "    s.data.segment.segmentattributes.maxspeedposkph,\n",
    "    s.data.segment.segmentattributes.nothroughtraffic,\n",
    "    s.data.segment.segmentattributes.trafficdirection,\n",
    "    s.data.segment.surfacetype,\n",
    "    s.data.segment.toll,\n",
    "    s.data.segment.type\n",
    "  FROM\n",
    "    umm.map_feature_segments_tomtom s\n",
    "    inner JOIN dwh.dim_city c on ST_Contains(\n",
    "      ST_GeometryFromText(c.simplified_shape),\n",
    "      ST_Point(\n",
    "        s.geometry.polyline.points [1].lnge7 / 1e7,\n",
    "        s.geometry.polyline.points [1].late7 / 1e7\n",
    "      )\n",
    "    )\n",
    "  where\n",
    "    builduuid = '{builduuid}'\n",
    "    and c.city_id {city_filter}\n",
    ")\n",
    "select\n",
    "  t.candidate_segment,\n",
    "  carriageway,\n",
    "  controlledaccessroad,\n",
    "  defaultspeednegkph,\n",
    "  defaultspeedposkph,\n",
    "  drivingside,\n",
    "  groundlevel,\n",
    "  hovroad,\n",
    "  inintersection,\n",
    "  lanecount,\n",
    "  privateroad,\n",
    "  roadclass,\n",
    "  roadusage,\n",
    "  maxspeednegkph,\n",
    "  maxspeedposkph,\n",
    "  nothroughtraffic,\n",
    "  trafficdirection,\n",
    "  surfacetype,\n",
    "  toll,\n",
    "  type\n",
    "from\n",
    "  subset_segments s\n",
    "  join candidate_segments t on s.segment_id = t.candidate_segment\n",
    "''',\n",
    "    \n",
    "# query to find actual streaks on the segment\n",
    "'actual_streaks': '''\n",
    "with candidate_segments as (\n",
    "  select distinct\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    uuid as segment_id,\n",
    "    data.segment.startjunctionuuid,\n",
    "    data.segment.endjunctionuuid\n",
    "  from\n",
    "    umm.map_feature_segments_tomtom s\n",
    "    join candidate_segments t on s.uuid = t.candidate_segment\n",
    "  where\n",
    "    builduuid = '{builduuid}'\n",
    "),\n",
    "subset_aggregated_actual_streaks_w1 as (\n",
    "  select\n",
    "    msg.graphsegment.segmentuuid,\n",
    "    msg.graphsegment.startjunctionuuid,\n",
    "    msg.graphsegment.endjunctionuuid,\n",
    "    count(uuid) as streaks,\n",
    "    avg(msg.speedkmph) as avg_speed\n",
    "  from\n",
    "    rawdata_user.kafka_hp_maps_historical_streaks_tomtom_nodedup\n",
    "    join candidate_segments on msg.graphsegment.segmentuuid = candidate_segment\n",
    "  where\n",
    "    datestr {date_filter_streaks1}\n",
    "    and msg.classification = 'valid'\n",
    "  group by\n",
    "    1, 2, 3\n",
    "),\n",
    "subset_aggregated_actual_streaks_w2 as (\n",
    "  select\n",
    "    msg.graphsegment.segmentuuid,\n",
    "    msg.graphsegment.startjunctionuuid,\n",
    "    msg.graphsegment.endjunctionuuid,\n",
    "    count(uuid) as streaks,\n",
    "    avg(msg.speedkmph) as avg_speed\n",
    "  from\n",
    "    rawdata_user.kafka_hp_maps_historical_streaks_tomtom_nodedup\n",
    "    join candidate_segments on msg.graphsegment.segmentuuid = candidate_segment\n",
    "  where\n",
    "    datestr {date_filter_streaks2}\n",
    "    and msg.classification = 'valid'\n",
    "  group by\n",
    "    1, 2, 3\n",
    ")\n",
    "select\n",
    "  s.segment_id as segment_id,\n",
    "  sum(case when (aacs1.startjunctionuuid = s.startjunctionuuid and aacs1.endjunctionuuid = s.endjunctionuuid) then aacs1.streaks else 0 end) as actual_streaks_forward_w1,\n",
    "  sum(case when (aacs2.startjunctionuuid = s.startjunctionuuid and aacs2.endjunctionuuid = s.endjunctionuuid) then aacs2.streaks else 0 end) as actual_streaks_forward_w2,\n",
    "  sum(case when (aacs1.startjunctionuuid = s.endjunctionuuid and aacs1.endjunctionuuid = s.startjunctionuuid) then aacs1.streaks else 0 end) as actual_streaks_backward_w1,\n",
    "  sum(case when (aacs2.startjunctionuuid = s.endjunctionuuid and aacs2.endjunctionuuid = s.startjunctionuuid) then aacs2.streaks else 0 end) as actual_streaks_backward_w2,  \n",
    "  sum(case when (aacs1.startjunctionuuid = s.startjunctionuuid and aacs1.endjunctionuuid = s.endjunctionuuid) then aacs1.avg_speed else 0 end) as avg_speed_forward_w1,\n",
    "  sum(case when (aacs2.startjunctionuuid = s.startjunctionuuid and aacs2.endjunctionuuid = s.endjunctionuuid) then aacs2.avg_speed else 0 end) as avg_speed_forward_w2,\n",
    "  sum(case when (aacs1.startjunctionuuid = s.endjunctionuuid and aacs1.endjunctionuuid = s.startjunctionuuid) then aacs1.avg_speed else 0 end) as avg_speed_backward_w1,\n",
    "  sum(case when (aacs2.startjunctionuuid = s.endjunctionuuid and aacs2.endjunctionuuid = s.startjunctionuuid) then aacs2.avg_speed else 0 end) as avg_speed_backward_w2\n",
    "from\n",
    "  subset_segments s\n",
    "  join subset_aggregated_actual_streaks_w1 aacs1 on s.segment_id = aacs1.segmentuuid\n",
    "  join subset_aggregated_actual_streaks_w2 aacs2 on s.segment_id = aacs2.segmentuuid\n",
    "group by\n",
    "  1\n",
    "''',\n",
    "    \n",
    "# query to find suggested streaks on the segment\n",
    "'suggested_streaks': '''\n",
    "with candidate_segments as (\n",
    "  select\n",
    "    distinct candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    uuid as segment_id,\n",
    "    data.segment.startjunctionuuid,\n",
    "    data.segment.endjunctionuuid\n",
    "  from\n",
    "    umm.map_feature_segments_tomtom s\n",
    "    join candidate_segments t on s.uuid = t.candidate_segment\n",
    "  where\n",
    "    builduuid = '{builduuid}'\n",
    ")\n",
    "select\n",
    "  asus.segmentuuid as segment_id,\n",
    "  sum(case when ((asus.startjunctionuuid = s.startjunctionuuid) and (asus.endjunctionuuid = s.endjunctionuuid) and (asus.datestr {date_filter_streaks1})) then asus.suggested_streaks else 0 end) as suggested_streaks_forward_w1,\n",
    "  sum(case when ((asus.startjunctionuuid = s.startjunctionuuid) and (asus.endjunctionuuid = s.endjunctionuuid) and (asus.datestr {date_filter_streaks2})) then asus.suggested_streaks else 0 end) as suggested_streaks_forward_w2,\n",
    "  sum(case when ((asus.startjunctionuuid = s.endjunctionuuid) and (asus.endjunctionuuid = s.startjunctionuuid) and (asus.datestr {date_filter_streaks1})) then asus.suggested_streaks else 0 end) as suggested_streaks_backward_w1,\n",
    "  sum(case when ((asus.startjunctionuuid = s.endjunctionuuid) and (asus.endjunctionuuid = s.startjunctionuuid) and (asus.datestr {date_filter_streaks2})) then asus.suggested_streaks else 0 end) as suggested_streaks_backward_w2\n",
    "from\n",
    "  {db}.agg_suggested_streaks{experiment} asus\n",
    "  join subset_segments s on s.segment_id = asus.segmentuuid\n",
    "group by\n",
    "  1\n",
    "''',\n",
    "    \n",
    "# query to find if the candidate segment was fixed\n",
    "'fixed_segments': '''\n",
    "with candidate_segments as (\n",
    "  select distinct\n",
    "    map_ticket_id,\n",
    "    candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} t\n",
    "    cross join unnest(t.candidate_segments) as temp (candidate_segment)\n",
    "),\n",
    "fixed_segments as (\n",
    "  select distinct\n",
    "    msg.map_ticket_id,\n",
    "    msg.fixed_map_feature_ids as fixed_segment_ids\n",
    "  from\n",
    "    rawdata_user.kafka_hp_umm_hotfix_backend_living_maps_nodedup\n",
    "  where\n",
    "    msg.fixed_map_feature_ids is not null\n",
    "    and cardinality(msg.fixed_map_feature_ids) > 0\n",
    ")\n",
    "select\n",
    "  t.map_ticket_id,\n",
    "  t.candidate_segment,\n",
    "  cast(contains(f.fixed_segment_ids, t.candidate_segment) as tinyint) as fix\n",
    "from\n",
    "  candidate_segments t\n",
    "  join fixed_segments f on t.map_ticket_id = f.map_ticket_id\n",
    "'''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_queries(config):\n",
    "    \"Takes a config object, replaces the parameters in the query templates, returns query dictionary (name: query)\"\n",
    "    queries = {}\n",
    "    for name, query_template in features_query_templates.items():\n",
    "        queries[name] = query_template.format(**config)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features(queries, sleep_time=120):\n",
    "    \"Runs all the queries for a experiment and returns a dictionary of dataframes (name: dataframe)\"\n",
    "    \n",
    "    print('Submitting queries...')\n",
    "    execution_ids = {}\n",
    "    for name, query in queries.items():\n",
    "        execution_id = qr.submit_execution('presto', query, datacenter='dca1', user_email=email)\n",
    "        execution_ids[name] = execution_id\n",
    "    print('Queries submitted.')\n",
    "\n",
    "    while (1):\n",
    "        status = []\n",
    "        for execution_id in execution_ids.values():\n",
    "            status.append(qr.is_complete(execution_id))\n",
    "\n",
    "        if all(status):\n",
    "            print('All queries executed. Collecting results...')\n",
    "            dfs = {}\n",
    "            for name, execution_id in execution_ids.items():\n",
    "                dfs[name] = pd.DataFrame(qr.get_result(execution_id).load_data())\n",
    "            print('Results collected.')\n",
    "            return dfs\n",
    "        else:\n",
    "            print('{}/{} queries executed.'.format(sum(status), len(status)))\n",
    "        \n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collecting features for week 1\n",
    "queries = make_queries(config1)\n",
    "dfs1 = collect_features(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collecting features for week 2\n",
    "queries = make_queries(config2)\n",
    "dfs2 = collect_features(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collecting features for week 3\n",
    "queries = make_queries(config3)\n",
    "dfs3 = collect_features(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_features(dfs):\n",
    "    # Merges all the features for an experiment\n",
    "    \n",
    "    # Ticket based features\n",
    "    merged_df = pd.merge(dfs['candidate_segments'], dfs['report_location_dist'], on=['map_ticket_id', 'candidate_segment'], how='left')\n",
    "    \n",
    "    # Trip based features\n",
    "    merged_df = pd.merge(merged_df, dfs['segment_suggested'], on=['trip_id', 'candidate_segment'], how='left')\n",
    "    merged_df = pd.merge(merged_df, dfs['segment_traversed'], on=['trip_id', 'candidate_segment'], how='left')\n",
    "    merged_df = pd.merge(merged_df, dfs['avg_trip_speed'], left_on=['trip_id', 'candidate_segment'], right_on=['trip_id', 'segment_id'], how='left')\n",
    "    merged_df.drop(['segment_id'], axis=1, inplace=True)\n",
    "    \n",
    "    # Segment based features\n",
    "    merged_df = pd.merge(merged_df, dfs['segment_features'], on='candidate_segment', how='left')\n",
    "\n",
    "    # Aggregate streak based features\n",
    "    merged_df = pd.merge(merged_df, dfs['actual_streaks'], left_on=['candidate_segment'], right_on=['segment_id'], how='left')\n",
    "    merged_df.drop(['segment_id'], axis=1, inplace=True)\n",
    "    merged_df = pd.merge(merged_df, dfs['suggested_streaks'], left_on=['candidate_segment'], right_on=['segment_id'], how='left')\n",
    "    merged_df.drop(['segment_id'], axis=1, inplace=True)\n",
    "\n",
    "    # Training label\n",
    "    merged_df = pd.merge(merged_df, dfs['fixed_segments'], on=['map_ticket_id', 'candidate_segment'], how='left')\n",
    "    \n",
    "    assert len(dfs['candidate_segments']) == len(merged_df) # Number of rows should remain same\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1 = merge_features(dfs1)\n",
    "merged_df2 = merge_features(dfs2)\n",
    "merged_df3 = merge_features(dfs3)\n",
    "merged_df = pd.concat([merged_df1, merged_df2, merged_df3]) # Concatenate merged features for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('merged_df.p', 'wb') as f:\n",
    "    pickle.dump(merged_df, f) # Save the merged df for later use\n",
    "    \n",
    "# with open('three_weeks.p', 'rb') as f:\n",
    "#     merged_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop all the rows with no fix value available\n",
    "df = merged_df[merged_df['fix'].notnull()].reset_index(drop=True).copy() \n",
    "df['fix'] = pd.to_numeric(df['fix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fix'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dates = ['2020-07-23', '2020-07-24', '2020-07-25']\n",
    "test_dates = ['2020-07-27', '2020-07-28', '2020-07-29']\n",
    "train_df = df[~df['report_date'].isin(test_dates)].copy()\n",
    "test_df = df[df['report_date'].isin(test_dates)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns will be one hot encoded\n",
    "categorical_features = ['carriageway', 'drivingside', 'groundlevel', 'roadclass',\n",
    "                        'roadusage', 'trafficdirection', 'surfacetype', 'toll', 'type']\n",
    "# These two lists should be changed in tandem\n",
    "categories = [['SINGLE_CARRIAGE_WAY', 'DUAL_CARRIAGE_WAY'],\n",
    "              ['LEFT', 'RIGHT'],\n",
    "              ['TUNNEL', 'BRIDGE'],\n",
    "              ['MOTORWAY' , 'MINOR_HIGHWAY', 'LOCAL_ROAD', 'MAJOR_ARTERY', 'MINOR_ARTERY', 'HIGHWAY'],\n",
    "              ['SLIP_ROAD', 'WALKWAY', 'PARKING_ROAD', 'CONNECTOR', 'RAMP', 'STAIRS', 'ROUNDABOUT'],\n",
    "              ['BOTH', 'BACKWARD', 'NONE', 'FORWARD'],\n",
    "              ['POOR', 'PAVED'],\n",
    "              ['FORWARD', 'BOTH', 'BACKWARD'],\n",
    "              ['ROAD', 'FERRY']]\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value='na', missing_values=np.nan)\n",
    "categorical_imputer = categorical_imputer.fit(train_df[categorical_features])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(categories=categories, handle_unknown='ignore', sparse=False)\n",
    "one_hot_encoder = one_hot_encoder.fit(categorical_imputer.transform(train_df[categorical_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(t_df, return_feature_names=False):\n",
    "    drop_cols = ['map_ticket_id', 'trip_id', 'city_id', 'report_lat', 'report_long',\n",
    "                 'report_date', 'candidate_segment']\n",
    "    t_df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    convert_to_numeric_cols = ['segment_suggested', 'segment_traversed', 'controlledaccessroad', 'hovroad',\n",
    "                               'inintersection', 'privateroad', 'nothroughtraffic']\n",
    "    for col in convert_to_numeric_cols:\n",
    "        t_df[col] = pd.to_numeric(t_df[col], errors='coerce').astype('float')\n",
    "\n",
    "    t_df[categorical_features] = categorical_imputer.transform(t_df[categorical_features])\n",
    "    \n",
    "    # forward by backward ratios\n",
    "    t_df['actual_streaks_fb_ratio_w1'] = t_df['actual_streaks_forward_w1'] / t_df['actual_streaks_backward_w1']\n",
    "    t_df['actual_streaks_fb_ratio_w2'] = t_df['actual_streaks_forward_w2'] / t_df['actual_streaks_backward_w2']\n",
    "    t_df['avg_speed_fb_ratio_w1'] = t_df['avg_speed_forward_w1'] / t_df['avg_speed_backward_w1']\n",
    "    t_df['avg_speed_fb_ratio_w2'] = t_df['avg_speed_forward_w2'] / t_df['avg_speed_backward_w2']\n",
    "    t_df['suggested_streaks_fb_ratio_w1'] = t_df['suggested_streaks_forward_w1'] / t_df['suggested_streaks_backward_w1']\n",
    "    t_df['suggested_streaks_fb_ratio_w2'] = t_df['suggested_streaks_forward_w2'] / t_df['suggested_streaks_backward_w2']\n",
    "\n",
    "    # suggested by actual ratios\n",
    "    t_df['suggested_actual_forward_ratio_w1'] = t_df['suggested_streaks_forward_w1'] / t_df['actual_streaks_forward_w1']\n",
    "    t_df['suggested_actual_forward_ratio_w2'] = t_df['suggested_streaks_forward_w2'] / t_df['actual_streaks_forward_w2']\n",
    "    t_df['suggested_actual_backward_ratio_w1'] = t_df['suggested_streaks_backward_w1'] / t_df['actual_streaks_backward_w1']\n",
    "    t_df['suggested_actual_backward_ratio_w2'] = t_df['suggested_streaks_backward_w2'] / t_df['actual_streaks_backward_w2']\n",
    "\n",
    "    # w1 by w2 ratios\n",
    "    t_df['actual_streaks_forward_ratio12'] = t_df['actual_streaks_forward_w1'] / t_df['actual_streaks_forward_w2']\n",
    "    t_df['actual_streaks_backward_ratio12'] = t_df['actual_streaks_backward_w1'] / t_df['actual_streaks_backward_w2']\n",
    "    t_df['avg_speed_forward_ratio12'] = t_df['avg_speed_forward_w1'] / t_df['avg_speed_forward_w2']\n",
    "    t_df['avg_speed_backward_ratio12'] = t_df['avg_speed_backward_w1'] / t_df['avg_speed_backward_w2']\n",
    "    t_df['suggested_streaks_forward_ratio12'] = t_df['suggested_streaks_forward_w1'] / t_df['suggested_streaks_forward_w2']\n",
    "    t_df['suggested_streaks_backward_ratio12'] = t_df['suggested_streaks_backward_w1'] / t_df['suggested_streaks_backward_w2']\n",
    "\n",
    "    # trip by aggregate ratios\n",
    "    t_df['avg_speed_forward_ratio_trip_agg1'] = t_df['trip_avg_speed_forward'] / t_df['avg_speed_forward_w1']\n",
    "    t_df['avg_speed_forward_ratio_trip_agg2'] = t_df['trip_avg_speed_forward'] / t_df['avg_speed_forward_w2']\n",
    "    t_df['avg_speed_backward_ratio_trip_agg1'] = t_df['trip_avg_speed_backward'] / t_df['avg_speed_backward_w1']\n",
    "    t_df['avg_speed_backward_ratio_trip_agg2'] = t_df['trip_avg_speed_backward'] / t_df['avg_speed_backward_w2']\n",
    "\n",
    "    X_t = np.concatenate([one_hot_encoder.transform(t_df[categorical_features]),\n",
    "                          t_df.drop(categorical_features, axis=1).values], axis=1)\n",
    "    \n",
    "    if return_feature_names:\n",
    "        feature_names = list(one_hot_encoder.get_feature_names()) + list(t_df.drop(categorical_features, axis=1).columns)\n",
    "        return X_t, feature_names\n",
    "    else:\n",
    "        return X_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['fix']\n",
    "X, feature_names = process_features(train_df.drop(['fix'], axis=1), return_feature_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import xgboost as xgb\n",
    "\n",
    "# required for ranking model\n",
    "group_lengths = [len(list(group)) for key, group in groupby(train_df['map_ticket_id'])]\n",
    "\n",
    "# Split into train and val\n",
    "total_groups = len(group_lengths) # in train and val\n",
    "val_groups = group_lengths[-1 * (total_groups // 5):]\n",
    "train_groups = group_lengths[:-1 * (total_groups // 5)]\n",
    "\n",
    "X_train, y_train = X[:sum(train_groups)], y[:sum(train_groups)]\n",
    "X_val, y_val = X[sum(train_groups):], y[sum(train_groups):]\n",
    "\n",
    "train_dmatrix = xgb.DMatrix(X_train, y_train, feature_names=feature_names)\n",
    "valid_dmatrix = xgb.DMatrix(X_val, y_val, feature_names=feature_names)\n",
    "\n",
    "# for ranking\n",
    "train_dmatrix.set_group(train_groups)\n",
    "valid_dmatrix.set_group(val_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'objective': 'rank:map',\n",
    "          'eval_metric': 'map',\n",
    "          'max_depth': 5, # increase to overfit (default: 6)\n",
    "          'min_child_weight': 1, # increase to underfit (default: 1)\n",
    "          'min_split_loss': 0, # increase to underfit (default: 0)\n",
    "          'subsample': 0.8, # decrease to underfit (default: 1)\n",
    "          'colsample_bytree': 0.5, # decrease to underfit (default: 1)\n",
    "          'learning_rate': 0.1, # decrease to underfit\n",
    "          'verbosity': 3} \n",
    "\n",
    "model = xgb.train(params, train_dmatrix, maximize=True, num_boost_round=200, early_stopping_rounds=20,\n",
    "                  verbose_eval=True, evals=[(train_dmatrix, 'train'), (valid_dmatrix, 'validation')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get fixed segments for all the map tickets\n",
    "\n",
    "query = '''\n",
    "select distinct\n",
    "  msg.map_ticket_id,\n",
    "  msg.fixed_map_feature_ids as fixed_segment_ids\n",
    "from\n",
    "  rawdata_user.kafka_hp_umm_hotfix_backend_living_maps_nodedup\n",
    "where\n",
    "  msg.fixed_map_feature_ids is not null\n",
    "  and cardinality(msg.fixed_map_feature_ids) > 0\n",
    "'''\n",
    "\n",
    "query_result = qr.execute('presto', query)\n",
    "\n",
    "fixed_segments_dict = {} # map_ticket_id: [fixed_segment, fixed_segment, ...]\n",
    "for dict_ in query_result.load_data():\n",
    "    fixed_segments_dict[dict_['map_ticket_id']] = dict_['fixed_segment_ids'].split('\\x02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test set\n",
    "X_test = process_features(test_df.drop(['fix'], axis=1))\n",
    "test_dmatrix = xgb.DMatrix(X_test, feature_names=feature_names)\n",
    "\n",
    "# Run model\n",
    "confidence_df = test_df[['map_ticket_id', 'candidate_segment']].copy()\n",
    "confidence_df['confidence'] = model.predict(test_dmatrix, ntree_limit=model.best_ntree_limit) \n",
    "# confidence is not really a \"confidence\", but an arbitrary score\n",
    "\n",
    "# We group all the segments with their confidence scores for \n",
    "confidence_scores = {} # {map_ticket_id: [(candidate_segment, score), (candidate_segment, score), ...]}\n",
    "for ix, s in confidence_df.iterrows():\n",
    "    if s['map_ticket_id'] in confidence_scores:\n",
    "        confidence_scores[s['map_ticket_id']].append((s['candidate_segment'], s['confidence']))\n",
    "    else:\n",
    "        confidence_scores[s['map_ticket_id']] = [(s['candidate_segment'], s['confidence'])]\n",
    "\n",
    "# Sort the candidate segments according to the scores\n",
    "for map_ticket_id in confidence_scores:\n",
    "    confidence_scores[map_ticket_id] = sorted(confidence_scores[map_ticket_id], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(k):\n",
    "    df = []\n",
    "    for map_ticket_id in confidence_scores:\n",
    "        fixed_segments = set(fixed_segments_dict[map_ticket_id])\n",
    "        candidate_segments = set([x[0] for x in confidence_scores[map_ticket_id]])\n",
    "        top_k_segments = set([x[0] for x in confidence_scores[map_ticket_id]][:k])\n",
    "        df.append({'map_ticket_id': map_ticket_id,\n",
    "                   'candidate_segments': candidate_segments, \n",
    "                   'fixed_segments': fixed_segments,\n",
    "                   'top_k_segments': top_k_segments})\n",
    "    result = pd.DataFrame(df).set_index('map_ticket_id')\n",
    "    \n",
    "    def intersection_cols(df, col1, col2): \n",
    "        return [a.intersection(b) for a, b in zip(df[col1], df[col2])]\n",
    "    \n",
    "    result['candidate_fixed_intersection'] = intersection_cols(result, 'candidate_segments', 'fixed_segments')\n",
    "    result['top_k_fixed_intersection'] = intersection_cols(result, 'top_k_segments', 'fixed_segments')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(1, 6):\n",
    "    results = show_results(k = k)\n",
    "    print(\"k: {}, precision: {}\".format(k, (results['top_k_fixed_intersection'].apply(len) > 0).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(model, color='#0e9453', importance_type='weight')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
