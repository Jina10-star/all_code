{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Thanks for using Drogon for your interactive Spark application. We update Drogon/SparkMagic as often as possible to make it easier, faster and more reliable for you. Have a question or feedback? Ping us on [uChat](https://uchat.uberinternal.com/uber/channels/spark).</span>\n",
    "\n",
    "What's New\n",
    "- Now you can use `%%configure` and `%%spark` magics to configure and start a Spark session (deprecating hard-to-use `%load_ext sparkmagic.magics` and `manage_spark` magics). Check out [this example](https://workbench.uberinternal.com/explore/knowledge/localfile/cwang/sparkmagic_python2_example.ipynb) for more details.\n",
    "- Improved `%%configure` magic. You now can use it to make all Spark and Drogon configurations from within notebook itself. Check out our [latest documentation & examples](https://docs.google.com/document/d/1mkYtDHquh4FjqTeA0Fxii8lyV-P6qzmoABhmmRwm_00/edit#heading=h.xn14pmoorsn0) for more details.\n",
    "- Bug fixes and performance updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"pyFiles\": [], \n",
    "  \"kind\": \"pyspark\", \n",
    "  \"proxyUser\": \"abhishek.sharma\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_automation\", \n",
    "  \"numExecutors\": 1000, \n",
    "  \"conf\": {\n",
    "    \"spark.hadoop.fs.s3a.attempts.maximum\": \"10\",\n",
    "    \"spark.hadoop.fs.s3a.multipart.size\": \"100857600\",\n",
    "    \"spark.hadoop.fs.s3a.block.size\": \"33554432\",\n",
    "    \"spark.hadoop.fs.s3a.threads.keepalivetime\": \"60\",\n",
    "    \"spark.hadoop.fs.s3a.threads.core\": \"64\",\n",
    "    \"spark.hadoop.fs.s3a.threads.max\": \"128\",\n",
    "    \"spark.hadoop.fs.s3a.connection.maximum\": \"64\",\n",
    "    \"spark.sql.catalogImplementation\": \"hive\",\n",
    "    \"spark.yarn.nodemanager.vmem-check-enabled\": \"false\",\n",
    "    \"spark.default.parallelism\" : \"2000\",\n",
    "    \"spark.sql.shuffle.partitions\":\"2000\",\n",
    "      \"spark.yarn.queue\": \"maps_automation\"\n",
    "  }, \n",
    "  \"executorCores\": 2, \n",
    "  \"driverCores\": 2, \n",
    "  \"jars\": [\n",
    "  \"hdfs:///lib/hive/jars/udf/fraud/hadoop-aws-2.8.2.jar\", \"hdfs:///lib/hive/jars/udf/fraud/aws-java-sdk-bundle-1.11.574.jar\"\n",
    "  ], \n",
    "  \"executorMemory\": \"14g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-Drogon-Auth-HDFS-DT\": \"<TOKEN_HERE>\", \n",
    "    \"X-DROGON-CLUSTER\": \"dca1/nonsecure\"\n",
    "  }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = '_23_07_29_07_20c'\n",
    "date_filter = \"between '2020-07-23' and '2020-07-29'\"\n",
    "date_filter_streaks =  \"between '2020-07-09' and '2020-07-22'\"\n",
    "\n",
    "# experiment = '_30_07_05_08_20c2'\n",
    "# date_filter = \"between '2020-07-30' and '2020-08-05'\"\n",
    "# date_filter_streaks =  \"between '2020-07-16' and '2020-07-29'\"\n",
    "\n",
    "# experiment = '_06_08_12_08_20c'\n",
    "# date_filter = \"between '2020-08-06' and '2020-08-12'\"\n",
    "# date_filter_streaks =  \"between '2020-07-23' and '2020-08-05'\"\n",
    "\n",
    "city_filter = \"in (20, 8, 14, 198, 5, 12, 134, 26, 23, 25, 24, 208, 27, 1541, 7, 6, 45, 227, 4, 35)\"\n",
    "city_filter_apostrophe = 'in ' + str(tuple(str(i) for i in eval(city_filter[4:-1])))\n",
    "base_dir = 'hdfs:///user/abhishek.sharma/'\n",
    "db = 'maps_automation'\n",
    "\n",
    "config = {'experiment': experiment, 'date_filter': date_filter, 'date_filter_streaks': date_filter_streaks,\n",
    "          'city_filter': city_filter, 'city_filter_apostrophe': city_filter_apostrophe, 'base_dir': base_dir,\n",
    "          'db': db}\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "hive_context = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_candidate_segments = \"\"\"\n",
    "with subset_tickets as (\n",
    "  select\n",
    "    t.msg.map_ticket_id,\n",
    "    t.msg.trip_id,\n",
    "    c.city_id,\n",
    "    t.msg.problem_location_point.lat as report_lat,\n",
    "    t.msg.problem_location_point.long as report_long,\n",
    "    t.datestr as report_date\n",
    "  from\n",
    "    rawdata_user.kafka_hp_umm_hotfix_backend_living_maps_nodedup t\n",
    "    join dwh.dim_city c on esri.ST_Contains(\n",
    "      esri.ST_GeomFromText(c.simplified_shape),\n",
    "      esri.ST_Point(\n",
    "        t.msg.problem_location_point.long,\n",
    "        t.msg.problem_location_point.lat\n",
    "      )\n",
    "    )\n",
    "  where\n",
    "    t.msg.env = 'production'\n",
    "    and t.msg.reporter_context = 'mobile'\n",
    "    and t.msg.map_ticket_state = 'REPORTED'\n",
    "    and t.datestr {date_filter}\n",
    "    and c.city_id {city_filter}\n",
    "),\n",
    "subset_segments as (\n",
    "  select\n",
    "    uuid as segment_id,\n",
    "    geometry.polyline.points [0].lnge7 / 1e7 as segment_start_long,\n",
    "    geometry.polyline.points [0].late7 / 1e7 as segment_start_lat,\n",
    "    geometry.polyline.points [size(geometry.polyline.points) - 1].lnge7 / 1e7 as segment_end_long,\n",
    "    geometry.polyline.points [size(geometry.polyline.points) - 1].late7 / 1e7 as segment_end_lat\n",
    "  FROM\n",
    "    umm.map_feature_segments_tomtom\n",
    "  where\n",
    "    builduuid = '6dc8e44c-be20-11ea-bb97-000af7f88c50'\n",
    "),\n",
    "suggested_segments as (\n",
    "  select\n",
    "    t.map_ticket_id as map_ticket_id,\n",
    "    collect_set(t.segment.segmentuuid) as suggested_segments\n",
    "  from\n",
    "    subset_tickets t\n",
    "    join rawdata_user.kafka_hp_gurafu_route_logs_nodedup su on su.msg.tripuuid = t.trip_id \n",
    "    lateral view explode (su.msg.segments) t as segment\n",
    "  where\n",
    "    su.datestr {date_filter}\n",
    "    and su.msg.cityid {city_filter}\n",
    "  group by\n",
    "    1\n",
    "),\n",
    "near_segments as (\n",
    "  select\n",
    "    t.map_ticket_id,\n",
    "    t.trip_id,\n",
    "    t.city_id,\n",
    "    t.report_lat,\n",
    "    t.report_long,\n",
    "    t.report_date,    \n",
    "    collect_list(s.segment_id) as near_segments,\n",
    "    collect_list(\n",
    "      case\n",
    "        when(\n",
    "          esri.ST_GeodesicLengthWGS84(\n",
    "            esri.ST_SetSRID(\n",
    "              esri.ST_Linestring(\n",
    "                s.segment_start_long,\n",
    "                s.segment_start_lat,\n",
    "                t.report_long,\n",
    "                t.report_lat\n",
    "              ),\n",
    "              4326\n",
    "            )\n",
    "          ) < 50\n",
    "          or esri.ST_GeodesicLengthWGS84(\n",
    "            esri.ST_SetSRID(\n",
    "              esri.ST_Linestring(\n",
    "                s.segment_end_long,\n",
    "                s.segment_end_lat,\n",
    "                t.report_long,\n",
    "                t.report_lat\n",
    "              ),\n",
    "              4326\n",
    "            )\n",
    "          ) < 50\n",
    "        )\n",
    "        then s.segment_id\n",
    "        else null\n",
    "      end\n",
    "    ) as nearer_segments\n",
    "  from\n",
    "    subset_tickets t\n",
    "    join subset_segments s on esri.ST_GeodesicLengthWGS84(\n",
    "      esri.ST_SetSRID(\n",
    "        esri.ST_Linestring(\n",
    "          s.segment_start_long,\n",
    "          s.segment_start_lat,\n",
    "          t.report_long,\n",
    "          t.report_lat\n",
    "        ),\n",
    "        4326\n",
    "      )\n",
    "    ) < 300\n",
    "    or esri.ST_GeodesicLengthWGS84(\n",
    "      esri.ST_SetSRID(\n",
    "        esri.ST_Linestring(\n",
    "          s.segment_end_long,\n",
    "          s.segment_end_lat,\n",
    "          t.report_long,\n",
    "          t.report_lat\n",
    "        ),\n",
    "        4326\n",
    "      )\n",
    "    ) < 300\n",
    "  group by\n",
    "    1, 2, 3, 4, 5, 6\n",
    ")\n",
    "select\n",
    "  n.map_ticket_id,\n",
    "  n.trip_id,\n",
    "  n.city_id,\n",
    "  n.report_lat,\n",
    "  n.report_long,\n",
    "  n.report_date,\n",
    "  etl.bhouse_array_union(\n",
    "    etl.bhouse_intersect_array(su.suggested_segments, n.near_segments),\n",
    "    n.nearer_segments\n",
    "  ) as candidate_segments\n",
    "from\n",
    "  suggested_segments su\n",
    "  join near_segments n on n.map_ticket_id = su.map_ticket_id\n",
    "\"\"\".format(**config)\n",
    "\n",
    "candidate_segments_df = hive_context.sql(query_candidate_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_location = '{base_dir}{db}_candidate_segments{experiment}'.format(**config)\n",
    "candidate_segments_df.coalesce(1000).write.mode(\"overwrite\").parquet(parquet_location)\n",
    "df = spark.read.parquet(parquet_location)\n",
    "df.coalesce(10).write.saveAsTable(\"{db}.candidate_segments{experiment}\".format(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agg_suggested_streaks = '''\n",
    "with candidate_segments as (\n",
    "  select\n",
    "    distinct t.candidate_segment\n",
    "  from\n",
    "    {db}.candidate_segments{experiment} c lateral view explode(c.candidate_segments) t as candidate_segment\n",
    "),\n",
    "suggested_routes as (\n",
    "  select\n",
    "    t.datestr,\n",
    "    t.segments as suggested_route\n",
    "  FROM\n",
    "    (\n",
    "      select\n",
    "        uuid\n",
    "      from\n",
    "        dwh.fact_trip ft\n",
    "      where\n",
    "        datestr {date_filter_streaks}\n",
    "        and city_id {city_filter}\n",
    "        and marketplace = 'personal_transport'\n",
    "    ) ft\n",
    "    join (\n",
    "      select\n",
    "        msg.tripUuid,\n",
    "        datestr,\n",
    "        msg.segments\n",
    "      from\n",
    "        rawdata_user.kafka_hp_gurafu_route_logs_nodedup\n",
    "      where\n",
    "        datestr {date_filter_streaks}\n",
    "        and msg.cityid {city_filter}\n",
    "        and msg.experimentname is NULL\n",
    "    ) t on t.tripUuid = ft.uuid\n",
    "),\n",
    "suggested_segments as (\n",
    "  select\n",
    "    datestr,\n",
    "    x.segment.segmentuuid,\n",
    "    x.segment.startjunctionuuid,\n",
    "    x.segment.endjunctionuuid\n",
    "  from\n",
    "    suggested_routes lateral view explode (suggested_route) x as segment\n",
    ")\n",
    "select\n",
    "  s.segmentuuid,\n",
    "  s.datestr,\n",
    "  s.startjunctionuuid,\n",
    "  s.endjunctionuuid,\n",
    "  count(1) as suggested_streaks\n",
    "from\n",
    "  candidate_segments c\n",
    "  join suggested_segments s on s.segmentuuid = c.candidate_segment\n",
    "group by\n",
    "  1, 2, 3, 4\n",
    "'''.format(**config)\n",
    "\n",
    "agg_suggested_streaks_df = hive_context.sql(query_agg_suggested_streaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_location = '{base_dir}{db}_agg_suggested_streaks{experiment}'.format(**config)\n",
    "agg_suggested_streaks_df.coalesce(1000).write.mode(\"overwrite\").parquet(parquet_location)\n",
    "df = spark.read.parquet(parquet_location)\n",
    "df.coalesce(10).write.saveAsTable(\"{db}.agg_suggested_streaks{experiment}\".format(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close spark sesssion\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkMagic(Remote Python3)",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
