#!/usr/bin/python

# this works with python 2
# estimate logistic regression performance using cross-validation
# try different sets of features

import logging
import random
import numpy as np
import pandas as pd
import datetime
import nltk

from contact_type_blacklist import *
from evaluate_classifier import repeated_eval_logistic_regression

keywords = ['destination', 'address', 'pick', 'drop', 'place', 'map', 'navigation', 'location',
            'routing', 'route', 'door', 'entrance', 'gps']

# nltk.download('stopwords')
stop_words = set(nltk.corpus.stopwords.words('english'))

NUM_TOP_WORDS = 20000
MAX_SEQUENCE_LENGTH = 256
EMBEDDING_DIM = 100
bliss_embeddings_path = "/Users/lyanez/Uber/BlissPlace/embeddings/bliss_embeddings_20180308/embeddings_100d.txt"
glove_embeddings_path = "/Users/lyanez/Uber/BlissPlace/embeddings/glove.6B/glove.6B.100d.txt"
labeled_tickets_path = "/Users/lyanez/Uber/BlissPlace/ticket_training_data_combined/tickets_with_contact_type.tsv"

NUM_REPEATS = 10
PRECISION_TARGET = 0.5

np.random.seed(0)
random.seed(0)

# uncomment lines to switch between logging and printing out results


def print_result():
    # return print
    return logger.info


def does_column_match_val(data, attr_col_name, target_attr):
    return np.array(data[attr_col_name].apply(lambda x: x == target_attr))


def sentence_to_vec(sentence, embeddings_index):

    # tokenize into individual words
    words_list = sentence.split()

    # remove stop words
    filtered_words_list = [w for w in words_list if w not in stop_words]

    output_embedding = np.zeros(EMBEDDING_DIM)
    num_words_found = 0.0
    num_words_in_embedding = 0.0
    any_keyword = False

    # take average of all word embeddings found in ticket
    # words not found in embedding are represented as vector of zeros
    for word in filtered_words_list:
        if word in embeddings_index:
            output_embedding += embeddings_index[word]
            num_words_in_embedding += 1.0
        num_words_found += 1.0
        if word in keywords:
            any_keyword = True

    if (num_words_in_embedding < 1):  # and (any_keyword == False):
        return (False, output_embedding)
    else:
        return (True, output_embedding/num_words_in_embedding)


def load_data(labeled_data_path):

    # this data file was generated by workbench file "blissplace_build_contact_types_blacklist.ipynb"
    # https://workbench.uberinternal.com/file/63804a54-24a7-402a-8292-c9e6c76bd858
    labeled_data = pd.read_csv(labeled_data_path, sep="\t")
    labeled_data = labeled_data[["ticket_id", "text_cleaned", "label", "creation_type_uuid",
                                 "creation_type_name", "csat_type_uuid", "csat_type_name", "source"]]
    #labeled_data = labeled_data[labeled_data["source"].isin(["keyword"])]
    labeled_data = labeled_data.reset_index()

    # print out some stats on composition of labeled data
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "pct of moved-place tickets which are positive: {0:0.3f}"
                                         .format(np.mean(labeled_data[labeled_data["source"] == "confirmed_moved"]["label"]))))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "pct of provider-refresh tickets which are positive: {0:0.3f}"
                                         .format(np.mean(labeled_data[labeled_data["source"] == "provider_refresh_1"]["label"]))))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "pct of keyword-sampled tickets which are positive: {0:0.3f}"
                                         .format(np.mean(labeled_data[labeled_data["source"] == "keyword"]["label"]))))
    return labeled_data


def load_embeddings(embeddings_path, embeddings_name):

    embeddings_index = {}
    f = open(embeddings_path)
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()

    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Found {0:d} word vectors in {1:s} embeddings".format(len(embeddings_index), embeddings_name)))
    return embeddings_index


def embed_data(labeled_data, embeddings_index, embeddings_name):

    # tokenize and convert to numpy matrix
    labels = np.asarray(list(labeled_data["label"]))
    texts = list(labeled_data["text_cleaned"])
    tokenized_texts = [sentence_to_vec(text, embeddings_index) for text in texts]

    # only keep tickets as determined in "sentence_to_vec"
    data_out = []
    labels_out = []
    ids = []

    for ticket_id, (keep, vec), label in zip(labeled_data["index"], tokenized_texts, labels):
        if keep:
            ids.append(ticket_id)
            data_out.append(vec)
            labels_out.append(label)

    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "{0:s} embeddings: kept {1:d} out of {2:d} tickets".format(embeddings_name, len(labels_out), len(labels))))
    data_out = np.asarray(data_out)
    labels_out = np.asarray(labels_out)

    return data_out, labels_out, ids


def add_contact_type_feature(data_embeddings, data_full, ids_to_keep):
    """
    - add contact type features as additional columns
    - define column mask to select only certain contact types
    - define row mask to auto-predict negative class on tickets of blacklisted contact types

    :param data_embeddings: numpy array of embedded data, dims = num_samples x embedding_dim
    :param labels: numpy array of labels with values in {0,1}
    :param data_full: original dataframe containing contact type info
    :return:
    """

    data_full = data_full[data_full["index"].isin(ids_to_keep)].copy(deep=True)

    # get unique contact types for creation and csat types
    contact_types_creation = list(set(data_full["creation_type_uuid"]))
    contact_types_csat = list(set(data_full["csat_type_uuid"]))

    # build row mask: indices where the creation or csat type is blacklisted
    row_mask_creation = np.where(
        np.array(data_full["creation_type_uuid"].isin(creation_type_blacklist)))[0]
    row_mask_csat = np.where(np.array(data_full["csat_type_uuid"].isin(csat_type_blacklist)))[0]
    row_mask_both = np.array(map(int, list(set(row_mask_creation).union(set(row_mask_csat)))))

    # build col mask: indices where the feature refers to a creation or csat contact type
    col_mask_creation = np.where(np.array([True] * EMBEDDING_DIM + [True] * len(contact_types_creation) +
                                          [False] * len(contact_types_csat)))[0]
    col_mask_csat = np.where(np.array([True] * EMBEDDING_DIM + [False] * len(contact_types_creation) +
                                      [True] * len(contact_types_csat)))[0]
    col_mask_both = np.array(map(int, list(set(col_mask_creation).union(set(col_mask_csat)))))

    # add one-hot encoded columns for both creation and csat contact types
    match_types_creation = np.array([does_column_match_val(data_full, "creation_type_uuid", contact_type)
                                     for contact_type in contact_types_creation])
    match_types_csat = np.array([does_column_match_val(data_full, "csat_type_uuid", contact_type)
                                 for contact_type in contact_types_csat])

    data_out = np.hstack((data_embeddings, match_types_creation.T, match_types_csat.T))

    return {"data": data_out,
            "row_mask": {"creation": row_mask_creation, "csat": row_mask_csat, "both": row_mask_both},
            "col_mask": {"creation": col_mask_creation, "csat": col_mask_csat, "both": col_mask_both}}


def build_df_row(cv_stats, creation_features_used, csat_features_used, embeddings_used, blacklisted_tickets, index):
    """

    :param cv_stats: the stats["cross_validation"] coming out of repeated_eval_logistic_regression
    :param creation_features_used: bool signifying if ticket creation type features were used
    :param csat_features_used: bool signifying if csat creation type features were used
    :param embeddings_used: String describing which embeddings were used
    :param blacklisted_tickets: bool signifying if tickets with blacklisted contact types were
                auto-classified as negative
    :param index: the index to add in the dataframe (required by pandas)
    :return: a 1-row dataframe to be appended to the one containing all the different model results being compared
    """

    curr_dict = cv_stats
    curr_dict["creation_used"] = creation_features_used
    curr_dict["csat_used"] = csat_features_used
    curr_dict["embeddings_used"] = embeddings_used
    curr_dict["blacklisted_tickets"] = blacklisted_tickets
    return pd.DataFrame(curr_dict, index=[index])


def run():

    # ########################################################################
    # 1. load in labeled ticket data with contact types added
    # ########################################################################
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "Loading data..."))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    labeled_tickets = load_data(labeled_tickets_path)

    # ########################################################################
    # 2. load in embeddings
    # ########################################################################
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "Loading embeddings..."))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    glove_embeddings_index = load_embeddings(glove_embeddings_path, "glove")
    bliss_embeddings_index = load_embeddings(bliss_embeddings_path, "bliss")

    # ########################################################################
    # 3. apply embeddings
    # ########################################################################
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "Applying embeddings..."))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    data_glove, labels_glove, _ = embed_data(labeled_tickets, glove_embeddings_index, "glove")
    data_bliss, labels_bliss, ids = embed_data(labeled_tickets, bliss_embeddings_index, "bliss")

    # ########################################################################
    # 4. try adding contact type as a feature to bliss-embedded data
    #    try just creation contact type, csat contact type, or both
    # ########################################################################
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Adding contact type as a feature..."))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    data_with_ct = add_contact_type_feature(data_bliss, labeled_tickets, ids)

    # ########################################################################
    # 5. evaluate various combinations of features
    # ########################################################################
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "Evaluating results..."))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    all_results = pd.DataFrame(columns=["auc_roc", "auc_pr", "accuracy", "precision", "recall",
                                        "creation_used", "csat_used",  "embeddings_used", "blacklisted_tickets"])

    # bliss with creation contact types only
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Bliss embeddings, creation contact type added"))
    _, _, stats = repeated_eval_logistic_regression(data_with_ct["data"][:, data_with_ct["col_mask"]["creation"]],
                                                    labels_bliss, NUM_REPEATS, PRECISION_TARGET)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=True,
                              csat_features_used=False, embeddings_used="bliss", blacklisted_tickets=False,
                              index=0)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # bliss with csat contact types only
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Bliss embeddings, csat contact type added"))
    _, _, stats = repeated_eval_logistic_regression(data_with_ct["data"][:, data_with_ct["col_mask"]["csat"]],
                                                    labels_bliss, NUM_REPEATS, PRECISION_TARGET)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=False,
                              csat_features_used=True, embeddings_used="bliss", blacklisted_tickets=False,
                              index=1)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # bliss with both creation and csat contact types
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Bliss embeddings, both contact types added"))
    _, _, stats = repeated_eval_logistic_regression(data_with_ct["data"][:, data_with_ct["col_mask"]["both"]],
                                                    labels_bliss, NUM_REPEATS, PRECISION_TARGET)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=True,
                              csat_features_used=True, embeddings_used="bliss", blacklisted_tickets=False,
                              index=2)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # bliss with auto-prediction of 0 for blacklisted contact types
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "Bliss embeddings, auto-prediction of 0" +
                                         " for blacklisted contact types"))
    _, _, stats = repeated_eval_logistic_regression(data_bliss, labels_bliss, NUM_REPEATS, PRECISION_TARGET,
                                                    mask_locs=data_with_ct["row_mask"]["both"],
                                                    mask_fill_val=0.0)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=False,
                              csat_features_used=False, embeddings_used="bliss", blacklisted_tickets=True,
                              index=3)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # bliss, no filtering by contact type
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Bliss embeddings, no contact type info added"))
    _, _, stats = repeated_eval_logistic_regression(
        data_bliss, labels_bliss, NUM_REPEATS, PRECISION_TARGET)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=False,
                              csat_features_used=False, embeddings_used="bliss", blacklisted_tickets=False,
                              index=4)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # glove, no filtering by contact type
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         "Glove embeddings, no contact type info added"))
    _, _, stats = repeated_eval_logistic_regression(
        data_glove, labels_glove, NUM_REPEATS, PRECISION_TARGET)
    curr_stats = build_df_row(cv_stats=stats["cross_validation"], creation_features_used=False,
                              csat_features_used=False, embeddings_used="glove", blacklisted_tickets=False,
                              index=5)
    all_results = all_results.append(curr_stats.copy(deep=True))

    # combine into single dataframe to print out
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(),
                                         all_results[["creation_used", "csat_used", "embeddings_used", "blacklisted_tickets",
                                                      "auc_pr", "auc_roc", "precision", "recall"]]))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), ""))
    print_result()("{0:s}\t{1:s}".format(datetime.datetime.now(), "all done"))


if __name__ == '__main__':

    """
    Measure performance of word embeddings + logistic regression model
        - Try different sets of features such as contact type
    """

    entry_log_path = "train_lr_model.log"
    logger = logging.getLogger(__name__)
    log_path = entry_log_path
    fhandler = logging.FileHandler(filename=log_path, mode='a')
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fhandler.setFormatter(formatter)
    logger.addHandler(fhandler)
    logger.setLevel(logging.DEBUG)

    print("log location is %s" % entry_log_path)

    run()
