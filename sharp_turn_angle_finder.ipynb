{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Thanks for using Drogon for your interactive Spark application. We update Drogon/SparkMagic as often as possible to make it easier, faster and more reliable for you. Have a question or feedback? Ping us on [uChat](https://uchat.uberinternal.com/uber/channels/spark).</span>\n",
    "\n",
    "What's New\n",
    "- Now you can use `%%configure` and `%%spark` magics to configure and start a Spark session (deprecating hard-to-use `%load_ext sparkmagic.magics` and `manage_spark` magics). Check out [this example](https://workbench.uberinternal.com/explore/knowledge/localfile/cwang/sparkmagic_python2_example.ipynb) for more details.\n",
    "- Improved `%%configure` magic. You now can use it to make all Spark and Drogon configurations from within notebook itself. Check out our [latest documentation & examples](https://docs.google.com/document/d/1mkYtDHquh4FjqTeA0Fxii8lyV-P6qzmoABhmmRwm_00/edit#heading=h.xn14pmoorsn0) for more details.\n",
    "- Bug fixes and performance updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"kind\": \"spark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_route_analytics\", \n",
    "  \"numExecutors\": 100, \n",
    "  \"executorCores\": 1, \n",
    "  \"driverCores\": 4,\n",
    "  \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.executor.memoryOverhead\": 3072, \n",
    "    \"spark.locality.wait\": \"0\",\n",
    "    \"spark.default.parallelism\":10000\n",
    "  },\n",
    "  \"executorMemory\": \"24g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"PHX2/Secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * This section defines all the objects will be used in the following algorithm.\n",
    " */\n",
    "case class Location (\n",
    "    latitude: Double,\n",
    "    longitude: Double\n",
    ") extends Serializable\n",
    "\n",
    "case class Point (\n",
    "    latE7: Integer,\n",
    "    lngE7: Integer\n",
    ") extends Serializable\n",
    "\n",
    "case class Polyline (\n",
    "    points: List[Location]\n",
    ") extends Serializable\n",
    "\n",
    "case class Geometry(\n",
    "    polyline: Polyline\n",
    ") extends Serializable\n",
    "\n",
    "case class DirectedSegment (\n",
    "    uuid: String,\n",
    "    direction: String\n",
    ") extends Serializable\n",
    "\n",
    "case class Segment (\n",
    "    startJunctionUuid: String,\n",
    "    endJunctionUuid: String \n",
    ") extends Serializable\n",
    "\n",
    "case class TurnRestrictionMapFeature (\n",
    "    uuid: String,\n",
    "    segments: List[DirectedSegment]    \n",
    ") extends Serializable\n",
    "\n",
    "case class TurnRestrictionMapFeatures (\n",
    "    features: List[TurnRestrictionMapFeature]    \n",
    ") extends Serializable\n",
    "\n",
    "case class SegmentMapFeature(\n",
    "    uuid: String,\n",
    "    locations: List[Location]\n",
    ") extends Serializable\n",
    "\n",
    "case class JoinedManeuvers (\n",
    "    uuid: String,\n",
    "    segments: List[DirectedSegment],\n",
    "    segment_lines: List[SegmentMapFeature]\n",
    ") extends Serializable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Input params for the algorithm\n",
    " */\n",
    "val startDate= \"2022-12-11\"\n",
    "val endDate = \"2022-12-17\"\n",
    "val ummVersion = \"f905b99a-8137-11ed-9118-000af7d19b40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads maneuvers map features from umm.map_feature_maneuvers_tomtom\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "object TurnRestrictionMapFeatureLoader extends Serializable {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(builduuid: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | uuid,\n",
    "         | data.maneuver.segments as segments\n",
    "         | from umm.map_feature_maneuvers_tomtom\n",
    "         | where builduuid = '$builduuid'\n",
    "         | and data.maneuver.type in ('FORBIDDEN_MANEUVER','FORBIDDEN_U_TURN')\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[TurnRestrictionMapFeature] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        var segments = ListBuffer[DirectedSegment]()\n",
    "        r.getAs[Seq[Row]](\"segments\").foreach(row => segments += DirectedSegment(\n",
    "                                                                    row.getAs[String](\"uuid\"),\n",
    "                                                                    row.getAs[String](\"direction\")\n",
    "                                                                )\n",
    "                                             )\n",
    "        \n",
    "        val subSegments = segments.toList.sliding(2).toList\n",
    "        \n",
    "        var result = ListBuffer[TurnRestrictionMapFeature]()\n",
    "        \n",
    "        subSegments.foreach(ss => result += TurnRestrictionMapFeature(\n",
    "            uuid = r.getAs[String](\"uuid\"),\n",
    "            segments = ss\n",
    "          ))\n",
    "        \n",
    "        TurnRestrictionMapFeatures(result.toList)\n",
    "    }).flatMap(_.features)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load maneuvers map features from umm.map_feature_maneuvers_tomtom\n",
    "*/\n",
    "val turnRestrictionMapFeaturesRaw = TurnRestrictionMapFeatureLoader.load(ummVersion)\n",
    "val turnRestrictionMapFeatures = TurnRestrictionMapFeatureLoader.makeDataset(turnRestrictionMapFeaturesRaw).cache()\n",
    "turnRestrictionMapFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads segments map features from umm.map_feature_segments_tomtom\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object SegmentMapFeatureLoader extends Serializable {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(builduuid: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | uuid,\n",
    "         | data.segment as segment,\n",
    "         | geometry\n",
    "         | from umm.map_feature_segments_tomtom\n",
    "         | where builduuid = '$builduuid'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[SegmentMapFeature] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        \n",
    "        val segment = Segment(r.getAs[Row](\"segment\").getAs[String](\"startJunctionUuid\"),\n",
    "                              r.getAs[Row](\"segment\").getAs[String](\"endJunctionUuid\")\n",
    "                      )\n",
    "        \n",
    "        var locations = ListBuffer[Location]()\n",
    "        val polylineRaw = r.getAs[Row](\"geometry\").getAs[Row](\"polyline\")\n",
    "        if(polylineRaw != null) {\n",
    "            polylineRaw.getAs[Seq[Row]](\"points\").foreach(pt => {\n",
    "                    val lat = pt.getAs[Integer](\"latE7\") / 1.0E7\n",
    "                    val long = pt.getAs[Integer](\"lngE7\") / 1.0E7\n",
    "                    locations += Location(lat, long)\n",
    "                }\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        SegmentMapFeature(\n",
    "            r.getAs[String](\"uuid\"),\n",
    "            locations.toList\n",
    "          )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load segments map features from umm.map_feature_segments_tomtom\n",
    "*/\n",
    "val segmentMapFeaturesRaw = SegmentMapFeatureLoader.load(ummVersion)\n",
    "val segmentMapFeatures = SegmentMapFeatureLoader.makeDataset(segmentMapFeaturesRaw).cache()\n",
    "segmentMapFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Join maneuvers and segments to associate polylines to maneuvers.\n",
    "val joinedManeuvers = turnRestrictionMapFeatures.withColumn(\"segment\", explode(col(\"segments\"))).alias(\"M\").\n",
    "                        joinWith(segmentMapFeatures.alias(\"S\"), col(\"M.segment.uuid\")===col(\"S.uuid\")).\n",
    "                        groupBy(col(\"_1.uuid\"),col(\"_1.segments\")).\n",
    "                        agg(collect_list(\"_2\").as(\"segment_lines\")).\n",
    "                        cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Utils class to calculate turn angle between two segments.\n",
    "object Utils {\n",
    "    \n",
    "    def convertDeltaLongitudeToMeters(atLatitude: Double): Double = {\n",
    "        Math.cos(Math.toRadians(atLatitude)) * 111319.49079327357\n",
    "    }\n",
    "    \n",
    "    def approxAtan2(y: Double, x: Double): Double = {\n",
    "        var absY:Double = Math.abs(y);\n",
    "        var r:Double = 1.0;\n",
    "        var angle:Double = 0.0;\n",
    "        var xPlusAbsY:Double  = x + absY;\n",
    "        var xMinusAbsY:Double  = x - absY;\n",
    "        if (x < 0.0) {\n",
    "          if (xMinusAbsY == 0.0) {\n",
    "            // avoid divide by zero\n",
    "            xMinusAbsY = 1E-12;\n",
    "          }\n",
    "          r = -xPlusAbsY / xMinusAbsY;\n",
    "          angle = 2.356194490192345;\n",
    "        } else {\n",
    "          if (xPlusAbsY == 0.0) {\n",
    "            // avoid divide by zero\n",
    "            xPlusAbsY = 1E-12;\n",
    "          }\n",
    "          r = xMinusAbsY / xPlusAbsY;\n",
    "          angle = 0.7853981633974483;\n",
    "        }\n",
    "        angle += (0.1963 * r * r - 0.9817) * r;\n",
    "        // negate if in quadrant III or IV\n",
    "        if(y < 0.0) {\n",
    "            return  -angle;\n",
    "        } else {\n",
    "            return angle\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def fastApproxAzimuth(point1: Location, point2: Location): Option[Double] = {\n",
    "        \n",
    "        val verticalDistance:Double = (point2.latitude - point1.latitude) * 111319.49079327357;\n",
    "        val horizontalDistance:Double = (point2.longitude - point1.longitude) * convertDeltaLongitudeToMeters(point1.latitude);\n",
    "\n",
    "        var angleDegrees:Double =\n",
    "            Math.toDegrees(approxAtan2(verticalDistance, horizontalDistance));\n",
    "\n",
    "        // atan2 is on a standard unit circle - that means an angle of 0 corresponds to East and\n",
    "        // increases as you go counter-clockwise.  According to the official Azimuth definition, we\n",
    "        // want East to be 90, and increase as you go clockwise.\n",
    "        angleDegrees = 90 - angleDegrees;\n",
    "        return wrapAngle180(angleDegrees);\n",
    "    }\n",
    "    \n",
    "    def wrapAngle180(angleDegreesParam: Double): Option[Double] = {\n",
    "        val angleDegrees = angleDegreesParam % 360;\n",
    "        if(angleDegrees < -360 && angleDegrees > 360) {\n",
    "            return Option.empty\n",
    "        }\n",
    "        // wrapping to [-180,180)\n",
    "        if (angleDegrees >= 180) {\n",
    "          return Some(angleDegrees - 360);\n",
    "        } else if (angleDegrees < -180) {\n",
    "          return Some(angleDegrees + 360);\n",
    "        } else {\n",
    "          return Some(angleDegrees);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// compute the angle between two segments\n",
    "val angles = joinedManeuvers.\n",
    "filter(man => man.getAs[String](\"uuid\") == \"63c38201-7161-949d-4764-200176e7687a\").\n",
    "map(man => {\n",
    "    val segmentLines = man.getAs[Seq[Row]](\"segment_lines\")\n",
    "    \n",
    "    var segments = ListBuffer[SegmentMapFeature]()\n",
    "    segmentLines.foreach(segment => {\n",
    "        var locations = ListBuffer[Location]()\n",
    "        val polyline = segment.getAs[Seq[Row]](\"locations\")\n",
    "        polyline.foreach(pt => {\n",
    "                val lat = pt.getAs[Double](\"latitude\")\n",
    "                val long = pt.getAs[Double](\"longitude\")\n",
    "                locations += Location(lat, long)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        segments += SegmentMapFeature(\"\", locations.toList)\n",
    "    })\n",
    "    \n",
    "    val point1 = segments.head.locations.head\n",
    "    val point2 = segments.head.locations.last\n",
    "    val point3 = segments.last.locations.head\n",
    "    val point4 = segments.last.locations.last\n",
    "    \n",
    "    val bearing1 = Utils.fastApproxAzimuth(point1, point2)\n",
    "    val bearing2 = Utils.fastApproxAzimuth(point3, point4)\n",
    "        \n",
    "    if(bearing1.isDefined && bearing2.isDefined) {\n",
    "        Utils.wrapAngle180(bearing2.get - bearing1.get)\n",
    "    }\n",
    "    else {\n",
    "        None\n",
    "    }\n",
    "}).\n",
    "filter(r => r.isDefined).\n",
    "map(angle => Math.round(angle.get)).\n",
    "cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val total = angles.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
