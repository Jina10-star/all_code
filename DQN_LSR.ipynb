{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install bayesian-optimization\n",
    "%pip install dataclasses\n",
    "%pip install matching-ds-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "# from bayes_opt import BayesianOptimization\n",
    "import torch\n",
    "from mini_sim.DQN_offlineData import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from queryrunner_client import Client\n",
    "USER_EMAIL = 'ssadeghi@uber.com'\n",
    "qclient = Client(user_email=USER_EMAIL)\n",
    "CONSUMER_NAME = 'intelligentdispatch'\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#num_cores = multiprocessing.cpu_count()  -- 48\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from queryrunner_client import Client as QRClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import mdstk\n",
    "from mdstk.data_fetcher.data_fetcher import DataFetcher\n",
    "from mdstk.data_fetcher.cached_data_fetcher import CachedDataFetcher\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- San Francisco\n",
    "prefix = 'LSR'\n",
    "hex_digits = '34'\n",
    "\n",
    "city_id = 1\n",
    "vvid = 8\n",
    "\n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-11',\n",
    "    '2022-11-12',\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "]\n",
    "\n",
    "MAX_LAT = 38.19\n",
    "MIN_LAT = 37.09\n",
    "MAX_LNG = -121.55\n",
    "MIN_LNG = -122.60\n",
    "\n",
    "-- Detroit\n",
    "prefix = 'LSR'\n",
    "hex_digits = '34'\n",
    "\n",
    "city_id = 50\n",
    "vvid = 425\n",
    "\n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-11',\n",
    "    '2022-11-12',\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "]\n",
    "\n",
    "MAX_LAT = 42.89\n",
    "MIN_LAT = 42.01\n",
    "MAX_LNG = -82.68\n",
    "MIN_LNG = -83.93\n",
    "\n",
    "\n",
    "-- Philadelphia\n",
    "prefix = 'LSR'\n",
    "hex_digits = '34'\n",
    "\n",
    "city_id = 20\n",
    "vvid = 663\n",
    "\n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-11',\n",
    "    '2022-11-12',\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "]\n",
    "\n",
    "MAX_LAT = 40.22\n",
    "MIN_LAT = 39.74\n",
    "MAX_LNG = -74.86\n",
    "MIN_LNG = -75.46\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS \n",
    "\n",
    "prefix = 'LSR'\n",
    "hex_digits = '34'\n",
    "\n",
    "city_id = 50\n",
    "vvid = 425\n",
    "\n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-11',\n",
    "    '2022-11-12',\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "]\n",
    "\n",
    "MAX_LAT = 42.89\n",
    "MIN_LAT = 42.01\n",
    "MAX_LNG = -82.68\n",
    "MIN_LNG = -83.93\n",
    "\n",
    "DISCOUNT = 0.995\n",
    "VALUE_DEGRADE_LEVEL = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "QUERY = \"\"\"\n",
    "with dispatch as (\n",
    "select \n",
    "    datestr,\n",
    "    msg.cityid,\n",
    "    msg.ctplangenrequestuuid as plangen_uuid,\n",
    "    msg.ctrequestuuid as scan_uuid,\n",
    "    j as job_uuid,\n",
    "    msg.supplyuuid,\n",
    "    msg.planactiontype\n",
    "from \n",
    "    rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
    "cross join \n",
    "    unnest(msg.jobuuid) jobs(j)\n",
    "where \n",
    "    datestr = '{datestr}'\n",
    "    and msg.cityid = {city_id}\n",
    "    and msg.vehicleviewid = {vvid} \n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and CARDINALITY(msg.jobuuid) > 0\n",
    "    and substr(msg.ctrequestuuid, 1, length('{digits}')) = '{digits}'\n",
    "),\n",
    "plangen as (\n",
    "select \n",
    "    msg.scanuuid as plangen_uuid, \n",
    "    p.uuid as job_uuid,\n",
    "    j.supplyuuid\n",
    "from \n",
    "    rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
    "cross join \n",
    "    unnest(msg.proposals) as job(j)\n",
    "cross join \n",
    "    unnest(j.jobs) as plan(p)\n",
    "where \n",
    "    datestr = '{datestr}'\n",
    "    and msg.cityid = {city_id}\n",
    "    and msg.flowtype = 'solo_batch'\n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and j.status = 'eligible'\n",
    "),\n",
    "mgv as (\n",
    "select datestr,\n",
    "    msg.city_id,\n",
    "    msg.job_uuid,\n",
    "    msg.client_uuid,\n",
    "    msg.ct_request_uuid as plangen_uuid,\n",
    "    msg.supply_uuid,\n",
    "    msg.supply_plan_uuid as plan_uuid,\n",
    "    msg.unadjusted_eta as eta,\n",
    "    (CASE\n",
    "      WHEN msg.adjustedeta > 1500 THEN 1500.0\n",
    "      WHEN msg.adjustedeta < 0 THEN 0.0\n",
    "      ELSE msg.adjustedeta\n",
    "    END) as adjustedeta,\n",
    "    round(msg.job_surge, 4) as surge_mul,\n",
    "    round(msg.eventual_completion_probability, 4) as eventual_comp_prob,\n",
    "    msg.ranking_metric,\n",
    "    round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
    "    round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
    "    round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
    "    msg.preferred_destination_adjustment,\n",
    "    msg.objective_value as of_value,\n",
    "    msg.inconvenience_etd - msg.ranking_metric as trip_length,\n",
    "    msg.dropoff_latitude as dropoff_latitude,\n",
    "    msg.dropoff_longitude as dropoff_longitude,\n",
    "    msg.pickup_latitude as pickup_latitude,\n",
    "    msg.pickup_longitude as pickup_longitude \n",
    "from   \n",
    "    rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
    "where  \n",
    "    datestr = '{datestr}'\n",
    "    and msg.city_id = {city_id}\n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and msg.vehicle_view_id = {vvid} \n",
    "    and msg.flow_type = 'solo_batch'\n",
    "    and msg.job_uuid <> msg.client_uuid\n",
    "    and msg.calculator_type = 'markov_eta_v2'\n",
    "),\n",
    "geoloc as (\n",
    "    SELECT\n",
    "    datestr as datestr,\n",
    "    msg.cityid as city_id,\n",
    "    msg.etasec as eta,\n",
    "    msg.flowtype as flow_type,\n",
    "    msg.timestampms as ts,\n",
    "    msg.supplyuuid as supply_uuid,\n",
    "    originalsupplywaypoints.latitude as driver_origin_lat,\n",
    "    originalsupplywaypoints.longitude as driver_origin_lng,\n",
    "    jobs.uuid as job_uuid,\n",
    "    jobs.timetodropoffinplaninsecs as timetodropoffinplaninsecs\n",
    "    FROM\n",
    "    rawdata_user.kafka_hp_plangenerator_matching_plans_log_nodedup\n",
    "    CROSS JOIN\n",
    "    UNNEST(msg.originalsupplywaypoints) as t(originalsupplywaypoints)\n",
    "    CROSS JOIN\n",
    "    UNNEST(msg.jobs) as j(jobs)\n",
    "    WHERE\n",
    "    datestr='{datestr}'\n",
    "    and msg.cityid={city_id}\n",
    "),\n",
    "test as (\n",
    "select\n",
    "    mgv.datestr,\n",
    "    mgv.city_id,\n",
    "    dispatch.scan_uuid,\n",
    "    mgv.plangen_uuid,\n",
    "    mgv.job_uuid,\n",
    "    dispatch.planactiontype,\n",
    "    mgv.supply_uuid,\n",
    "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
    "    mgv.eta,\n",
    "    mgv.adjustedeta,\n",
    "    geoloc.timetodropoffinplaninsecs,\n",
    "    geoloc.driver_origin_lat,\n",
    "    geoloc.driver_origin_lng,\n",
    "    mgv.pickup_latitude,\n",
    "    mgv.pickup_longitude,\n",
    "    mgv.dropoff_latitude,\n",
    "    mgv.dropoff_longitude,\n",
    "    great_circle_distance(\n",
    "        mgv.pickup_latitude, mgv.pickup_longitude,\n",
    "        mgv.dropoff_latitude, mgv.dropoff_longitude\n",
    "      ) AS haversine_dist_km,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1), 4) as eta_one,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.05), 4) as eta_one_five,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.10), 4) as eta_one_ten,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.15), 4) as eta_one_fifteen,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.20), 4) as eta_one_twenty,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.25), 4) as eta_one_quarter,\n",
    "    mgv.surge_mul,\n",
    "    mgv.eventual_comp_prob,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 2)), 4) as network_contention_2,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 3)), 4) as network_contention_3,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 5)), 4) as network_contention_5,\n",
    "    mgv.ranking_metric,\n",
    "    mgv.d_proba,\n",
    "    mgv.r_proba,\n",
    "    mgv.s_proba,\n",
    "    round((1.0 - mgv.d_proba) * (1.0 - mgv.r_proba) * (1.0 - mgv.s_proba) + mgv.eventual_comp_prob * mgv.d_proba, 4) as cr_ratio,\n",
    "    round((1.0 - mgv.d_proba) * (1.0 - mgv.r_proba) + mgv.eventual_comp_prob * mgv.d_proba, 4) as crof_ratio,\n",
    "    mgv.preferred_destination_adjustment,\n",
    "    mgv.of_value,\n",
    "    mgv.trip_length,\n",
    "    fare.est_rider_quoted_final_fare as fare,\n",
    "    fare.est_rider_quoted_final_fare * 1.0 / fare.usd_fx_rate as fare_usd,\n",
    "    fare.request_timestamp_local as local_time\n",
    "from\n",
    "    mgv\n",
    "join\n",
    "    plangen\n",
    "on \n",
    "    mgv.plangen_uuid = plangen.plangen_uuid\n",
    "    and mgv.job_uuid = plangen.job_uuid\n",
    "    and mgv.supply_uuid = plangen.supplyuuid\n",
    "join\n",
    "    dispatch\n",
    "on\n",
    "    mgv.plangen_uuid = dispatch.plangen_uuid\n",
    "    and mgv.job_uuid = dispatch.job_uuid\n",
    "join\n",
    "    dwh.fact_trip_fare fare \n",
    "on\n",
    "    mgv.job_uuid = fare.trip_uuid\n",
    "    and fare.datestr = '{datestr}'\n",
    "    and fare.city_id = {city_id}\n",
    "join\n",
    "    geoloc\n",
    "on\n",
    "    mgv.job_uuid = geoloc.job_uuid    \n",
    "    and mgv.supply_uuid = geoloc.supply_uuid\n",
    ")\n",
    "select * from test\n",
    "LIMIT 2000000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    hex_digits: str\n",
    "    city_id: int\n",
    "    vvid: str\n",
    "    datestr: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.vvid}_{self.datestr}_segment{self.hex_digits}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, vvid=self.vvid, digits=self.hex_digits, datestr=self.datestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_origin_SV(row, term = 'mean'):\n",
    "    row['local_time'] = pd.to_datetime(row['local_time'])\n",
    "    weekday = row['local_time'].weekday()\n",
    "    time = row['local_time'].hour * 3600 + \\\n",
    "           row['local_time'].minute * 60 + \\\n",
    "           row['local_time'].second\n",
    "    if time > 24 * 3600:\n",
    "        time = time % (24 * 3600)    \n",
    "        weekday = (weekday + 1) % 7\n",
    "    lat = row.driver_origin_lat\n",
    "    lng = row.driver_origin_lng\n",
    "    inp = torch.tensor([weekday, time, lat, lng], dtype=torch.float).reshape(1,-1)\n",
    "    LTSV.eval()\n",
    "    if term == 'mean':\n",
    "        out,_ = LTSV(inp)\n",
    "    elif term == 'variance':\n",
    "        _, out = LTSV(inp)\n",
    "    else:\n",
    "        raise Exception(\"DNN output term invalid\")\n",
    "    return out.item()\n",
    "\n",
    "def add_pickup_SV(row, term = 'mean'):\n",
    "    row['local_time'] = pd.to_datetime(row['local_time'])\n",
    "    weekday = row['local_time'].weekday()\n",
    "    time = row['local_time'].hour * 3600 + \\\n",
    "           row['local_time'].minute * 60 + \\\n",
    "           row['local_time'].second + row['adjustedeta']\n",
    "    if time > 24 * 3600:\n",
    "        time = time % (24 * 3600)    \n",
    "        weekday = (weekday + 1) % 7\n",
    "    lat = row.pickup_latitude\n",
    "    lng = row.pickup_longitude\n",
    "    inp = torch.tensor([weekday, time, lat, lng], dtype=torch.float).reshape(1,-1)\n",
    "    LTSV.eval()\n",
    "    if term == 'mean':\n",
    "        out,_ = LTSV(inp)\n",
    "    elif term == 'variance':\n",
    "        _, out = LTSV(inp)\n",
    "    else:\n",
    "        raise Exception(\"DNN output term invalid\")\n",
    "    return out.item()\n",
    "\n",
    "def add_destination_SV(row, term = 'mean'):\n",
    "    row['local_time'] = pd.to_datetime(row['local_time'])\n",
    "    weekday = row['local_time'].weekday()\n",
    "    time = row['local_time'].hour * 3600 + \\\n",
    "           row['local_time'].minute * 60 + \\\n",
    "           row['local_time'].second + row['timetodropoffinplaninsecs']\n",
    "    if time > 24 * 3600:\n",
    "        time = time % (24 * 3600)\n",
    "        weekday = (weekday + 1) % 7\n",
    "    lat = row.dropoff_latitude\n",
    "    lng = row.dropoff_longitude\n",
    "    inp = torch.tensor([weekday, time, lat, lng], dtype=torch.float).reshape(1,-1)\n",
    "    LTSV.eval()\n",
    "    if term == 'mean':\n",
    "        out,_ = LTSV(inp)\n",
    "    elif term == 'variance':\n",
    "        _, out = LTSV(inp)\n",
    "    else:\n",
    "        raise Exception(\"DNN output term invalid\")\n",
    "    return out.item()\n",
    "\n",
    "\n",
    "# def batch_origin_SV(df):\n",
    "#     df['local_time'] = pd.to_datetime(df['local_time'])\n",
    "#     time = df['local_time'].hour * 3600 + \\\n",
    "#            df['local_time'].minute * 60 + \\\n",
    "#            df['local_time'].second + df['timetodropoffinplaninsecs']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate new objective function\n",
    "def clean_df(df):\n",
    "    df = df[df['fare'].notnull()]\n",
    "    df['trip_length'][df['trip_length'] <= 100] = 100\n",
    "    df = df.drop_duplicates(subset=['job_uuid', 'supply_uuid'])\n",
    "    df = df.dropna()\n",
    "    df['origin_SV_mean'] = df.apply(lambda row : add_origin_SV(row, term = 'mean'), axis = 1)\n",
    "    df['origin_SV_var'] = df.apply(lambda row : add_origin_SV(row, term = 'variance'), axis = 1)\n",
    "    df['pickup_SV_mean'] = df.apply(lambda row : add_pickup_SV(row, term = 'mean'), axis = 1)\n",
    "    df['pickup_SV_var'] = df.apply(lambda row : add_pickup_SV(row, term = 'variance'), axis = 1)\n",
    "    df['dest_SV_mean'] = df.apply(lambda row : add_destination_SV(row, term = 'mean'), axis = 1)\n",
    "    df['dest_SV_var'] = df.apply(lambda row : add_destination_SV(row, term = 'variance'), axis = 1)\n",
    "    return df\n",
    "\n",
    "# local solver\n",
    "def solve_dict(\n",
    "    scan: dict, \n",
    "    cost_col: str, \n",
    "    job_singleton: float = 1500,\n",
    "    infinity: float = 1000000\n",
    "):\n",
    "    job_list = list(set([k[0] for k in scan.keys()]))\n",
    "    job_idx = {j: i for i, j in enumerate(job_list)}\n",
    "    job_count = len(job_list)\n",
    "\n",
    "    supply_list = list(set([k[1] for k in scan.keys()]))\n",
    "    supply_idx = {s: i for i, s in enumerate(supply_list)}\n",
    "    supply_count = len(supply_list)\n",
    "    \n",
    "    utility = np.full((len(job_list), len(supply_list) + len(job_list)), infinity, dtype=np.float32)\n",
    "    for k in scan.keys():\n",
    "        jidx = job_idx[k[0]]\n",
    "        sidx = supply_idx[k[1]]\n",
    "        utility[jidx, sidx] = scan[k][cost_col]\n",
    "    for i in range(len(job_list)):\n",
    "        utility[i, supply_count + i] = job_singleton\n",
    "            \n",
    "    # solve\n",
    "    job_sol, supply_sol = linear_sum_assignment(utility)\n",
    "\n",
    "    result = set()\n",
    "    for jidx, sidx in zip(job_sol, supply_sol):\n",
    "        j = job_list[jidx]\n",
    "        if sidx >= supply_count:\n",
    "            result.add((j,))\n",
    "        else:\n",
    "            s = supply_list[sidx]\n",
    "            result.add((j, s))\n",
    "            \n",
    "    assert len(result) == len(job_list)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import field\n",
    "\n",
    "@dataclass\n",
    "class ScanMetrics:\n",
    "    total_jobs: int = 0.\n",
    "    total_eta: float = 0.\n",
    "    total_offer: float = 0.\n",
    "    total_ar: float = 0.\n",
    "    total_rc: float = 0.\n",
    "    total_trip: float = 0.\n",
    "    total_gb: float = 0.\n",
    "    total_fare: float = 0.\n",
    "    total_overwrite: int = 0.\n",
    "    total_jobsurge: int = 0.\n",
    "    list_etas: list = field(default_factory = list)\n",
    "    \n",
    "    def __add__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        return ScanMetrics(\n",
    "            self.total_jobs + o.total_jobs,\n",
    "            self.total_eta + o.total_eta,\n",
    "            self.total_offer + o.total_offer,\n",
    "            self.total_ar + o.total_ar,\n",
    "            self.total_rc + o.total_rc,\n",
    "            self.total_trip + o.total_trip,\n",
    "            self.total_overwrite + o.total_overwrite,\n",
    "            self.total_gb + o.total_gb,\n",
    "            self.total_fare + o.total_fare,\n",
    "            self.total_jobsurge + o.total_jobsurge,\n",
    "            self.list_etas.expand + o.list_etas\n",
    "        )\n",
    "    def __iadd__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        self.total_jobs += o.total_jobs\n",
    "        self.total_eta += o.total_eta\n",
    "        self.total_offer += o.total_offer\n",
    "        self.total_ar += o.total_ar\n",
    "        self.total_rc += o.total_rc\n",
    "        self.total_trip += o.total_trip\n",
    "        self.total_overwrite += o.total_overwrite\n",
    "        self.total_gb += o.total_gb\n",
    "        self.total_fare += o.total_fare\n",
    "        self.total_jobsurge += o.total_jobsurge\n",
    "        self.list_etas += o.list_etas\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Summary\n",
    "def metric_summary_dict(\n",
    "    scan_dict: Dict[str, Dict[str, Any]],\n",
    "    matching: set, \n",
    "    overwrite: int,\n",
    ") -> ScanMetrics:\n",
    "    sm = ScanMetrics()\n",
    "    sm.total_jobs = len(matching)\n",
    "    sm.total_overwrite = overwrite\n",
    "    \n",
    "    for m in matching:\n",
    "        if len(m) == 2:\n",
    "            row = scan_dict[(m[0], m[1])]\n",
    "            sm.total_offer += 1\n",
    "            sm.total_eta += row['eta']\n",
    "            sm.total_ar += 1 - row['d_proba']\n",
    "            sm.total_rc += row['r_proba']\n",
    "            sm.total_jobsurge += row['surge_mul']\n",
    "            if row['trip_length'] < 7200:\n",
    "                sm.total_trip += row['trip_length']\n",
    "            if row['fare_usd'] > 0:\n",
    "                sm.total_gb += (1 - row['d_proba']) * (1 - row['r_proba']) * row['fare_usd']\n",
    "                sm.total_fare += row['fare_usd']\n",
    "                \n",
    "            sm.list_etas.append(row['eta'])\n",
    "\n",
    "    return sm\n",
    "\n",
    "def solve_all_dict(df, solver: Callable[[dict], set]):\n",
    "    total_scans = dict(tuple(df.groupby('scan_uuid')))\n",
    "\n",
    "    sm = ScanMetrics()\n",
    "    overwrite_summary = {}\n",
    "    for scan_uuid, scan_df in total_scans.items():\n",
    "        scan = (scan_df.set_index(['job_uuid', 'supply_uuid']).to_dict(orient='index'))\n",
    "        matching, overwrite, out = solver(scan)\n",
    "        overwrite_summary[scan_uuid] = out\n",
    "        sm += metric_summary_dict(scan, matching, overwrite)\n",
    "        \n",
    "    return {'total_jobs': round(sm.total_jobs),\n",
    "            'match_rate': round(sm.total_offer * 1.0 / sm.total_jobs, 3),\n",
    "            'overwrite': round(sm.total_overwrite * 1.0 / sm.total_jobs, 3), # different decisions compared to Markov\n",
    "            'Average Matched ETA': round(sm.total_eta * 1.0 / sm.total_offer, 2),\n",
    "            'P90 Matched ETA': round(np.percentile(sm.list_etas, 90), 2),\n",
    "            'Driver AR': round(sm.total_ar * 1.0 / sm.total_offer, 3),\n",
    "            'Rider cancel': round(sm.total_rc * 1.0 / sm.total_offer, 3),\n",
    "            'Average trip length': round(sm.total_trip * 1.0 / sm.total_offer, 2),\n",
    "#             'Average Matched Fare': round(sm.total_fare * 1.0 / sm.total_offer, 2),\n",
    "            'Average Matched Fare': round(sm.total_gb * 1.0 / sm.total_offer, 2),\n",
    "            'Average Job Surge': round(sm.total_jobsurge * 1.0 / sm.total_offer, 2),\n",
    "            'Total GB': round(sm.total_gb),\n",
    "            'overwrite_summary': overwrite_summary,\n",
    "           }\n",
    "\n",
    "def different_matching_decision(m1,m2):\n",
    "    return m1.difference(m2), m2.difference(m1)\n",
    "\n",
    "def supply_cost_solve_dict(scan, is_markov = False, secondary_singleton = 0.0):\n",
    "    # Markov\n",
    "#     primary_matching = solve_dict(scan, 'of_value', job_singleton = 1500)\n",
    "    primary_matching = solve_dict(scan, 'of_value_EFOF', job_singleton = 0)\n",
    "    if is_markov:      \n",
    "        return primary_matching, 0\n",
    "    \n",
    "    # SCA solve\n",
    "    secondary_matching = solve_dict(scan, 'new_of', job_singleton = secondary_singleton)\n",
    "    \n",
    "    primary_dict = {}\n",
    "    secondary_dict = {}\n",
    "    # k[0] job and k[1] supply\n",
    "    for k in primary_matching:\n",
    "        try:\n",
    "            primary_dict[k[1]] = k[0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for k in secondary_matching:\n",
    "        try:\n",
    "            secondary_dict[k[1]] = k[0]\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "            \n",
    "    all_supplies = set(primary_dict.keys()).intersection(set(secondary_dict.keys()))\n",
    "    all_jobs = set(primary_dict.values()).intersection(set(secondary_dict.values()))\n",
    "    \n",
    "    rev_primary_dict = {v:k for k,v in primary_dict.items()}\n",
    "    rev_secondary_dict = {v:k for k,v in secondary_dict.items()}\n",
    "    \n",
    "#     show same supplies with different jobs (check dropoff values)\n",
    "    out = []\n",
    "    for k in all_supplies:\n",
    "        if primary_dict[k] != secondary_dict[k]:\n",
    "            out.append([k, primary_dict[k], secondary_dict[k]])\n",
    "\n",
    "# #     show same jobs with different supplies (check origin values)\n",
    "#     out = []\n",
    "#     for k in all_jobs:\n",
    "#         if rev_primary_dict[k] != rev_secondary_dict[k]:\n",
    "#             out.append([k, rev_primary_dict[k], rev_secondary_dict[k]])\n",
    "                \n",
    "    different_matches = len(different_matching_decision(primary_matching, secondary_matching)[0])\n",
    "    return secondary_matching, different_matches, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "queries = [\n",
    "    Query(prefix=prefix, hex_digits=hex_digits, city_id=city_id, vvid=vvid, datestr=datestr)\n",
    "    for datestr in datestrs\n",
    "]\n",
    "\n",
    "cache_qry_map = {\n",
    "    q.name: q.qry \n",
    "    for q in queries\n",
    "}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='phx2',\n",
    "    datasource='presto-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTSV = Net(MAX_LAT, MIN_LAT, MAX_LNG, MIN_LNG)\n",
    "PATH = f'checkpoints/LTSV_city{city_id}_vvid{vvid}_gamma{DISCOUNT}_discount{VALUE_DEGRADE_LEVEL}'\n",
    "LTSV.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10156489\n",
    "## 144925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# clean data\n",
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True) \n",
    "print(len(scans))\n",
    "df = scans\n",
    "df = clean_df(df)\n",
    "print(len(df))\n",
    "del scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MarkovETA\n",
    "\n",
    "{'total_jobs': 4613,\n",
    " 'match_rate': 0.893,\n",
    " 'overwrite': 0.0,\n",
    " 'Average Matched ETA': 405.74,\n",
    " 'P90 Matched ETA': 956.2,\n",
    " 'Driver AR': 0.341,\n",
    " 'Rider cancel': 0.08,\n",
    " 'Average trip length': 911.02,\n",
    " 'Average Matched Fare': 25.82,\n",
    " 'Total GB': 28180}\n",
    " \n",
    "EFOF\n",
    "{'total_jobs': 4613,\n",
    " 'match_rate': 0.909, \n",
    " 'overwrite': 0.341, \n",
    " 'Average Matched ETA': 429.62, \n",
    " 'P90 Matched ETA': 1036.6, \n",
    " 'Driver AR': 0.331, \n",
    " 'Rider cancel': 0.091, \n",
    " 'Average trip length': 980.51, \n",
    " 'Average Matched Fare': 27.83, \n",
    " 'Total GB': 30002}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_square = np.power(1 - df['adjustedeta'] / 1500.0, 2)\n",
    "tripValue = df['fare']\n",
    "OF = - eta_square * df['cr_ratio'] * tripValue\n",
    "df['of_value_EFOF'] = OF\n",
    "min_val = min(df['of_value_EFOF'])\n",
    "max_val = max(df['of_value_EFOF'])\n",
    "df['of_value_EFOF'] = -1 * (1 - (df['of_value_EFOF'] - min_val) / (max_val - min_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dest_SV_var'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_of(df, coef):\n",
    "    eta_square = np.power(1 - df['adjustedeta'] / 1500.0, 2)\n",
    "    ## approach 1\n",
    "    diffSV = df['dest_SV_mean'] / np.sqrt(df['dest_SV_var']) - df['origin_SV_mean'] / np.sqrt(df['origin_SV_var'])\n",
    "    diffSV = diffSV.clip(-10,10)\n",
    "    ## approach 2\n",
    "#     diffSV = df['dest_SV_mean'] - df['origin_SV_mean']\n",
    "#     diffSV[np.sqrt(df['origin_SV_var']) > 4] = 0\n",
    "    tripValue = df['fare'] + coef * diffSV\n",
    "    OF = - eta_square * df['cr_ratio'] * tripValue\n",
    "    df['new_of'] = OF\n",
    "    min_val = min(df['new_of'])\n",
    "    max_val = max(df['new_of'])\n",
    "    df['new_of'] = -1 * (1 - (df['new_of'] - min_val) / (max_val - min_val))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_dict = {'coef': [], 'match_rate': [], 'Average Matched ETA': [], 'Average Matched Fare': [], 'Total GB': [],\n",
    "           'overwrite': [], 'Driver AR': [], 'Rider cancel': [], 'Average trip length': [], 'Average Job Surge': []}\n",
    "\n",
    "for coef in np.arange(0,8.0,0.1):\n",
    "# for coef in [0, 12.5]:\n",
    "    df = compute_new_of(df, coef)\n",
    "    out = solve_all_dict(df, lambda scan: supply_cost_solve_dict(scan, is_markov = False))\n",
    "    res_dict['coef'].append(coef)\n",
    "    res_dict['match_rate'].append(out['match_rate'])\n",
    "    res_dict['Average Matched ETA'].append(out['Average Matched ETA'])\n",
    "    res_dict['Average Matched Fare'].append(out['Average Matched Fare'])\n",
    "    res_dict['Average trip length'].append(out['Average trip length'])\n",
    "    res_dict['Average Job Surge'].append(out['Average Job Surge'])\n",
    "    res_dict['Total GB'].append(out['Total GB'])\n",
    "    res_dict['overwrite'].append(out['overwrite'])\n",
    "    res_dict['Driver AR'].append(out['Driver AR'])\n",
    "    res_dict['Rider cancel'].append(out['Rider cancel'])\n",
    "    print(f'with coef {coef}:')\n",
    "    print(out['match_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # better with EFOF\n",
    "\n",
    "df.iloc[6399]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better with FLOF\n",
    "\n",
    "df.iloc[526]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500,600):\n",
    "    ttt = df.iloc[i]\n",
    "    if len(df[\n",
    "        (df.supply_uuid == ttt.supply_uuid) & \\\n",
    "        (df.scan_uuid == ttt.scan_uuid)][['of_value_EFOF','new_of']].sort_values('new_of').head(15)) > 10:\n",
    "        break\n",
    "\n",
    "df[\n",
    "        (df.supply_uuid == ttt.supply_uuid) & \\\n",
    "        (df.scan_uuid == ttt.scan_uuid)][['of_value_EFOF','new_of']].sort_values('new_of').head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df[df.job_uuid == '9a031c82-6507-4cf6-b0eb-4bc74b42f3f3'][['of_value_EFOF','new_of']].sort_values('new_of').head(15)\n",
    "\n",
    "df[\n",
    "    (df.supply_uuid == '258ee6f9-d13f-4725-ae1d-f347c9453baa') & \\\n",
    "    (df.scan_uuid == '342b06d2-a7b0-4f3d-a687-0569b0d669af')][['of_value_EFOF','new_of']].sort_values('new_of').head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in out['overwrite_summary']:\n",
    "#     print(k, len(out['overwrite_summary'][k]))\n",
    "\n",
    "# t = [item for k in out['overwrite_summary'] for item in out['overwrite_summary'][k]]\n",
    "# t = out['overwrite_summary']['36ccf21a-38d3-40d9-a165-e3cec6276542']\n",
    "\n",
    "# SUPS = [i[0] for i in t]\n",
    "# JOBS1 = [i[1] for i in t]\n",
    "# JOBS2 = [i[2] for i in t]\n",
    "\n",
    "mean_vals = []\n",
    "\n",
    "for k in out['overwrite_summary']:\n",
    "    if out['overwrite_summary'][k] != []:\n",
    "        JOBS1 = [i[1] for i in out['overwrite_summary'][k]]\n",
    "        JOBS2 = [i[2] for i in out['overwrite_summary'][k]]\n",
    "        p1 = df[(df.scan_uuid == k) & (df.job_uuid.isin(JOBS1))].dest_SV_mean.mean()\n",
    "        p2 = df[(df.scan_uuid == k) & (df.job_uuid.isin(JOBS2))].dest_SV_mean.mean()\n",
    "        if len(JOBS1) > 10:\n",
    "            mean_vals.append([p1, p2])\n",
    "            \n",
    "# for k in out['overwrite_summary']:\n",
    "#     if out['overwrite_summary'][k] != []:\n",
    "#         JOBS = [i[0] for i in out['overwrite_summary'][k]]\n",
    "#         SUPS1 = [i[1] for i in out['overwrite_summary'][k]]\n",
    "#         SUPS2 = [i[2] for i in out['overwrite_summary'][k]]\n",
    "#         p1 = df[(df.scan_uuid == k) &(df.job_uuid.isin(JOBS)) & (df.supply_uuid.isin(SUPS1))].origin_SV.mean()\n",
    "#         p2 = df[(df.scan_uuid == k) &(df.job_uuid.isin(JOBS)) & (df.supply_uuid.isin(SUPS2))].origin_SV.mean()\n",
    "#         if len(SUPS1) > 20:\n",
    "#             mean_vals.append([p1, p2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['match_rate'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('match rate')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['Average Matched ETA'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('Average Matched ETA')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['Average Matched Fare'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('Average Matched Fare')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['Total GB'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('Total GB')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['overwrite'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('overwrite')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['Driver AR'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('Driver AR')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(res_dict['coef'], res_dict['Rider cancel'])\n",
    "plt.xlabel('LTSV coef.')\n",
    "plt.ylabel('Rider cancel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 (default, Oct 31 2022, 14:04:00) \n[GCC 8.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
