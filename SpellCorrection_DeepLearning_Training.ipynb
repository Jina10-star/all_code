{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Training deep learning model for Spell Correction:\n",
    "\n",
    "- Training data stored in phx2 HDFS, Parquet format\n",
    "- Run with Kernel Python 3(General DS)\n",
    "- Train with default docker image deeplearning_examples\n",
    "- Training cluster: phx4-prod2\n",
    "- Resource pool: root/ProductPlatform/Michelangelo\n",
    "- Trained model will be saved in 'containers' of Instance 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import michelangelo.malib.lambdadl as ldl\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, CuDNNLSTM, Dense, Dropout, TimeDistributed, Masking, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please refer to https://docs.google.com/spreadsheets/d/1-XldpTa11qHMYJDR7ZTh27Vmo9HcUdqDXAqgMp4N6uU/edit#gid=2084303453 for the performance with various setting combinations\n",
    "# Bi-gram with 256 LSTM units show good performance\n",
    "\n",
    "NGRAM = 2\n",
    "\n",
    "USE_GPS = False\n",
    "\n",
    "if USE_GPS:\n",
    "   BASE_CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789 -.,\"    \n",
    "   DATA_COLS = [\"s2cell_id\", \"raw_query\", \"corrected_query\"] \n",
    "   QUERY_ENCODED_LEN = 52\n",
    "   LABEL_ENCODED_LEN = 54\n",
    "else:\n",
    "   BASE_CHARS = \"abcdefghijklmnopqrstuvwxyz0123456789 -\"\n",
    "   DATA_COLS = [\"raw_query\", \"corrected_query\"]\n",
    "   QUERY_ENCODED_LEN = 30\n",
    "   LABEL_ENCODED_LEN = 32\n",
    "\n",
    "# Network hidden units\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH_CNT = 200\n",
    "\n",
    "LEARNING_RATE = 1.0\n",
    "LEARNING_RATE_STR = 'lr1_0'\n",
    "\n",
    "PETASTORM_HDFS_DRIVER = 'libhdfs'\n",
    "ZONE = 'phx4-prod02'\n",
    "\n",
    "WORKER_CNT = 40\n",
    "MEMORY_SIZE_MB = 61440\n",
    "TIME_OUT_S = 20000\n",
    "\n",
    "# To get the delegation token, in terminal of development machine\n",
    "# $ drogon token --renew -user <username> --cluster phx4\n",
    "# $ drogon token-view --user <username> --cluster phx4 \n",
    "HDFS_DELEGATION_TOKEN = <hdfs_delegation_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/mnt/mesos/sandbox/'\n",
    "model_file_prefix = 'sc_lstm'\n",
    "\n",
    "if USE_GPS:\n",
    "    use_gps_str = '_gps'\n",
    "else:\n",
    "    use_gps_str = \"_nogps\"\n",
    "\n",
    "file_parts = \"_ngram\" + str(NGRAM) + \"_\" + LEARNING_RATE_STR + \"_batch\" + str(BATCH_SIZE) + \"_lstmunit\" + str(LSTM_UNITS) + use_gps_str\n",
    "\n",
    "# Result files\n",
    "model_file = model_path + model_file_prefix + file_parts + \".h5\"\n",
    "model_weights_file = model_path + model_file_prefix + \"weights_\" + file_parts + \".h5\"\n",
    "model_config_json = model_path + model_file_prefix + \"config_\" + file_parts + \".json\"\n",
    "model_checkpoint_file = model_path + model_file_prefix + \"{epoch:02d}_{acc:.4f}_{val_acc:.4f}_\" + file_parts + \".h5\"\n",
    "\n",
    "training_history_pickle = model_path + model_file_prefix + \"training_history_\" + file_parts + \".pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Validation datasets\n",
    "training_data202010 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-10_training4M/part-00000-de71c9bc-1905-49e6-8c48-89c1c40890db-c000.snappy.parquet'\n",
    "training_data202011 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-11_training4M/part-00000-ab29cacb-79a6-4f5e-b5fe-76a6f588555b-c000.snappy.parquet'\n",
    "training_data202012 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-12_training4M/part-00000-5c86a121-63e2-48c9-85c4-1eaec3250a19-c000.snappy.parquet'\n",
    "training_data202101 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-01_training4M/part-00000-bedd01f2-6061-4ee8-90e8-bb36ff8c26a7-c000.snappy.parquet'\n",
    "training_data202102 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-02_training4M/part-00000-6f651532-b1fe-426a-b9b8-9ba046d9c2d6-c000.snappy.parquet'\n",
    "training_data20210301 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-01_training4M/part-00000-e0141b8c-3ce7-4ccb-903e-294db6bb7431-c000.snappy.parquet'\n",
    "training_data20210302 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-02_training4M/part-00000-83f7938f-4247-4fc0-8f28-e7740bc1f867-c000.snappy.parquet'\n",
    "training_data20210303 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-03_training4M/part-00000-344ab067-ceb5-4f14-becc-ee6939dbaccf-c000.snappy.parquet'\n",
    "training_data20210304 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-04_training4M/part-00000-eb4d0113-67ee-481a-b9a9-3014fc7b1249-c000.snappy.parquet'\n",
    "training_data20210305 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-05_training4M/part-00000-056c0013-3a2c-472a-8241-bb05834fb70e-c000.snappy.parquet'\n",
    "\n",
    "val_data202010 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-10_val4M/part-00000-caede584-91ef-416c-8081-f0c01dad0fa4-c000.snappy.parquet'\n",
    "val_data202011 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-11_val4M/part-00000-bcc46407-4df3-41b3-a3cb-bd2ff976c0e9-c000.snappy.parquet'\n",
    "val_data202012 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2020-12_val4M/part-00000-c19c9487-8158-492c-b217-ac28c31447d0-c000.snappy.parquet'\n",
    "val_data202101 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-01_val4M/part-00000-f7b6dd0d-2620-42c7-a79c-8a19a2165115-c000.snappy.parquet'\n",
    "val_data202102 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-02_val4M/part-00000-0028f347-42bb-458e-98b1-0978ab283c16-c000.snappy.parquet'\n",
    "val_data20210301 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-01_val4M/part-00000-f83bb908-1d30-4e6a-8027-21ee275b8ac9-c000.snappy.parquet'\n",
    "val_data20210302 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-02_val4M/part-00000-77dae0c1-004e-4422-8c4d-e1f6221c0448-c000.snappy.parquet'\n",
    "val_data20210303 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-03_val4M/part-00000-c8218082-88a5-4df1-9e9e-9eafde61519e-c000.snappy.parquet'\n",
    "val_data20210304 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-04_val4M/part-00000-3cd137db-493b-4ba4-8c2b-b398f132e16e-c000.snappy.parquet'\n",
    "val_data20210305 = '/app/geosearch/spell_check/misspelling_corrected_s2cell_lvl6_parquet/US_2021-03-05_val4M/part-00000-68f5aa31-39b3-4b38-95f6-465b41c3b58c-c000.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(gramLen):\n",
    "    encodeVoc = dict()\n",
    "    decodeVoc = dict()\n",
    "    charCnt = len(BASE_CHARS)\n",
    "\n",
    "    gramIndex = 0\n",
    "    if gramLen == 1:\n",
    "        for i in range(charCnt):\n",
    "            gram = BASE_CHARS[i]\n",
    "            gramIndex = gramIndex + 1\n",
    "            encodeVoc[gram] = gramIndex\n",
    "            decodeVoc[gramIndex] = gram\n",
    "    elif gramLen == 2:\n",
    "        for i in range(charCnt):\n",
    "            for j in range(charCnt):\n",
    "                gram = BASE_CHARS[i] + BASE_CHARS[j]\n",
    "                gramIndex = gramIndex + 1\n",
    "                encodeVoc[gram] = gramIndex\n",
    "                decodeVoc[gramIndex] = gram\n",
    "    elif gramLen == 3:\n",
    "        for i in range(charCnt):\n",
    "            for j in range(charCnt):\n",
    "                for k in range(charCnt):\n",
    "                    gram = BASE_CHARS[i] + BASE_CHARS[j] + BASE_CHARS[k]\n",
    "                    gramIndex = gramIndex + 1\n",
    "                    encodeVoc[gram] = gramIndex\n",
    "                    decodeVoc[gramIndex] = gram\n",
    "\n",
    "    return encodeVoc,decodeVoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeVoc,decodeVoc = buildVocabulary(NGRAM)\n",
    "vocabulary_size = len(BASE_CHARS)**NGRAM + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init(mactx):\n",
    "    mactx.execute('pip install petastorm==0.10.0')\n",
    "    \n",
    "system_config = ldl.SystemConfig(memory_size_mb=MEMORY_SIZE_MB, hdfs_delegation_token=HDFS_DELEGATION_TOKEN)\n",
    "job_config = ldl.DLJobConfig(zone=ZONE, timeout=TIME_OUT_S, num_workers=WORKER_CNT, num_gpus=1, system_config=system_config)\n",
    "\n",
    "mactx = ldl.DLJobContext(job_config, on_job_init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSetRows(data_paths):\n",
    "    from pyarrow.parquet import ParquetDataset \n",
    "    nrows = 0\n",
    "    nrow_groups = 0\n",
    "    dataset = ParquetDataset(data_paths)\n",
    "    for data_meta in dataset.pieces:\n",
    "       nrows += data_meta.get_metadata().num_rows\n",
    "       nrow_groups += data_meta.get_metadata().num_row_groups\n",
    "    return nrows, nrow_groups\n",
    "\n",
    "\n",
    "def get_training_configuration(batchSize):\n",
    "    import pydoop.hdfs as hdfs\n",
    "    from pyarrow.parquet import ParquetDataset    \n",
    "            \n",
    "    trainingDataSet = []\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data20210301))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data20210302))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data20210303))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data20210304))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data202010))    \n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data202011))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data202012))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data202101))\n",
    "    trainingDataSet.append(hdfs.path.abspath(training_data202102))\n",
    "    \n",
    "    trainingSampleCnt, trainingSampleRowGroupCnt = getDataSetRows(trainingDataSet)\n",
    "\n",
    "    valDataSet = []\n",
    "    valDataSet.append(hdfs.path.abspath(val_data20210301))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data20210302))    \n",
    "    valDataSet.append(hdfs.path.abspath(val_data20210303))    \n",
    "    valDataSet.append(hdfs.path.abspath(val_data20210304))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data202010))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data202011))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data202012))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data202101))\n",
    "    valDataSet.append(hdfs.path.abspath(val_data202102))\n",
    "\n",
    "    valSampleCnt, valSampleRowGroupCnt = getDataSetRows(valDataSet)\n",
    "    \n",
    "    return trainingDataSet, valDataSet, trainingSampleCnt, valSampleCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDataSet, valDataSet, trainingSampleCnt, valSampleCnt = \\\n",
    "    mactx.run_single(get_training_configuration, args=(BATCH_SIZE,))\n",
    "\n",
    "print(\"[Training config] trainingSampleCnt = {}\".format(trainingSampleCnt))\n",
    "print(\"[Training config] valSampleCnt = {}\".format(valSampleCnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def isValidTrainingSample(trainingSample):\n",
    "    query = trainingSample[2].lstrip()\n",
    "    if len(query) < 3:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "          \n",
    "\n",
    "def getGramIndex(voc, xgram):\n",
    "    if xgram in voc:\n",
    "        return voc[xgram]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def encodeString(strData, encodeVoc, gramLen, expectedLen):\n",
    "    encoded = []\n",
    "\n",
    "    for i in range(len(strData)-gramLen+1):\n",
    "        ngram = strData[i:i+gramLen]\n",
    "        idx = getGramIndex(encodeVoc, ngram)\n",
    "        encoded.append(idx)\n",
    "        if len(encoded) == expectedLen:\n",
    "            break\n",
    "\n",
    "    return np.asarray(encoded, dtype=np.uint16)\n",
    "\n",
    "\n",
    "def encodeSampleFromList(trainingSample, encodeVoc, gramLen, withGPS = False):\n",
    "    if withGPS:\n",
    "        input = trainingSample[0] + \",\" + trainingSample[1]\n",
    "        target = trainingSample[0] + \",\" + trainingSample[2]\n",
    "    else:\n",
    "        input = trainingSample[0]\n",
    "        target = trainingSample[1]\n",
    "\n",
    "    encodedInput = encodeString(input, encodeVoc, gramLen, QUERY_ENCODED_LEN)\n",
    "    encodedOutput = encodeString(target, encodeVoc, gramLen, LABEL_ENCODED_LEN)\n",
    "    return [encodedInput, encodedOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def createModel(vocabulary_size, hvd):\n",
    "    encoder_inputs = Input(shape=(None, vocabulary_size), name='encoder_inputs')\n",
    "    encoder_lstm = LSTM(LSTM_UNITS, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(None, vocabulary_size), name='decoder_inputs')\n",
    "    decoder_lstm = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "    decoder_dense = Dense(vocabulary_size, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='encoder_decoder_training')\n",
    "    \n",
    "    # Horovod: adjust learning rate based on number of GPUs.  1.0\n",
    "    opt = keras.optimizers.Adadelta(learning_rate=LEARNING_RATE * hvd.size())\n",
    "\n",
    "    # Horovod: add Horovod Distributed Optimizer.\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_generator(train_ds, sc, sess, batchSize, graph, trainingShuffleSize = None):\n",
    "    try:\n",
    "        with graph.as_default():\n",
    "            if trainingShuffleSize is None:\n",
    "                train_ds = train_ds \\\n",
    "                    .apply(tf.data.experimental.unbatch()) \\\n",
    "                    .batch(batchSize) \n",
    "            else:\n",
    "                train_ds = train_ds \\\n",
    "                    .apply(tf.data.experimental.unbatch()) \\\n",
    "                    .shuffle(trainingShuffleSize) \\\n",
    "                    .batch(batchSize) \n",
    "                \n",
    "            myiter = train_ds.make_one_shot_iterator()\n",
    "            \n",
    "            mynext = myiter.get_next()\n",
    "            \n",
    "            while True:\n",
    "                train_ds_batch = sess.run(mynext)\n",
    "                                \n",
    "                raw_query_batch =  train_ds_batch.raw_query                                \n",
    "                raw_query_batch = np.char.decode(raw_query_batch.astype(np.bytes_), 'utf-8')\n",
    "\n",
    "                corrected_query_batch =  train_ds_batch.corrected_query\n",
    "                corrected_query_batch = np.char.decode(corrected_query_batch.astype(np.bytes_), 'utf-8')\n",
    "                \n",
    "                if USE_GPS:\n",
    "                    s2cell_id_batch = train_ds_batch.s2cell_id\n",
    "                    s2cell_id_batch = np.char.decode(s2cell_id_batch.astype(np.bytes_), 'utf-8')\n",
    "                    \n",
    "                    trainingSet = np.hstack((s2cell_id_batch.reshape(len(s2cell_id_batch),1), raw_query_batch.reshape(len(raw_query_batch),1), corrected_query_batch.reshape(len(corrected_query_batch),1)))                    \n",
    "                else:\n",
    "                    trainingSet = np.hstack((raw_query_batch.reshape(len(raw_query_batch),1), corrected_query_batch.reshape(len(corrected_query_batch),1)))\n",
    "                    \n",
    "                trainingSetRDD = sc.parallelize(trainingSet)\n",
    "                #trainingSetRDD = trainingSetRDD.filter(isValidTrainingSample)\n",
    "                encodedTrainingSet = trainingSetRDD.map(lambda x: encodeSampleFromList(x, encodeVoc, NGRAM, USE_GPS))\n",
    "                encodedTrainingSet = encodedTrainingSet.collect()\n",
    "                encodedTrainingSet = np.array(encodedTrainingSet, dtype=object)\n",
    "\n",
    "                X = encodedTrainingSet[:,0]\n",
    "                Y = encodedTrainingSet[:,1]\n",
    "                # Pad the n-gram vector to expected length\n",
    "                # X shape:  #sample * QUERY_ENCODED_LEN\n",
    "                X = keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=QUERY_ENCODED_LEN)\n",
    "                Y = keras.preprocessing.sequence.pad_sequences(Y, padding='post', maxlen=LABEL_ENCODED_LEN-2)\n",
    "\n",
    "                # Shift Y to the right by one position (for the starting token) and add stop token in the end\n",
    "                # Y shape:  #sample * LABEL_ENCODED_LEN\n",
    "                Y = np.insert(Y, 0, 0, axis=1)\n",
    "                Y = np.hstack((Y, np.zeros((Y.shape[0], 1), dtype=np.uint16)))\n",
    "\n",
    "                # One-hot encode the ngram vectors\n",
    "                # encoder_input_data shape: #sample * QUERY_ENCODED_LEN * vocabulary_size\n",
    "                # decoder_input_data shape: #sample * LABEL_ENCODED_LEN * vocabulary_size\n",
    "                # decoder_target_data shape: #sample * LABEL_ENCODED_LEN * vocabulary_size\n",
    "                encoder_input_data = to_categorical(X, num_classes=vocabulary_size, dtype='uint16')\n",
    "                decoder_input_data = to_categorical(Y, num_classes=vocabulary_size, dtype='uint16')\n",
    "                decoder_target_data = np.hstack((decoder_input_data[:,1:,:], np.zeros((decoder_input_data.shape[0],1,decoder_input_data.shape[2]), dtype=np.uint16)))\n",
    "\n",
    "                yield [encoder_input_data, decoder_input_data], decoder_target_data\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_seq2seq(batchSize):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import horovod\n",
    "    import horovod.tensorflow.keras as hvd    \n",
    "    import pydoop.hdfs as hdfs\n",
    "    from pyarrow.parquet import ParquetDataset    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    from petastorm import make_batch_reader\n",
    "    from petastorm.tf_utils import make_petastorm_dataset\n",
    "    from petastorm.predicates import in_lambda\n",
    "        \n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    sc = SparkContext()\n",
    "            \n",
    "    hvd.init()\n",
    "    \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "              \n",
    "    trainingShuffleSize = int(trainingSampleCnt / hvd.size())\n",
    "    \n",
    "    # Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "    keras.backend.set_session(tf.Session(config=config))\n",
    "    \n",
    "    model = createModel(vocabulary_size, hvd)\n",
    "\n",
    "    sess = tf.Session()\n",
    "            \n",
    "    callbacks_list = [\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "        hvd.callbacks.MetricAverageCallback(),\n",
    "        hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, initial_lr=LEARNING_RATE * hvd.size()),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10),\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=20),\n",
    "        keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "        model_checkpoint = ModelCheckpoint(model_checkpoint_file, monitor='val_acc', save_best_only=True, save_weights_only=True, mode='auto')\n",
    "        callbacks_list.append(model_checkpoint)\n",
    "    \n",
    "    training_start_tm = datetime.datetime.now()\n",
    "\n",
    "    predicate_filter = in_lambda([\"raw_query\"], lambda x: x.str.len() >= 3)\n",
    "    # argument for make_batch_reader()\n",
    "    # predicate = predicate_filter,\n",
    "    \n",
    "    with make_batch_reader(trainingDataSet, num_epochs=None,\n",
    "                           cur_shard=hvd.rank(), shard_count=hvd.size(), \n",
    "                           schema_fields=DATA_COLS,\n",
    "                           hdfs_driver=PETASTORM_HDFS_DRIVER) as train_reader:\n",
    "        with make_batch_reader(valDataSet, num_epochs=None,\n",
    "                               cur_shard=hvd.rank(), shard_count=hvd.size(),\n",
    "                               schema_fields=DATA_COLS,\n",
    "                               hdfs_driver=PETASTORM_HDFS_DRIVER) as val_reader:\n",
    "            \n",
    "            train_ds = make_petastorm_dataset(train_reader)\n",
    "            train_ds_gen = make_generator(train_ds, sc, sess, batchSize, graph, trainingShuffleSize)\n",
    "\n",
    "            val_ds = make_petastorm_dataset(val_reader)\n",
    "            val_ds_gen = make_generator(val_ds, sc, sess, batchSize, graph)\n",
    "            \n",
    "            history = model.fit(train_ds_gen,\n",
    "                          validation_data=val_ds_gen,\n",
    "                          epochs = EPOCH_CNT, \n",
    "                          steps_per_epoch=int(trainingSampleCnt/batchSize/hvd.size()),\n",
    "                          validation_steps=int(valSampleCnt/batchSize/hvd.size()),\n",
    "                          callbacks = callbacks_list,\n",
    "                          verbose=2 if hvd.rank() == 0 else 0)\n",
    "\n",
    "            training_end_tm = datetime.datetime.now()\n",
    "\n",
    "            print(f\"\\nTraining Finished!!!  Time: {training_end_tm - training_start_tm}\")\n",
    "\n",
    "            print(\"Saving Model file...\")\n",
    "            model.save(model_file)\n",
    "            model.save_weights(model_weights_file)\n",
    "\n",
    "            json_config = model.to_json()\n",
    "            with open(model_config_json, 'w') as json_file:\n",
    "                json_file.write(json_config)\n",
    "                \n",
    "            with open(training_history_pickle, 'wb') as history_file:\n",
    "                pickle.dump(history.history, history_file)\n",
    "            \n",
    "            print(\"Model file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mactx.run(train_seq2seq, args=(BATCH_SIZE,), env={'LIBHDFS_OPTS': '-Xms8192m -Xmx8192m', 'TF_CPP_MIN_LOG_LEVEL': '3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mactx.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
