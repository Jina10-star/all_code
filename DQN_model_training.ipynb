{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install dataclasses\n",
    "# %pip install matching-ds-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import re\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from queryrunner_client import Client\n",
    "USER_EMAIL = 'ssadeghi@uber.com'\n",
    "qclient = Client(user_email=USER_EMAIL)\n",
    "CONSUMER_NAME = 'intelligentdispatch'\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#num_cores = multiprocessing.cpu_count()  -- 48\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from queryrunner_client import Client as QRClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import mdstk\n",
    "from mdstk.data_fetcher.data_fetcher import DataFetcher\n",
    "from mdstk.data_fetcher.cached_data_fetcher import CachedDataFetcher\n",
    "import datetime\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CASE 1: San Francisco\n",
    "city_id = 1\n",
    "vvid = 8\n",
    "MAX_LAT = 38.19\n",
    "MIN_LAT = 37.09\n",
    "MAX_LNG = -121.55\n",
    "MIN_LNG = -122.60\n",
    "LAT_CENTER = 37.6\n",
    "LON_CENTER = -122.2\n",
    "\n",
    "\n",
    "CASE 2: Detroit\n",
    "city_id = 50\n",
    "vvid = 425\n",
    "MAX_LAT = 42.89\n",
    "MIN_LAT = 42.01\n",
    "MAX_LNG = -82.68\n",
    "MIN_LNG = -83.93\n",
    "LAT_CENTER = 42.420149389121406\n",
    "LON_CENTER = -83.15996619595755\n",
    "\n",
    "\n",
    "Case 3: Philadelphia\n",
    "city_id = 20\n",
    "vvid = 663\n",
    "MAX_LAT = 40.22\n",
    "MIN_LAT = 39.74\n",
    "MAX_LNG = -75.46\n",
    "MIN_LNG = -74.86\n",
    "LAT_CENTER = 39.95201837418434\n",
    "LON_CENTER = -75.15727285438611\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INPUTS\n",
    "\n",
    "prefix = 'replay'\n",
    "\n",
    "city_id = 20\n",
    "vvid = 663\n",
    "MAX_LAT = 40.22\n",
    "MIN_LAT = 39.74\n",
    "MAX_LNG = -74.86\n",
    "MIN_LNG = -75.46\n",
    "LAT_CENTER = 39.95201837418434\n",
    "LON_CENTER = -75.15727285438611\n",
    "        \n",
    "datestrs = [  # 1 week\n",
    "    '2022-11-13',\n",
    "    '2022-11-14',\n",
    "    '2022-11-15',\n",
    "    '2022-11-16',\n",
    "    '2022-11-17',\n",
    "    '2022-11-18',\n",
    "    '2022-11-20',\n",
    "    '2022-11-21',\n",
    "    '2022-11-22',\n",
    "    '2022-11-23',\n",
    "    '2022-11-24',\n",
    "    '2022-11-25',\n",
    "    '2022-11-26',\n",
    "    '2022-11-27',\n",
    "    '2022-11-28',\n",
    "    '2022-11-29',\n",
    "    '2022-11-30',\n",
    "    '2022-12-01',\n",
    "    '2022-12-02',\n",
    "    '2022-12-03',\n",
    "    '2022-12-04',\n",
    "    '2022-12-05',\n",
    "    '2022-12-06',\n",
    "    '2022-12-07',\n",
    "    '2022-12-08',\n",
    "]\n",
    "\n",
    "\n",
    "## ML TRAINING\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "ITERATIONS = 500000\n",
    "DISCOUNT = 0.995\n",
    "VALUE_DEGRADE_LEVEL = 0.85\n",
    "FRAC_IDLES = 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(VALUE_DEGRADE_LEVEL) / np.log(DISCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "QUERY = \"\"\"\n",
    "-- This query has been rewritten by ssadeghi\n",
    "SELECT\n",
    "  --Request Information\n",
    "  ft.datestr,\n",
    "  ft.city_id,\n",
    "  ft.uuid as trip_uuid,\n",
    "  ft.session_id as session_uuid,\n",
    "  ft.driver_uuid,\n",
    "\n",
    "  --Time Information,\n",
    "  ft.request_timestamp_local,\n",
    "  ft.request_timestamp_utc,\n",
    "  ft.eta,\n",
    "  ft.client_upfront_fare_usd as fare,\n",
    "\n",
    "  --Distances and duration of the request,\n",
    "  ft.trip_duration_seconds / 60 as duration_min,\n",
    "\n",
    "  --Rider Pricing information\n",
    "--  kfk.msg.rider_fare_fs_totals_total as Rider_Price_Total,\n",
    "--  kfk.msg.rider_fare_fs_totals_total AS Total_Rider_Fare,\n",
    "--  kfk.msg.context_rider_surge_multiplier AS Rider_Surge_Multiplier,\n",
    "  \n",
    "--   Driver origin information\n",
    "  mez.BASE.accepted_lat as driver_origin_lat,\n",
    "  mez.BASE.accepted_lng as driver_origin_lng,\n",
    "  mez.BASE.begintrip_lat as pickup_lat,\n",
    "  mez.BASE.begintrip_lng as pickup_lng,\n",
    "  mez.BASE.dropoff_lat as dropoff_lat,\n",
    "  mez.BASE.dropoff_lng as dropoff_lng\n",
    "\n",
    "  from rawdata.schemaless_mezzanine_trips_rows mez\n",
    "--  left join rawdata_user.kafka_hp_fares_intelligence_rides_fare_driveroffer_nodedup kfk ON mez.BASE.uuid = kfk.msg.context_trip_uuid\n",
    "  join  restricted_dwh.fact_trip ft on mez.BASE.uuid = ft.uuid\n",
    "--   CROSS JOIN UNNEST(provider_charges) AS t(charges)\n",
    "where\n",
    "  TRUE\n",
    "  and lower(ft.global_product_name)='uberx'\n",
    "  and lower(ft.status)='completed'\n",
    "  and ft.city_id = {city_id}\n",
    "  and date(ft.datestr) = CAST('{datestr}' as date)\n",
    "  and date(mez.datestr) = CAST('{datestr}' as date)\n",
    "  and ft.dispatch_type IS NULL  \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_id, num_days, datestr\n",
    "\n",
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    city_id: int\n",
    "    datestr: str\n",
    "    num_days: int\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.datestr}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, datestr=self.datestr)\n",
    "        \n",
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [Query(prefix=prefix, city_id=city_id, datestr=datestr, num_days=1) for datestr in datestrs]\n",
    "\n",
    "cache_qry_map = {q.name: q.qry for q in queries}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='phx2',\n",
    "    datasource='presto-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new objective function\n",
    "def clean_df(df):\n",
    "    df = df[df['fare'].notnull()]\n",
    "    df = df[df['driver_origin_lng'] < MAX_LNG]\n",
    "    df = df[df['driver_origin_lng'] > MIN_LNG]\n",
    "    df = df[df['driver_origin_lat'] < MAX_LAT]\n",
    "    df = df[df['driver_origin_lat'] > MIN_LAT]\n",
    "    df['request_timestamp_local'] = pd.to_datetime(df.request_timestamp_local)\n",
    "    df['weekday'] = df.request_timestamp_local.dt.dayofweek\n",
    "    df['second_in_day'] = df.request_timestamp_local.dt.hour * 3600 + \\\n",
    "                          df.request_timestamp_local.dt.minute * 60 + \\\n",
    "                          df.request_timestamp_local.dt.second\n",
    "    df['trip_duration_seconds'] = df.duration_min * 60\n",
    "    df['total_driver_trip_time'] = df.trip_duration_seconds + df.eta\n",
    "    df['destination_arrival_time'] = df.total_driver_trip_time + df.second_in_day\n",
    "    mask = df['destination_arrival_time'] > 24 * 3600\n",
    "    df['destination_arrival_time'] = df['destination_arrival_time'].mod(24 * 3600)\n",
    "    df['weekday'][mask] = (df['weekday'][mask] + 1) % 7\n",
    "#     df['trip_length'][df['trip_length'] <= 100] = 100\n",
    "#     df = df.drop_duplicates(subset=['job_uuid', 'supply_uuid'])\n",
    "    df = df.dropna()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot.scatter(x='driver_origin_lat', y='driver_origin_lng', c='Rider_Price_Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query supply idling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "QUERY = \"\"\"\n",
    "SELECT\n",
    "sp.earner_uuid as supply_uuid,\n",
    "sp.datestr as datestr,\n",
    "sp.city_id as city_id,\n",
    "sp.duration_ms.open as open_duration,\n",
    "sp.earner_state as state,\n",
    "sp.start_timestamp.local as local_time,\n",
    "sp.location.lat as lat,\n",
    "sp.location.lng as lng,\n",
    "sp.flow_type as flow_type\n",
    "FROM\n",
    "driver.fact_earner_supply_minute as sp\n",
    "WHERE\n",
    "sp.datestr='{datestr}' and\n",
    "LOWER(sp.flow_type) IN ('uberx', 'p2p') and\n",
    "sp.city_id={city_id} and\n",
    "LOWER(sp.earner_state)='open'\n",
    "LIMIT 1000000\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_id, num_days, datestr\n",
    "\n",
    "prefix = 'supply'\n",
    "\n",
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    city_id: int\n",
    "    datestr: str\n",
    "    num_days: int\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.datestr}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, datestr=self.datestr)\n",
    "        \n",
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [Query(prefix=prefix, city_id=city_id, datestr=datestr, num_days=1) for datestr in datestrs]\n",
    "\n",
    "cache_qry_map = {q.name: q.qry for q in queries}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='phx2',\n",
    "    datasource='presto-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplies = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True) \n",
    "supplies.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(supplies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new objective function\n",
    "def clean_df(df):\n",
    "    df = df[df['local_time'].notnull()]\n",
    "    df = df[df['lng'] < MAX_LNG]\n",
    "    df = df[df['lng'] > MIN_LNG]\n",
    "    df = df[df['lat'] < MAX_LAT]\n",
    "    df = df[df['lat'] > MIN_LAT]    \n",
    "    df['local_time'] = pd.to_datetime(df.local_time)\n",
    "    df['weekday'] = df.local_time.dt.dayofweek\n",
    "    df['second_in_day'] = df.local_time.dt.hour * 3600 + \\\n",
    "                          df.local_time.dt.minute * 60 + \\\n",
    "                          df.local_time.dt.second\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplies = clean_df(supplies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "supplies.plot.scatter(x='lat', y='lng', color = 'red', s = 2, ax=ax)\n",
    "df.plot.scatter(x='driver_origin_lat', y='driver_origin_lng', color = 'blue', s = 2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training offline DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_sim.util import *\n",
    "from mini_sim.DQN_offlineData import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from collections import defaultdict, deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idleTripDuration = np.log(VALUE_DEGRADE_LEVEL) / np.log(DISCOUNT)\n",
    "\n",
    "class training():\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda'\n",
    "        self.valueNetwork = self.neuralNetInit()\n",
    "        self.targetNetwork = self.neuralNetInit()\n",
    "#         self.Loss = nn.SmoothL1Loss()\n",
    "        self.Loss = nn.GaussianNLLLoss()\n",
    "        self.Optimizer = torch.optim.RMSprop(self.valueNetwork.parameters(), lr = 1e-6)\n",
    "        self.lossLog = deque([],maxlen = 10000)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.df = df\n",
    "        self.idles = supplies\n",
    "        self.discount = DISCOUNT\n",
    "        self.lenDF = len(self.df)\n",
    "        self.lenSUP = len(self.idles)\n",
    "        \n",
    "    def batchMaker(self, n):\n",
    "        n_trip_samples = int((1 - FRAC_IDLES) * n)\n",
    "        n_idle_samples = n - n_trip_samples\n",
    "        sample = self.df.iloc[np.random.choice(self.lenDF, n_trip_samples)]\n",
    "        sampleIdle = self.idles.iloc[np.random.choice(self.lenSUP, n_idle_samples)]\n",
    "        batch_v1 = torch.zeros(n, 4).to(self.device)\n",
    "        batch_v2 = torch.zeros(n, 4).to(self.device)\n",
    "        batch_eta = torch.zeros(n, 1).to(self.device)\n",
    "        batch_tripDuration = torch.zeros(n, 1).to(self.device)\n",
    "        batch_fares = torch.zeros(n, 1).to(self.device)\n",
    "\n",
    "        ### make samples based on en routes\n",
    "        batch_v1[:n_trip_samples, 0] = torch.tensor(sample.weekday.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:n_trip_samples, 1] = torch.tensor(sample.second_in_day.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:n_trip_samples, 2] = torch.tensor(sample.driver_origin_lat.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[:n_trip_samples, 3] = torch.tensor(sample.driver_origin_lng.to_numpy(), dtype=torch.float)\n",
    "        batch_eta[:n_trip_samples,0] = torch.tensor(sample.eta.to_numpy(), dtype=torch.float)\n",
    "        batch_tripDuration[:n_trip_samples,0] = torch.tensor(sample.trip_duration_seconds.to_numpy(), dtype=torch.float)\n",
    "        batch_fares[:n_trip_samples,0] = torch.tensor(sample.fare.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:n_trip_samples, 0] = torch.tensor(sample.weekday.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:n_trip_samples, 1] = torch.tensor(sample.destination_arrival_time.to_numpy() , dtype=torch.float)\n",
    "        batch_v2[:n_trip_samples, 2] = torch.tensor(sample.dropoff_lat.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[:n_trip_samples, 3] = torch.tensor(sample.dropoff_lng.to_numpy(), dtype=torch.float)\n",
    "        \n",
    "        ### make samples based on idles\n",
    "        batch_v1[n_trip_samples:, 0] = torch.tensor(sampleIdle.weekday.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[n_trip_samples:, 1] = torch.tensor(sampleIdle.second_in_day.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[n_trip_samples:, 2] = torch.tensor(sampleIdle.lat.to_numpy(), dtype=torch.float)\n",
    "        batch_v1[n_trip_samples:, 3] = torch.tensor(sampleIdle.lng.to_numpy(), dtype=torch.float)\n",
    "        batch_v2[n_trip_samples:] = batch_v1[n_trip_samples:].clone()\n",
    "        batch_tripDuration[n_trip_samples:,0] = idleTripDuration\n",
    "        \n",
    "        return (batch_v1, batch_v2, batch_eta, batch_tripDuration, batch_fares)\n",
    "\n",
    "    def neuralNetInit(self):\n",
    "        model = Net(MAX_LAT, MIN_LAT, MAX_LNG, MIN_LNG).to(self.device)\n",
    "        init_weights(model)\n",
    "        return model\n",
    "    \n",
    "    def targetNetworkGet(self, batch_x):\n",
    "        self.targetNetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            pred, _ = self.targetNetwork(batch_x)\n",
    "        return pred\n",
    "\n",
    "    def gradient_clipping(self, net, clip_val=0.1):\n",
    "        for p in net.parameters():\n",
    "            p.data.clamp(-clip_val, clip_val)\n",
    "    \n",
    "    def updateTargetNetwork(self):\n",
    "        self.targetNetwork.load_state_dict(self.valueNetwork.state_dict())\n",
    "        \n",
    "    def neuralNetworkStep(self):\n",
    "        self.valueNetwork.train()\n",
    "        batch = self.batchMaker(self.batch_size)\n",
    "        batch_v1, batch_v2, batch_eta, batch_tripDuration, batch_fares = batch\n",
    "        self.Optimizer.zero_grad()\n",
    "        ## state 1 value is estimated using policy net\n",
    "        stateValue1, variance = self.valueNetwork(batch_v1.to(self.device))\n",
    "        ## state 2 value is estimated using target net\n",
    "        stateValue2 = self.targetNetworkGet(batch_v2.to(self.device))\n",
    "        ## state 1 new approximate = gamma(idle + eta) * rew + gamma(idle + eta + trip) * state 2\n",
    "        timeNextState = batch_eta + batch_tripDuration\n",
    "        discountedNextState = torch.pow(self.discount, timeNextState) * stateValue2\n",
    "        timeFareCollected = batch_eta\n",
    "        discountedFare = torch.pow(self.discount, timeFareCollected) * batch_fares\n",
    "        newState1Estimate = discountedFare + discountedNextState\n",
    "        loss = self.Loss(stateValue1, newState1Estimate, variance) #+ F.mse_loss(stateValue1, newState1Estimate)\n",
    "        self.lossLog.append(loss.item())\n",
    "        loss.backward()\n",
    "        assert torch.sum(torch.abs(self.valueNetwork.fc1.weight.grad)) > 1e-5\n",
    "        self.gradient_clipping(self.valueNetwork)\n",
    "        self.Optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "\n",
    "tr = training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(ITERATIONS):\n",
    "    tr.neuralNetworkStep()\n",
    "    if i % 200 == 0:\n",
    "        tr.updateTargetNetwork()\n",
    "    if i % 1000 == 0:\n",
    "        torch.save(tr.targetNetwork.state_dict(), f'checkpoints/LTSV_city{city_id}_vvid{vvid}_gamma{DISCOUNT}_discount{VALUE_DEGRADE_LEVEL}')\n",
    "        print(f'loss at {i} is {tr.lossLog[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.semilogy(tr.lossLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "\n",
    "tr1 = training()\n",
    "LTSV = Net(MAX_LAT, MIN_LAT, MAX_LNG, MIN_LNG).cuda()\n",
    "PATH = f'checkpoints/LTSV_city{city_id}_vvid{vvid}_gamma{DISCOUNT}_discount{VALUE_DEGRADE_LEVEL}'\n",
    "LTSV.load_state_dict(torch.load(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotMap(tr, n = 5000, weekday = 2, stat = 'mean'):\n",
    "    LTSV.eval()\n",
    "    data = np.zeros((24 * n, 4))\n",
    "    batch_v1, _, _, _, _ = tr.batchMaker(n)\n",
    "    batch_v1[:, 0] = weekday\n",
    "    data[:, 1:3] = batch_v1[:, -2:].cpu().repeat(24, 1)\n",
    "    with torch.no_grad():\n",
    "        for idx, selectedTime in enumerate(range(0, 24 * 3600, 3600)):\n",
    "            data[idx * n: (idx + 1) * n, 0] = idx + 1\n",
    "            batch_v1[:, 1] = selectedTime\n",
    "#             print(LTSV(batch_v1))\n",
    "#             sss\n",
    "            if stat == 'mean':\n",
    "                out, _ = LTSV(batch_v1)\n",
    "                out = out.cpu()\n",
    "            elif stat == 'cov':\n",
    "                mean, out = LTSV(batch_v1)\n",
    "                out = torch.sqrt(out) / mean\n",
    "                out = out.cpu()\n",
    "            else:\n",
    "                raise Exception('stat invalid')\n",
    "            data[idx * n: (idx + 1) * n, -1] = out[:,0]\n",
    "\n",
    "    hours_df = pd.DataFrame(data = data, columns = ['hour', 'lat', 'lng', 'value'])\n",
    "\n",
    "    fig = ff.create_hexbin_mapbox(\n",
    "        data_frame=hours_df, \n",
    "        lat=\"lat\",\n",
    "        lon=\"lng\",\n",
    "        color='value',\n",
    "        animation_frame='hour',\n",
    "        nx_hexagon=100, opacity=0.8,\n",
    "        min_count=20\n",
    "    )    \n",
    "\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=8, mapbox_center = {\"lat\": LAT_CENTER, \"lon\": LON_CENTER},)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "    fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 600\n",
    "    fig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 600\n",
    "    fig.layout.coloraxis.showscale = True   \n",
    "    fig.layout.sliders[0].pad.t = 10\n",
    "    fig.layout.updatemenus[0].pad.t= 10             \n",
    "\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotMap(tr1, n = 100000, weekday = 6, stat = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMap(tr1, n = 100000, weekday = 6, stat = 'cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1am, x1, y1) --> (2am, x2, y2) w/ $10\n",
    "# (4am, x1, y1) --> (4:30am, x3, y3) w/ $8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week, sec, lat, lng\n",
    "LTSV.eval()\n",
    "n = 10000\n",
    "hr = 10\n",
    "data = np.zeros((n, 4))\n",
    "data[:,0] = 2\n",
    "data[:,0] = 24 * hr\n",
    "data[:,2] = np.random.rand(n) * (MAX_LAT - MIN_LAT) + MIN_LAT\n",
    "data[:,3] = np.random.rand(n) * (MAX_LNG - MIN_LNG) + MIN_LNG\n",
    "with torch.no_grad():\n",
    "    inp = torch.tensor(data).float().cuda()\n",
    "    mean, out = LTSV(inp)\n",
    "    out = torch.sqrt(out) / mean\n",
    "    out = out.cpu()\n",
    "    data[idx * n: (idx + 1) * n, -1] = out[:,0]\n",
    "\n",
    "hours_df = pd.DataFrame(data = data, columns = ['hour', 'lat', 'lng', 'value'])\n",
    "\n",
    "fig = ff.create_hexbin_mapbox(\n",
    "    data_frame=hours_df, \n",
    "    lat=\"lat\",\n",
    "    lon=\"lng\",\n",
    "    color='value',\n",
    "    animation_frame='hour',\n",
    "    nx_hexagon=100, opacity=0.8,\n",
    "    min_count=20\n",
    ")    \n",
    "\n",
    "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=8, mapbox_center = {\"lat\": LAT_CENTER, \"lon\": LON_CENTER},)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 600\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"transition\"][\"duration\"] = 600\n",
    "fig.layout.coloraxis.showscale = True   \n",
    "fig.layout.sliders[0].pad.t = 10\n",
    "fig.layout.updatemenus[0].pad.t= 10             \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Python 3.7 (General DS)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
