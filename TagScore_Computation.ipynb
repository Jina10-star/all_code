{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These bad words were mined by first identifying the most popular tags, and then manually inspecting top-K of them..\n",
    "#Most popular tags were found from the file \"PopularTagsPostProcessed.csv\", which was produced by the Java function \"cleanUpTagItemData()\" and \"processPopularTagsFile.csv\"\n",
    "\n",
    "badWordSet1 = set (['main', 'side', 'special', 'dish', 'entrée', 'lunch', 'breakfast', 'menu', 'plate', 'popular', 'meal', 'extra', 'combo', 'order', 'item', 'signature', 'platter', 'course', 'combination', 'dinner', 'carte', 'local', 'favorite', 'speciality', 'bowl', 'classic', 'small', 'bite', 'plato', 'bar', 'day', 'grocery', 'brunch', 'entrée', 'kitchen', 'deal', 'treat', 'hand', 'foot', 'desayuno', 'add', 'pantry', 'general', 'cuisine', 'care', 'buy', 'one', 'get', 'free', 'offer', 'value', 'eat', 'add-on', 'new', 'quick', 'basket', 'value', 'mix', 'miscellaneous', 'something', 'different', 'set', 'family', 'style', 'refreshment', 'big', 'misc', 'chef', 'recommendation', 'les', 'plus', 'populaires', 'food', 'weekend', 'papa', 'serve', 'everything', 'else', 'smalls'])\n",
    "\n",
    "badWordSet2 = set (['main', 'side', 'special', 'breakfast', 'lunch', 'combo', 'dinner', 'plate', 'platter', 'popular', 'entrée', 'food', 'menu', 'speciality', 'dish', 'signature', 'extra', 'combo', 'order', 'item', 'house', 'something', 'additional', 'course', 'extra', 'bowl', 'carte', 'small', 'soft', 'favorite', 'local', 'quick', 'bite', 'grocery', 'classic', 'day', 'hand', 'foot', 'treat', 'slice', 'chef', 'desayuno', 'combination', 'lip', 'general', 'pantry', 'cuisine', 'add', 'share', 'family', 'entrante', 'daily', 'offer', 'limited', 'time', 'extra', 'buy', 'one', 'get', 'free', 'new', 'item', 'arrival', 'basket', 'build', 'pick', 'supplies'])\n",
    "\n",
    "badWordSet = badWordSet1 | badWordSet2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: sentence-transformers in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.11.2)\n",
      "Requirement already satisfied: scikit-learn in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: nltk in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (3.6.7)\n",
      "Requirement already satisfied: sentencepiece in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (4.18.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: scipy in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: numpy in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: requests in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.24.0)\n",
      "Requirement already satisfied: importlib-metadata in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: dataclasses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.53)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: importlib-resources in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from tqdm->sentence-transformers) (5.4.0)\n",
      "Requirement already satisfied: joblib in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from nltk->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: click in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from torchvision->sentence-transformers) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: six in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: scikit-learn==0.21 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.21.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from scikit-learn==0.21) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from scikit-learn==0.21) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from scikit-learn==0.21) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==0.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testString = \"lunch\\/dinner\" \n",
    "testString = testString.replace(\"\\/\", \" \")\n",
    "#repStr = re.sub(\"\\/\", \" \", testString)\n",
    "nameSet = set (testString.split())\n",
    "nameSet.issubset(badWordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TagToItemFileName = '../TagsToItems.txt' #This file comes from \"cleanUpTagItemData() function of Java\". This is the input file name\n",
    "\n",
    "with open(TagToItemFileName) as f:\n",
    "    tagToItems = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34485"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (tagToItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooldrink--->>>51--->>>pepsi;;;red bull;;;fanta pineapple;;;ice cube;;;ice tea lemon;;;soft drink;;;lemon ice tea;;;litre pepsi;;;ice tea peach;;;schweppe ginger ale litre;;;schweppe lemonade;;;water;;;appletiser;;;stoney 2l;;;sparkling water;;;fanta orange 2l;;;sprite 2l;;;grapetiser;;;ice bag;;;coke;;;coke light;;;schweppe tonic water;;;appletizer;;;valpre still;;;red grapetiser;;;tizer;;;schweppe tonic water litre;;;sparberry 2l;;;fanta orange;;;cappy juice;;;powerade mountain blast;;;cream soda;;;tomato juice;;;still water;;;sprite;;;sterus stumpie strawberry;;;tab;;;grapetizer;;;chinese herbal tea;;;stoney;;;coca cola 2l;;;water still;;;creme soda;;;fanta;;;redbull;;;coke 2l;;;powerade jagged ice;;;coke litre;;;coca cola;;;peach ice tea;;;coke sugar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagToItems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import math\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagToItemList = []\n",
    "for l in tagToItems:\n",
    "    tokens = l.split(\"--->>>\")\n",
    "    tagToItemList.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lip care'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagToItemList[5540][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUTLIERS_LIMIT = 0.1 #We consider top 10% most distant \n",
    "\n",
    "MAX_ITEMS_LIMIT = 20000 #Any tag having more items than this is considered a bad tag, as it is too diversified..\n",
    "\n",
    "DIVERSITY_THRESHOLD = 5 # We compute 1/d.A, if 1/d.A is less than this, the tag is considered too widespread, and marked as bad\n",
    "\n",
    "SCORE_THRESHOLD = 5 # We compute the overall score as (log (N)/log (k))/d.A\n",
    "\n",
    "FinalScoreList = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "lines_to_skip = 0 #34450\n",
    "\n",
    "TagScoreFilePath = \"TagScore_File_Test_lines_to_skip_26250.csv\" #This is the outputfile name\n",
    "\n",
    "f = open (TagScoreFilePath, \"a\") #changed from \"w\" to \"a\"\n",
    "\n",
    "for l in range (len(tagToItemList)):\n",
    "#for l in range (35):\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    if (counter % 10 == 0):\n",
    "        f.flush()\n",
    "        print (\"Tags Processed So far:\" + str (counter))\n",
    "\n",
    "    itemsList = tagToItemList[l + lines_to_skip][2]\n",
    "    itemsTokens = itemsList.split(\";;;\")\n",
    "    tagName = tagToItemList[l + lines_to_skip][0]\n",
    "    \n",
    "    tagName = tagName.replace (\"\\/\", \" \") #Due to some parsing error, we have this pattern in some tags\n",
    "    \n",
    "    tagWordSet = set (tagName.split());\n",
    "    \n",
    "    if (tagWordSet.issubset(badWordSet)):\n",
    "        \n",
    "        score = 0\n",
    "        temp = [tagName, score]\n",
    "        #f.write (str (tagName) + \", \" + str (score) + \"\\n\")\n",
    "        f.write (str (tagName) + \", \" + str (score) + \", \" + str (0) + \", \" + str (len (itemsTokens)) + \", \" +\"Banned Tags\" + \"\\n\")\n",
    "  \n",
    "        FinalScoreList.append(temp)\n",
    "        continue\n",
    "        \n",
    "\n",
    "    if (len (itemsTokens) == 1):\n",
    "        score = 0\n",
    "        temp = [tagName, score]\n",
    "        #f.write (str (tagName) + \", \" + str (score) + \"\\n\")\n",
    "        f.write (str (tagName) + \", \" + str (score) + \", \" + str (0) + \", \" + str (len (itemsTokens)) + \", \" +\"Bad - too small\" + \"\\n\")\n",
    "  \n",
    "        FinalScoreList.append(temp)\n",
    "        continue\n",
    "    if (len (itemsTokens) >= MAX_ITEMS_LIMIT):\n",
    "        score = 0\n",
    "        temp = [tagName, score]\n",
    "        #f.write (str (tagName) + \", \" + str (score) + \"\\n\")\n",
    "        f.write (str (tagName) + \", \" + str (score) + \", \" + str (0) + \", \" + str (len (itemsTokens)) + \", \" +\"Bad - too big\" + \"\\n\")\n",
    "        FinalScoreList.append(temp)\n",
    "        continue\n",
    "\n",
    "    tag_embeddings = model.encode(itemsTokens)\n",
    "    pw_distance = pairwise_distances(tag_embeddings, metric=\"cosine\", n_jobs=-1)\n",
    "\n",
    "\n",
    "    avgDist =[]\n",
    "\n",
    "    for i in range (len (pw_distance)):\n",
    "        avgDist.append(stat.mean (pw_distance[i]))\n",
    "\n",
    "    arr = np.array (avgDist)\n",
    "\n",
    "    n = math.floor (len(itemsTokens)*MAX_OUTLIERS_LIMIT)\n",
    "\n",
    "    idx = (-arr).argsort()[:n] #idx is the index of outliers!!\n",
    "\n",
    "    idxSet = set (idx)\n",
    "\n",
    "    maximum = -99999\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for i in range (len (pw_distance)):\n",
    "        if (i not in idxSet):\n",
    "            maximum = max (maximum, max (pw_distance[i]))\n",
    "            if (first == True):\n",
    "                current_tag_embeddings = tag_embeddings[i]\n",
    "                first = False\n",
    "            else :\n",
    "                current_tag_embeddings = np.vstack ((current_tag_embeddings, tag_embeddings[i])) \n",
    "\n",
    "\n",
    "    array = np.transpose (current_tag_embeddings)\n",
    "    # passing the sum, array to function\n",
    "    result = list(map(sum, array))\n",
    "\n",
    "    result = np.array (result)\n",
    "\n",
    "    result = result/(len(current_tag_embeddings) - len (idx)) #Computed the centroid of all the points. Result is the \n",
    "                                                              #Centroid of all the points after removing the outliers\n",
    "\n",
    "    #result = getCentroid (current_tag_embeddings, idx)\n",
    "\n",
    "    #Now, computing the average distance of the points w.r.t. centroid\n",
    "\n",
    "    avgDistList = []\n",
    "    for i in range (len (current_tag_embeddings)):\n",
    "        avgDistList.append (cosine (result, current_tag_embeddings[i]))\n",
    "\n",
    "\n",
    "    avgDist = stat.mean (avgDistList)\n",
    "    \n",
    "    threshold = 1/(avgDist*maximum)\n",
    "    \n",
    "    score = math.log ((1+len (tag_embeddings)), 5)/(avgDist*maximum)\n",
    "    \n",
    "    if (threshold < DIVERSITY_THRESHOLD or score <= SCORE_THRESHOLD):\n",
    "        f.write (str (tagName) + \", \" + str (score) + \", \" + str (threshold) + \", \" + str (len (tag_embeddings)) + \", \" +\"Bad\" + \"\\n\")\n",
    "    else:\n",
    "        f.write (str (tagName) + \", \" + str (score) + \", \" + str (threshold) + \", \" + str (len (tag_embeddings)) + \", \" +\"Good\" + \"\\n\")\n",
    "    \n",
    "    print (str (tagName) + \", \" + str (score) + \"\\n\")\n",
    "\n",
    "    temp = [tagName, score]\n",
    "    FinalScoreList.append(temp)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCentroid (current_tag_embeddings, idx):\n",
    "    array = np.transpose (current_tag_embeddings)\n",
    "    # passing the sum, array to function\n",
    "    result = list(map(sum, array))\n",
    "\n",
    "    result = np.array (result)\n",
    "\n",
    "    result = result/(len(current_tag_embeddings) - len (idx)) #Computed the centroid of all the points. Result is the \n",
    "                                                              #Centroid of all the points after removing the outliers\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
