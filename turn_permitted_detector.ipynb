{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"kind\": \"spark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_route_analytics\", \n",
    "  \"numExecutors\": 400, \n",
    "  \"executorCores\": 1, \n",
    "  \"driverCores\": 4,\n",
    "  \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.executor.memoryOverhead\": 3072, \n",
    "    \"spark.locality.wait\": \"0\",\n",
    "    \"spark.default.parallelism\":10000\n",
    "  },\n",
    "  \"executorMemory\": \"24g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"PHX2/Secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * This section defines all the objects will be used in the following algorithm.\n",
    " */\n",
    "case class Segment (\n",
    "    segment_uuid: String,\n",
    "    start_junction_uuid: String,\n",
    "    end_junction_uuid: String\n",
    ")\n",
    "\n",
    "case class Location (\n",
    "    latitude: Double,\n",
    "    longitude: Double\n",
    ")\n",
    "\n",
    "case class SegmentTraversalCount (\n",
    "    segment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class NavRouteDivergence (\n",
    "    trip_id: List[String],\n",
    "    pre_div_segment: Segment,\n",
    "    div_segment: Segment,\n",
    "    post_div_suggested_segment: Segment,\n",
    "    post_div_traversed_segment: Segment\n",
    ")\n",
    "\n",
    "case class NavRouteDivergenceCount (\n",
    "    preDivSegment: Segment,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int,\n",
    "    sampleTrips: List[String]\n",
    ")\n",
    "\n",
    "case class TransitionTraversalCount (\n",
    "    firstSegment: Segment,\n",
    "    lastSegment: Segment,\n",
    "    viaSegment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class TransitionDivergenceFeature (\n",
    "    actualTransition: TransitionTraversalCount,\n",
    "    suggestedTransition: TransitionTraversalCount,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int\n",
    ")\n",
    "\n",
    "case class TransitionTraversalCountPair (\n",
    "    actualTransition: TransitionTraversalCount,\n",
    "    suggestedTransition: TransitionTraversalCount\n",
    ")\n",
    "\n",
    "case class TurnPermittedFeature (\n",
    "    segmentIds: List[String],\n",
    "    actualTraversalsCountOnTransition: Int,\n",
    "    suggestedTraversalsCountOnTransition: Int,\n",
    "    actualTraversalsCountOnSuggestedSegment: Int,\n",
    "    sampleTripsUuids: List[String]\n",
    ")\n",
    "\n",
    "case class UMMIssue (\n",
    "    issueuuid: String,\n",
    "    ummbuilduuid: String,\n",
    "    latitude: Double,\n",
    "    longitude: Double,\n",
    "    sampletripuuids: String,\n",
    "    featureuuids: String,\n",
    "    numberoftrips: Int,\n",
    "    cityid: Int\n",
    ")\n",
    "\n",
    "case class MapFeature (\n",
    "    uuid: String,\n",
    "    segments: List[String]\n",
    ")\n",
    "\n",
    "case class PermanentBarrier (\n",
    "    uuid: String,\n",
    "    featureType: String,\n",
    "    segment: String,\n",
    "    direction: String,\n",
    "    isCondition: Boolean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Input params for the algorithm\n",
    " */\n",
    "val startDate= \"2022-12-11\"\n",
    "val endDate = \"2022-12-17\"\n",
    "val ummVersion = \"f905b99a-8137-11ed-9118-000af7d19b40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "This class loads transitions from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession, Column}\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import scala.collection.mutable.Map\n",
    "\n",
    "\n",
    "object TransitionTraversalCountsLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadTTC(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select first_segment, last_segment, via_segments, \n",
    "         | sum(suggested_traversals) as suggested_traversals, \n",
    "         | sum(actual_traversals) as actual_traversals, \n",
    "         | sum(overlap_traversals) as overlap_traversals \n",
    "         | from route_corpus_features.transition_traversal_counts\n",
    "         | where datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | AND first_segment is not NULL\n",
    "         | AND last_segment is not NULL\n",
    "         | AND via_segments is not NULL\n",
    "         | AND via_segments[0] is not NULL\n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | group by first_segment, last_segment, via_segments\"\"\".stripMargin.replaceAll(\"\\n\", \" \")\n",
    "      \n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[TransitionTraversalCount] = {\n",
    "    import spark.implicits._\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        \n",
    "        val firstSegment = Segment(r.getAs[Row](\"first_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"first_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        val lastSegment = Segment(r.getAs[Row](\"last_segment\").getAs[String](\"segment_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"start_junction_uuid\"),\n",
    "                             r.getAs[Row](\"last_segment\").getAs[String](\"end_junction_uuid\"))\n",
    "        \n",
    "        var viaSegmentsBuffer = ListBuffer[Segment]()\n",
    "        \n",
    "        r.getAs[Seq[Any]](\"via_segments\").foreach(row => {\n",
    "            val segmentInfo = row.asInstanceOf[Row]\n",
    "            viaSegmentsBuffer += Segment(segmentInfo.getAs[String](\"segment_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"start_junction_uuid\"),\n",
    "                                         segmentInfo.getAs[String](\"end_junction_uuid\")\n",
    "                                        )\n",
    "        })\n",
    "        \n",
    "        \n",
    "      TransitionTraversalCount(\n",
    "        firstSegment,\n",
    "        lastSegment,\n",
    "        viaSegmentsBuffer.toList.head,  \n",
    "        r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        r.getAs[Long](\"actual_traversals\").toInt)\n",
    "    })\n",
    "      .filter(T => T.firstSegment.end_junction_uuid == T.viaSegment.start_junction_uuid && \n",
    "            T.viaSegment.end_junction_uuid == T.lastSegment.start_junction_uuid)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load divergences from route_corpus_features.transition_traversal_counts\n",
    "*/\n",
    "val ttcRaw = TransitionTraversalCountsLoader.loadTTC(startDate, endDate)\n",
    "val ttc = TransitionTraversalCountsLoader.makeDataset(ttcRaw).cache\n",
    "ttc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads maneuvers map features from umm.map_feature_maneuvers_tomtom\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object MapFeatureLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadTurnRestrictions(builduuid: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | uuid,\n",
    "         | data.maneuver.segments as segments\n",
    "         | from umm.map_feature_maneuvers_tomtom\n",
    "         | where builduuid = '$builduuid'\n",
    "         | and data.maneuver.type in ('FORBIDDEN_MANEUVER','FORBIDDEN_U_TURN')\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[MapFeature] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        var segments = ListBuffer[String]()\n",
    "        r.getAs[Seq[Row]](\"segments\").foreach(row => segments += row.getAs[String](\"uuid\"))    \n",
    "        \n",
    "        MapFeature(\n",
    "            uuid = r.getAs[String](\"uuid\"),\n",
    "            segments = segments.toList\n",
    "          )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load maneuvers map features from umm.map_feature_maneuvers_tomtom\n",
    "*/\n",
    "val mapFeaturesRaw = MapFeatureLoader.loadTurnRestrictions(ummVersion)\n",
    "val turnRestrictionMapFeatures = MapFeatureLoader.makeDataset(mapFeaturesRaw).cache()\n",
    "turnRestrictionMapFeatures.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object PermanentBarrierLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(buildId: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select uuid, \n",
    "         | data.roadfurniture.type as type, \n",
    "         | data.roadfurniture.condition as condition, \n",
    "         | data.roadfurniture.onsegment as onsegment\n",
    "         | from umm.map_feature_road_furnitures_tomtom\n",
    "         | where uuid is not null\n",
    "         | AND builduuid = '$buildId'\n",
    "         | AND data.roadfurniture.onsegment is not null\n",
    "         | AND data.roadfurniture.onsegment.uuid is not null\n",
    "         | AND data.roadfurniture.condition is null\n",
    "         | AND data.roadfurniture.type = 'PERMANENT_BARRIER'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[PermanentBarrier] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      PermanentBarrier(\n",
    "        uuid = r.getAs[String](\"uuid\"),\n",
    "        featureType = r.getAs[String](\"type\"),\n",
    "        segment = r.getAs[Row](\"onsegment\").getAs[String](\"uuid\"),\n",
    "        direction = r.getAs[Row](\"onsegment\").getAs[String](\"direction\"),\n",
    "        isCondition = false\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "val permanentBarriersRaw = PermanentBarrierLoader.load(ummVersion)\n",
    "val permanentBarriers = PermanentBarrierLoader.makeDataset(permanentBarriersRaw).cache\n",
    "permanentBarriers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trWithPb = turnRestrictionMapFeatures.alias(\"TR\").\n",
    "joinWith(permanentBarriers.alias(\"PB\"), \n",
    "         col(\"TR.segments\")(0)===col(\"PB.segment\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val trWithPb2 = turnRestrictionMapFeatures.alias(\"TR\").\n",
    "joinWith(permanentBarriers.alias(\"PB\"), \n",
    "         col(\"TR.segments\")(1)===col(\"PB.segment\")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trWithPb.map(r => r._1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trWithPb.map(r => r._1).distinct().filter(r => r.segments.length == 2).limit(20).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trWithPb2.map(r => r._1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trWithPb2.map(r => r._1).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trWithPb.map(r => r._1).distinct().union(trWithPb2.map(r => r._1).distinct()).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// ================================================================================================================\n",
    "// DATA LOADING DONE...\n",
    "// ================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* Finding the logic for 2 segment features\n",
    " * 1. Get all 2 segment turn restrictions\n",
    " * 2. Join with transitions where first and second segments of transition match or \n",
    " *     second and third segment of transition match\n",
    " */\n",
    "\n",
    "val mapFeaturesWith2Segments = turnRestrictionMapFeatures.except(trWithPb.map(r => r._1)).filter(r => r.segments.size <= 3).cache\n",
    "\n",
    "val features = ttc.alias(\"TTF\").joinWith(mapFeaturesWith2Segments.alias(\"TRF\"),\n",
    "                                        (col(\"TTF.firstSegment.segment_uuid\")===element_at(col(\"TRF.segments\"), 1)&&\n",
    "                                        col(\"TTF.viaSegment.segment_uuid\")===element_at(col(\"TRF.segments\"), 2)&&\n",
    "                                        col(\"TTF.lastSegment.segment_uuid\")===element_at(col(\"TRF.segments\"), 3))\n",
    "                                        ).\n",
    "                                groupBy(col(\"_2.uuid\")).\n",
    "                                agg(sum(col(\"_1.actualCount\")).alias(\"netActualCount\"),\n",
    "                                   sum(col(\"_1.suggestedCount\")).alias(\"netSuggestedCount\")).cache\n",
    "\n",
    "features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "publish features generated to HDFS in csv format\n",
    "*/\n",
    "features.\n",
    "filter(r => r.getAs[Long](\"netActualCount\") > 21).\n",
    "map(r => {\n",
    "    \n",
    "    val sampleTrips = r.getAs[Row](\"sampleTripsUuids\").getList(0)\n",
    "    \n",
    "    UMMIssue (\n",
    "        issueuuid = \"\",\n",
    "        ummbuilduuid = \"9cfbd494-5212-11ed-9455-5c6f6910eaea\",\n",
    "        latitude = 0.0,\n",
    "        longitude = 0.0,\n",
    "        sampletripuuids = sampleTrips,\n",
    "        featureuuids = r.getAs[String](\"uuid\"),\n",
    "        numberoftrips = r.getAs[Long](\"netActualCount\").toInt,\n",
    "        cityid = -1\n",
    "    )\n",
    "}).\n",
    "limit(2).collect().foreach(println)\n",
    "// repartition(1).\n",
    "// write.\n",
    "// mode(SaveMode.Overwrite).\n",
    "// option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\").\n",
    "// option(\"header\",\"true\").\n",
    "// csv(\"/user/dhruven.vora/turn_permitted_issues_2_seg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
