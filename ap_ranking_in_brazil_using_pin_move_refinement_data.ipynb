{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2a010b-2862-4af3-b225-eae5fc6d51f1",
   "metadata": {},
   "source": [
    "## AP Ranking in Brazil using Pin Move Refinement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510fe8-d418-4070-8204-5b93418238d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import ipyleaflet\n",
    "import lime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import sklearn.inspection\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "from eli5 import show_prediction, explain_prediction\n",
    "from ipyleaflet import Map, Heatmap, Marker, CircleMarker, MarkerCluster, basemaps, basemap_to_tiles, AwesomeIcon, LegendControl\n",
    "from IPython.display import Image, display_html\n",
    "from ipywidgets import HTML, IntSlider, Output\n",
    "from lime import lime_tabular\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, log_loss, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "random_seed = 42\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea46f7",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a6e6b-9afa-4dda-a454-7a9504ed1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_same_value_features = [\n",
    "    'request_gps_and_anchor_point_distance',\n",
    "    'request_gps_naive_snap_snapped_distance',\n",
    "    'pickup_anchor_naive_snap_snapped_distance'\n",
    "]\n",
    "\n",
    "numerical_different_features = [\n",
    "    'num_trips',\n",
    "    'arrived_cancel',\n",
    "    'arrived_contacts',\n",
    "    'avg_nple',\n",
    "    'nple_p95',\n",
    "    'loops_around', \n",
    "    'max_circle',\n",
    "    'overshoot_distance',\n",
    "    'distance_to_request_gps',\n",
    "    'distance_to_anchorpoint',\n",
    "    'distance_to_pickup_anchor_naive_snap',\n",
    "    'distance_to_request_gps_naive_snap'\n",
    "]\n",
    "\n",
    "numerical_features = numerical_same_value_features + numerical_different_features\n",
    "\n",
    "categorical_features = [    \n",
    "    'pickup_anchor_confidence',\n",
    "    'provider',\n",
    "    'product_type_name',\n",
    "    'time_bucket',\n",
    "    'city_id',\n",
    "    'pickup_waypoint_location_source',\n",
    "    'request_gps_naive_snap_traffic_direction',\n",
    "    'request_gps_naive_snap_road_usage',\n",
    "    'request_gps_naive_snap_road_side',\n",
    "    'request_gps_naive_snap_road_class',\n",
    "    'pickup_anchor_naive_snap_traffic_direction',\n",
    "    'pickup_anchor_naive_snap_road_usage',\n",
    "    'pickup_anchor_naive_snap_road_side',\n",
    "    'pickup_anchor_naive_snap_road_class'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd47d0",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b20832-784e-4626-8bcb-ebb191e533fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '<DATASET_DIR>'\n",
    "df_raw = pd.read_csv(os.path.join(DATA_DIR, '<data>.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541045c-5d28-4468-8882-73f920ceb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of trips in raw data: {df_raw.shape[0]}\")\n",
    "\n",
    "df_raw.columns = [x.split('.')[1] if '.' in (x) else x for x in df_raw.columns]\n",
    "\n",
    "# drop duplicates\n",
    "df_raw = df_raw.drop_duplicates(subset=['trip_uuid'])\n",
    "print(f\"Number of trips after dropping duplicates: {df_raw.shape[0]}\")\n",
    "\n",
    "# apply first distance to last filter\n",
    "df_raw = df_raw.loc[df_raw['first_distance_to_pickup'] >= 10]\n",
    "print(f\"Number of trips after applying first_distance_to_pickup filter: {df_raw.shape[0]}\")\n",
    "\n",
    "# apply PLE filter\n",
    "df_raw = df_raw.loc[df_raw['pickup_location_error'] <= 50]\n",
    "print(f\"Number of trips after applying PLE filter: {df_raw.shape[0]}\")\n",
    "\n",
    "# drop NULL values\n",
    "for col in numerical_different_features + ['hex_13']:\n",
    "    for prefix in ['first', 'last', 'begintrip', 'pickup']:\n",
    "        df_raw = df_raw.loc[pd.notnull(df_raw[f'{prefix}_' + col]) & (df_raw[f'{prefix}_' + col] != '\\\\N') & (df_raw[f'{prefix}_' + col] != float('inf')) & (df_raw[f'{prefix}_' + col] != -float('inf'))]\n",
    "print(f\"Number of trips after dropping null values: {df_raw.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b636f73-8041-4e9e-9548-cb6225ee2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, preferred_colname, unpreferred_colname):\n",
    "    for feature in numerical_same_value_features + categorical_features + ['datestr', 'trip_uuid']:\n",
    "        if feature in categorical_features:\n",
    "            from sklearn import preprocessing\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(df[feature])\n",
    "            df[feature] = le.transform(df[feature])\n",
    "        df[f'{preferred_colname}_{feature}'] = df[f'{unpreferred_colname}_{feature}'] = df[feature]\n",
    "        \n",
    "    # take log of the num_trips column\n",
    "    df[f'{preferred_colname}_num_trips'] = np.log(df[f'{preferred_colname}_num_trips'])\n",
    "    df[f'{unpreferred_colname}_num_trips'] = np.log(df[f'{unpreferred_colname}_num_trips'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_data(df_input, preferred_colname, unpreferred_colname, feature_list, filter_p95=False, filter_p5=False, filter_p99=False):\n",
    "    df_input = df_input.copy()\n",
    "    print(f\"Number of examples before filtering: {df_input.shape[0]}\")\n",
    "    \n",
    "    # drop NULL values\n",
    "    for col in feature_list:\n",
    "        df_input = df_input.loc[pd.notnull(df_input[f'{unpreferred_colname}_' + col]) & (df_input[f'{unpreferred_colname}_' + col] != '\\\\N') & (df_input[f'{unpreferred_colname}_' + col] != float('inf')) & (df_input[f'{unpreferred_colname}_' + col] != -float('inf'))]\n",
    "        df_input = df_input.loc[pd.notnull(df_input[f'{preferred_colname}_' + col]) & (df_input[f'{preferred_colname}_' + col] != '\\\\N') & (df_input[f'{preferred_colname}_' + col] != float('inf')) & (df_input[f'{preferred_colname}_' + col] != -float('inf'))]\n",
    "        try:\n",
    "            df_input[f'{unpreferred_colname}_' + col] = df_input[f'{unpreferred_colname}_' + col].astype(float)\n",
    "            df_input[f'{preferred_colname}_' + col] = df_input[f'{preferred_colname}_' + col].astype(float)\n",
    "        except:\n",
    "            continue\n",
    "    print(f\"Number of examples after dropping null values: {df_input.shape[0]}\")\n",
    "\n",
    "    # clean data\n",
    "    df_input = df_input[\n",
    "        (df_input[f'first_distance_to_{preferred_colname}'] < 200)\n",
    "        & (df_input[f'{preferred_colname}_distance_to_request_gps'] <= 200)\n",
    "        & (df_input[f'{unpreferred_colname}_distance_to_request_gps'] <= 200)\n",
    "    ]\n",
    "    print(f'Number of examples in cleaned data: {df_input.shape[0]}')\n",
    "    \n",
    "    if filter_p99:\n",
    "        for col in feature_list:\n",
    "            if df_input[f'{unpreferred_colname}_{col}'].dtype == 'float64':\n",
    "                df_input = df_input.loc[df_input[f'{unpreferred_colname}_{col}'] <= np.percentile(df_input[f'{unpreferred_colname}_{col}'], 99)]\n",
    "                df_input = df_input.loc[df_input[f'{preferred_colname}_{col}'] <= np.percentile(df_input[f'{preferred_colname}_{col}'], 99)]\n",
    "        print(f'Number of examples after applying p99 filter: {df_input.shape[0]}')\n",
    "        \n",
    "    if filter_p95:\n",
    "        for col in feature_list:\n",
    "            if df_input[f'{unpreferred_colname}_{col}'].dtype == 'float64':\n",
    "                df_input = df_input.loc[df_input[f'{unpreferred_colname}_{col}'] <= np.percentile(df_input[f'{unpreferred_colname}_{col}'], 95)]\n",
    "                df_input = df_input.loc[df_input[f'{preferred_colname}_{col}'] <= np.percentile(df_input[f'{preferred_colname}_{col}'], 95)]\n",
    "        print(f'Number of examples after applying p95 filter: {df_input.shape[0]}')\n",
    "        \n",
    "    if filter_p5:\n",
    "        for col in feature_list:\n",
    "            if df_input[f'{unpreferred_colname}_{col}'].dtype == 'float64':\n",
    "                df_input = df_input.loc[df_input[f'{unpreferred_colname}_{col}'] >= np.percentile(df_input[f'{unpreferred_colname}_{col}'], 5)]\n",
    "                df_input = df_input.loc[df_input[f'{preferred_colname}_{col}'] >= np.percentile(df_input[f'{preferred_colname}_{col}'], 5)]\n",
    "        print(f'Number of examples after applying p5 filter: {df_input.shape[0]}')\n",
    "\n",
    "    return df_input\n",
    "\n",
    "def generate_training_data(df, preferred_colname, unpreferred_colname, features_list):\n",
    "    df = df.copy()\n",
    "    \n",
    "    features_list_unpreferred, features_list_preferred = [], []\n",
    "    for col in features_list + ['datestr', 'trip_uuid']:\n",
    "        features_list_unpreferred.append(f'{unpreferred_colname}_{col}')\n",
    "        features_list_preferred.append(f'{preferred_colname}_{col}')\n",
    "    \n",
    "    df_unpreferred = df[features_list_unpreferred]\n",
    "    df_unpreferred.columns = [col.split(f'{unpreferred_colname}_')[1] for col in features_list_unpreferred]\n",
    "    df_unpreferred['y'] = 0\n",
    "    \n",
    "    df_preferred = df[features_list_preferred]\n",
    "    df_preferred.columns = [col[len(f'{preferred_colname}_'):] for col in features_list_preferred]\n",
    "    df_preferred['y'] = 1\n",
    "\n",
    "    df = pd.concat([df_unpreferred, df_preferred], axis=0)\n",
    "    df = df.sort_values(by=['datestr'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b85ee7",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e39d88-0bc3-4e84-b8c4-568e893589f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(df, feature_list):\n",
    "    \"\"\"\n",
    "    Train logistic regression model and return its parameters.\n",
    "    \n",
    "    :param df: transformed dataframe including both training and test set\n",
    "    :param feature_list: list of features to be put into the model\n",
    "    :return:\n",
    "        params dictionary of the logistic regression;\n",
    "        thresholds dictionary to handle large values of each feature.\n",
    "    \"\"\"\n",
    "    thresholds = dict(df[[c for c in feature_list if c != 'distancetotarget']].describe().loc['max'])\n",
    "\n",
    "    X = np.array(df[feature_list])\n",
    "    y = np.array(df['y'])\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    n = X.shape[0]\n",
    "    X_train, X_test, y_train, y_test = X[:int(0.8 * n)], X[int(0.8 * n):], y[:int(0.8 * n)], y[int(0.8 * n):]\n",
    "    \n",
    "    # normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    print(f\"Normalization means: {scaler.mean_}\")\n",
    "    \n",
    "    # model\n",
    "    clf = LogisticRegression(random_state=random_seed, max_iter=500).fit(X_train, y_train)\n",
    "    print('Accuracy of training set =', clf.score(X_train, y_train))\n",
    "    print('Accuracy of test set =', clf.score(X_test, y_test))\n",
    "    params = dict(zip(feature_list + ['intercept'], list(clf.coef_[0]) + [clf.intercept_[0]]))\n",
    "    print('Parameters: ', params)\n",
    "\n",
    "    # classification report\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_test_pred_proba = clf.predict_proba(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, y_test_pred))\n",
    "    print(\"Log loss =\", log_loss(y_test, y_test_pred_proba))\n",
    "    print(\"AUC-ROC =\", roc_auc_score(y_test, y_test_pred_proba[:, 1]))\n",
    "\n",
    "    return params, thresholds, clf, X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f107e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(df, feature_list):\n",
    "    X, y = np.array(df[feature_list]), np.array(df['y'])\n",
    "\n",
    "    n = X.shape[0]\n",
    "    X_train, X_test, y_train, y_test = X[:int(0.8 * n)], X[int(0.8 * n):], y[:int(0.8 * n)], y[int(0.8 * n):]\n",
    "    \n",
    "    # normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # model\n",
    "    clf = sklearn.svm.SVC(random_state=random_seed, max_iter=500, probability=False, kernel='rbf').fit(X_train, y_train)\n",
    "    print('Accuracy of training set =', clf.score(X_train, y_train))\n",
    "    print('Accuracy of test set =', clf.score(X_test, y_test))\n",
    "\n",
    "    # classification report\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b074ed7-67da-4047-b11a-28459e126fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(df, feature_list, monotone_constraints=None, enable_categorical=False):\n",
    "    X, y = np.array(df[feature_list]), np.array(df['y'])\n",
    "    n = X.shape[0]\n",
    "    X_train, X_test, y_train, y_test = X[:int(0.8 * n)], X[int(0.8 * n):], y[:int(0.8 * n)], y[int(0.8 * n):]\n",
    "    \n",
    "    # model\n",
    "    clf = xgb.XGBClassifier(tree_method='hist', n_jobs=1, monotone_constraints=monotone_constraints, \n",
    "                            enable_categorical=enable_categorical).fit(X_train, y_train)\n",
    "    print('Accuracy of training set =', clf.score(X_train, y_train))\n",
    "    print('Accuracy of test set =', clf.score(X_test, y_test))\n",
    "\n",
    "    # classification report\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_test_pred_proba = clf.predict_proba(X_test)\n",
    "    print(\"Classification Report:\", classification_report(y_test, y_test_pred))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, y_test_pred))\n",
    "    print(\"Log loss =\", log_loss(y_test, y_test_pred_proba))\n",
    "    print(\"AUC-ROC =\", roc_auc_score(y_test, y_test_pred_proba[:, 1]))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pairwise_rank(df, feature_list, unpreferred_colname, preferred_colname):\n",
    "    df = df.sort_values(by=['datestr'])\n",
    "    unpreferred = df[[f'{unpreferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "    preferred = df[[f'{preferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "    X, y = preferred - unpreferred, np.ones(preferred.shape[0])\n",
    "    X[::2] *= -1; y[::2] *= -1\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    X_train, X_test, y_train, y_test = X[:int(0.8 * n)], X[int(0.8 * n):], y[:int(0.8 * n)], y[int(0.8 * n):]\n",
    "    \n",
    "    # normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # model\n",
    "    clf = LogisticRegression(random_state=random_seed, max_iter=500).fit(X_train, y_train)\n",
    "    print('Accuracy of training set =', clf.score(X_train, y_train))\n",
    "    print('Accuracy of test set =', clf.score(X_test, y_test))\n",
    "    params = dict(zip(train_features_list + ['intercept'], list(clf.coef_[0]) + [clf.intercept_[0]]))\n",
    "    print('Parameters: ', params)\n",
    "\n",
    "    # classification report\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_test_pred_proba = clf.predict_proba(X_test)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Confusion Matrix:\", confusion_matrix(y_test, y_test_pred))\n",
    "    print(\"Log loss =\", log_loss(y_test, y_test_pred_proba))\n",
    "    print(\"AUC-ROC =\", roc_auc_score(y_test, y_test_pred_proba[:, 1]))\n",
    "    \n",
    "    return params, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbranker(df, feature_list, unpreferred_colname, preferred_colname, monotone_constraints=None, enable_categorical=False):\n",
    "    df = df.sort_values(by=['datestr'])\n",
    "    ngroups = df.shape[0]\n",
    "    unpreferred = df[[f'{unpreferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "    preferred = df[[f'{preferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "    X = np.empty((2 * ngroups, len(feature_list)), dtype=np.float64)\n",
    "    X[0::2] = unpreferred\n",
    "    X[1::2] = preferred\n",
    "    y = np.array([0,1] * ngroups)\n",
    "    \n",
    "    ntrain = int(0.8 * ngroups) * 2\n",
    "    X_train, X_test, y_train, y_test = X[:ntrain], X[ntrain:], y[:ntrain], y[ntrain:]\n",
    "    groups = 2 * np.ones(ntrain // 2)\n",
    "\n",
    "    model = xgb.XGBRanker(\n",
    "        tree_method='hist',\n",
    "        booster='gbtree',\n",
    "        objective='rank:pairwise',\n",
    "        random_state=random_seed, \n",
    "        learning_rate=0.1,\n",
    "        colsample_bytree=0.9,\n",
    "        eta=0.05, \n",
    "        max_depth=7, \n",
    "        n_estimators=300, \n",
    "        subsample=0.75,\n",
    "        monotone_constraints=monotone_constraints,\n",
    "        enable_categorical=enable_categorical\n",
    "    ).fit(X_train, y_train, group=groups)\n",
    "\n",
    "    y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "    print('Accuracy of training set =', np.sum(y_train_pred[0::2] <= y_train_pred[1::2]) / (y_train_pred.shape[0] / 2))\n",
    "    print('Accuracy of test set =', np.sum(y_test_pred[0::2] <= y_test_pred[1::2]) / (y_test_pred.shape[0] / 2))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c5e0a-91b2-4888-b884-f4a88991e375",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809d991-4a7a-4034-b68b-0eb2cee8b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpreferred_colname, preferred_colname = 'first', 'pickup'\n",
    "\n",
    "df = df_raw.copy()\n",
    "df = generate_features(df, preferred_colname, unpreferred_colname)\n",
    "\n",
    "blocklisted_features = {\n",
    "    'distance_to_request_gps_naive_snap',\n",
    "    'request_gps_and_anchor_point_distance',\n",
    "    'request_gps_naive_snap_snapped_distance',\n",
    "    'pickup_anchor_naive_snap_snapped_distance',\n",
    "    'loops_around', \n",
    "    'max_circle',\n",
    "    'overshoot_distance',\n",
    "    'distance_to_anchorpoint',\n",
    "    'distance_to_pickup_anchor_naive_snap',\n",
    "}\n",
    "train_features_list = list(filter(lambda feature: feature not in blocklisted_features, numerical_features))\n",
    "\n",
    "df = filter_data(df, preferred_colname, unpreferred_colname, feature_list=train_features_list, filter_p99=True)\n",
    "\n",
    "data = generate_training_data(df, preferred_colname, unpreferred_colname, train_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992c52a",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32493f-4bcb-435f-9615-25bdde2c9563",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params, thresholds, model, X, y = logistic_regression(data, train_features_list)\n",
    "\n",
    "# plot feature importance\n",
    "names, values = [], []\n",
    "for k, v in params.items():\n",
    "    names.append(k); values.append(v)\n",
    "\n",
    "rank = list(np.argsort(values)[::-1])\n",
    "sns.set(rc = {'figure.figsize':(10,10)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot([names[i] for i in rank], [values[i] for i in rank], color=\"royalblue\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0a10a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "pdp = sklearn.inspection.PartialDependenceDisplay.from_estimator(model, X[:5000], list(range(len(train_features_list))) + [(0,5)], feature_names=train_features_list, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6798974e",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9834ee-c423-4a4a-99e9-6ae646fddc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = {'num_trips': 1,'arrived_cancel': -1,'arrived_contacts': -1,'avg_nple': -1,'nple_p95': -1,'distance_to_request_gps': -1,'distance_to_anchorpoint': -1}\n",
    "model = xgboost(data, train_features_list, \n",
    "                monotone_constraints='(1,-1,-1,-1,-1,-1,-1,-1,-1)')\n",
    "\n",
    "feature_important = list(model.get_booster().get_score(importance_type='gain').values())\n",
    "rank = list(np.argsort(feature_important)[::-1])\n",
    "ax = sns.barplot([train_features_list[i] for i in rank], [feature_important[i] for i in rank], color=\"royalblue\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_booster().feature_names = train_features_list\n",
    "fig, ax = plt.subplots(figsize=(240, 240))\n",
    "xgb.plot_tree(model, num_trees=0, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c81ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sklearn.inspection.PartialDependenceDisplay.from_estimator(model, X[:10000], list(range(len(train_features_list))), feature_names=train_features_list, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac7eff",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Pairwise Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8630606",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, model = simple_pairwise_rank(df, train_features_list, unpreferred_colname, preferred_colname)\n",
    "\n",
    "# plot feature importance\n",
    "names, values = [], []\n",
    "for k, v in params.items():\n",
    "    names.append(k); values.append(v)\n",
    "\n",
    "rank = list(np.argsort(values)[::-1])\n",
    "sns.set(rc = {'figure.figsize':(10,10)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot([names[i] for i in rank], [values[i] for i in rank], color=\"royalblue\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sklearn.inspection.PartialDependenceDisplay.from_estimator(model, X, list(range(len(train_features_list))) + [(0,5)], feature_names=train_features_list, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3644b",
   "metadata": {},
   "source": [
    "#### LambdaMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab745e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgbranker(df, train_features_list, unpreferred_colname, preferred_colname, '(1,-1,-1,-1,-1,-1)')\n",
    "\n",
    "feature_important = list(model.get_booster().get_score(importance_type='gain').values())\n",
    "rank = list(np.argsort(feature_important)[::-1])\n",
    "ax = sns.barplot([train_features_list[i] for i in rank], [feature_important[i] for i in rank], color=\"royalblue\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f397f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_booster().feature_names = train_features_list\n",
    "fig, ax = plt.subplots(figsize=(240, 240))\n",
    "xgb.plot_tree(model, num_trees=0, ax=ax)\n",
    "# plt.gcf().set_size_inches(18.5, 20.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ff602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "model._estimator_type = 'regressor'\n",
    "sklearn.inspection.PartialDependenceDisplay.from_estimator(model, X, list(range(len(train_features_list))), feature_names=train_features_list, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f3f8c",
   "metadata": {},
   "source": [
    "### Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f89448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_map(center, zoom=20, height='400px', width='400px'):\n",
    "    m = Map(basemap=basemap_to_tiles(basemaps.OpenStreetMap.Mapnik), center=center, zoom=zoom)\n",
    "    m.layout.height, m.layout.width = height, width\n",
    "    return m\n",
    "\n",
    "def visualize_trip(df, idx):\n",
    "    request_gps_lat, request_gps_lng = df.iloc[idx]['request_gps_lat'], df.iloc[idx]['request_gps_lng']\n",
    "    m = get_base_map((request_gps_lat, request_gps_lng))\n",
    "    \n",
    "    m.add_layer(Marker(location=(request_gps_lat, request_gps_lng), icon=AwesomeIcon(name='male', marker_color='red'), draggable=False, opacity=1))\n",
    "    m.add_layer(Marker(location=(df.iloc[idx]['pickup_anchor_lat'], df.iloc[idx]['pickup_anchor_lng']), icon=AwesomeIcon(name='anchor', marker_color='darkblue'), draggable=False, opacity=1.0))\n",
    "    m.add_layer(Marker(location=(df.iloc[idx]['first_lat'], df.iloc[idx]['first_lng']), icon=AwesomeIcon(name='thumbs-down', marker_color='lightgray'), draggable=False, opacity=1))\n",
    "    m.add_layer(Marker(location=(df.iloc[idx]['pickup_lat'], df.iloc[idx]['pickup_lng']), icon=AwesomeIcon(name='thumbs-up', marker_color='black'), draggable=False, opacity=1))\n",
    "    \n",
    "    display(m)\n",
    "\n",
    "    trip_uuid = df.iloc[idx]['trip_uuid']\n",
    "    examples = data[data['trip_uuid'] == trip_uuid]\n",
    "    print('Unpreferred Access Point')\n",
    "    display(show_prediction(model, examples[examples['y'] == 0][train_features_list].to_numpy().reshape((-1,)), show_feature_values=True, feature_names=train_features_list))\n",
    "    print('Preferred Acces Point')\n",
    "    display(show_prediction(model, examples[examples['y'] == 1][train_features_list].to_numpy().reshape((-1,)), show_feature_values=True, feature_names=train_features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba708d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_trip(df, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a270997",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(htmldoc, strip_tags = ['html','meta','head','body'], outfile=None, verbose=False):    \n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(htmldoc)\n",
    "    \n",
    "    for tag in strip_tags:\n",
    "        rmtag = soup.find(tag)\n",
    "        if rmtag is not None:\n",
    "            rmtag.unwrap()\n",
    "            if verbose: print(tag,'tags removed')\n",
    "    \n",
    "    stripped = soup.prettify()\n",
    "    if outfile is not None:\n",
    "        with open(outfile, 'w', encoding='utf-8') as f:\n",
    "            f.write(stripped)\n",
    "        if verbose: \n",
    "            print(f'file saved to: {outfile}')\n",
    "    else:\n",
    "        return stripped\n",
    "\n",
    "limeparams = dict(\n",
    "    training_data = X, \n",
    "    training_labels = y, \n",
    "    feature_names = train_features_list, \n",
    "    class_names = ['unpreferred','preferred']\n",
    ")\n",
    "lte = lime_tabular.LimeTabularExplainer(**limeparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b85ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_uuid = '983c803d-9db2-4cc3-86fe-caedf57d617f'\n",
    "examples = data[data['trip_uuid'] == trip_uuid]  \n",
    "lte_expl = lte.explain_instance(examples[examples['y'] == 0][train_features_list].to_numpy().reshape((-1,)), model.predict_proba)\n",
    "display_html(strip_html(lte_expl.as_html()), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3474b",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f98169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap_xgbc = shap.TreeExplainer(model)\n",
    "shapvals_xgbc = shap_xgbc.shap_values(X, y)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(shap_xgbc.expected_value, shapvals_xgbc[0], features=X[0], link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b367094",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shapvals_xgbc, X, feature_names=train_features_list, class_names=['unpreferred','preferred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4854af",
   "metadata": {},
   "outputs": [],
   "source": [
    "siv_xgbc = shap_xgbc.shap_interaction_values(X)\n",
    "shap.summary_plot(siv_xgbc, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a1de",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "#### Manifold Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X, y = data[train_features_list], data['y'].to_numpy()\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X[:2500])\n",
    "\n",
    "label = pd.DataFrame(['preferred' if label == 1 else 'unpreferred' for label in y[:2500]])\n",
    "sns_data = pd.concat([pd.DataFrame(X_embedded), label], axis=1)\n",
    "sns_data.columns = ['x', 'y', 'Label']\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set(rc = {'figure.figsize':(10,10)})\n",
    "\n",
    "sns.scatterplot(data=sns_data, x=\"x\", y=\"y\", hue=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285eb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = train_features_list\n",
    "unpreferred = df[[f'{unpreferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "preferred = df[[f'{preferred_colname}_{feature}' for feature in feature_list]].to_numpy()\n",
    "X, y = preferred - unpreferred, np.ones(preferred.shape[0])\n",
    "X[::2] *= -1; y[::2] *= -1\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X[:2500])\n",
    "\n",
    "label = pd.DataFrame(['1' if label == 1 else '-1' for label in y[:2500]])\n",
    "sns_data = pd.concat([pd.DataFrame(X_embedded), label], axis=1)\n",
    "sns_data.columns = ['x', 'y', 'Label']\n",
    "\n",
    "sns.scatterplot(data=sns_data, x=\"x\", y=\"y\", hue=\"Label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
