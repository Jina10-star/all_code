{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook illustrates the variance reduction concept with trip GB as the statistic\n",
    "\n",
    "# Experiments are typically evaluated on finite samples, the results are subject to sampling error (or generalization error). \n",
    "# Standard error tells you how accurate is the statistic estimated from the sample compared to the full population\n",
    "# When the population is not homogeneous, random sampling approach can often have limited representativeness and samples can be poorly balanced across the population strata.\n",
    "# Stratified sampling is a well known technique to reduce the standard error (compared to random sampling approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show reduction in std error for 'trip GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.070192107672136"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ground truth using full sample\n",
    "data = pd.read_csv(\"data/gb_by_city_id_feb_1_feb_7_2022.csv\")\n",
    "mu = data.gross_bookings_usd.sum()/data.num_rows.sum()\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "n_trials = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9686478331678763"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimation standard error with random sampling\n",
    "cities = data.city_id.tolist()\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = set(np.random.choice(cities, sample_size))\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.gross_bookings_usd.sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "std_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3435206920933553"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate standard error with stratified sampling\n",
    "\n",
    "# load cluster data\n",
    "region = \"global\"\n",
    "clusters = joblib.load(\"clusters/{}.pkl\".format(region))\n",
    "total_cluster_gb = sum([c['cluster_gb'] for c in clusters])\n",
    "\n",
    "# sample_size= 10\n",
    "def MIN_CITIES_PER_CLUSTER(gb):\n",
    "    return 1\n",
    "#     return 1 if gb > 5 else 1\n",
    "\n",
    "\n",
    "sample_size_by_cluster = [int(10*c['cluster_gb']/total_cluster_gb) + MIN_CITIES_PER_CLUSTER(c['cluster_gb']) for c in clusters]\n",
    "print(sample_size_by_cluster, sum(sample_size_by_cluster))\n",
    "\n",
    "def stratified_sample():\n",
    "    sampled_cities = set()\n",
    "    for i, c in enumerate(clusters):\n",
    "        cities = [city['data.city_id'] for city in c['cities']]\n",
    "        sampled_cities = sampled_cities | set(np.random.choice(cities, sample_size_by_cluster[i]))\n",
    "    return sampled_cities\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = stratified_sample()\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.gross_bookings_usd.sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "std_error    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show reduction in std error for 'driver cancel rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7422799907176918"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ground truth using full sample\n",
    "data = pd.read_csv(\"data/driver_cancel_rate_by_city_id_march_1_2022.csv\")\n",
    "mu = data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/data.num_rows.sum()\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "n_trials = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12838041932606514"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimation standard error with random sampling\n",
    "cities = data.city_id.tolist()\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = set(np.random.choice(cities, sample_size))\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "std_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07409033163124304"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate standard error with stratified sampling\n",
    "\n",
    "# load cluster data\n",
    "region = \"global\"\n",
    "clusters = joblib.load(\"clusters/{}.pkl\".format(region))\n",
    "total_cluster_gb = sum([c['cluster_gb'] for c in clusters])\n",
    "\n",
    "# sample_size= 10\n",
    "def MIN_CITIES_PER_CLUSTER(gb):\n",
    "    return 1\n",
    "#     return 1 if gb > 5 else 1\n",
    "\n",
    "sample_size_by_cluster = [int(10*c['cluster_gb']/total_cluster_gb) + MIN_CITIES_PER_CLUSTER(c['cluster_gb']) for c in clusters]\n",
    "print(sample_size_by_cluster, sum(sample_size_by_cluster))\n",
    "\n",
    "def stratified_sample():\n",
    "    sampled_cities = set()\n",
    "    for i, c in enumerate(clusters):\n",
    "        cities = [city['data.city_id'] for city in c['cities']]\n",
    "        sampled_cities = sampled_cities | set(np.random.choice(cities, sample_size_by_cluster[i]))\n",
    "    return sampled_cities\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = stratified_sample()\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "std_error    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results by mega-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_region_to_city_id = pd.read_csv(\"data/city_level_metrics_1_23_2022_to_1_29_2022_w_metadata_excluding_outliers_add_lat_lng.csv\") #https://michelangelo-studio.uberinternal.com/file/d55c6275-b239-4d8b-81d5-ac2962a70d3f\n",
    "mega_region_to_city_id['city.mega_region'] = mega_region_to_city_id['city.mega_region'].map({'APAC' : 'apac', 'EMEA' : 'emea', 'LatAm' : 'latam', 'US & Canada' : 'usc'})\n",
    "mega_region_to_city_id = mega_region_to_city_id.groupby('city.mega_region')['data.city_id'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"usc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trip GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.614406572907649\n",
      "1.2362409345080334\n",
      "[9, 2, 2, 1, 1, 1, 1, 1, 1, 1] 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9561136122460295"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ground truth using full sample\n",
    "data = pd.read_csv(\"data/gb_by_city_id_jan_23_jan_29_2022.csv\")\n",
    "data = data[data.city_id.isin(mega_region_to_city_id[region])]\n",
    "mu = data.gross_bookings_usd.sum()/data.num_rows.sum()\n",
    "print(mu)\n",
    "# estimation standard error with random sampling\n",
    "cities = data.city_id.tolist()\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = set(np.random.choice(cities, sample_size))\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.gross_bookings_usd.sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "print(std_error)\n",
    "\n",
    "# estimate standard error with stratified sampling\n",
    "\n",
    "# load cluster data\n",
    "clusters = joblib.load(\"clusters/{}.pkl\".format(region))\n",
    "total_cluster_gb = sum([c['cluster_gb'] for c in clusters])\n",
    "\n",
    "# sample_size= 10\n",
    "def MIN_CITIES_PER_CLUSTER(gb):\n",
    "    return 1\n",
    "#     return 1 if gb > 5 else 1\n",
    "\n",
    "\n",
    "target = {'apac': 15, 'emea': 14, 'latam': 15, 'usc': 14}\n",
    "sample_size_by_cluster = [int(target[region]*c['cluster_gb']/total_cluster_gb) + MIN_CITIES_PER_CLUSTER(c['cluster_gb']) for c in clusters]\n",
    "print(sample_size_by_cluster, sum(sample_size_by_cluster))\n",
    "\n",
    "def stratified_sample():\n",
    "    sampled_cities = set()\n",
    "    for i, c in enumerate(clusters):\n",
    "        cities = [city['data.city_id'] for city in c['cities']]\n",
    "        sampled_cities = sampled_cities | set(np.random.choice(cities, sample_size_by_cluster[i]))\n",
    "    return sampled_cities\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = stratified_sample()\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.gross_bookings_usd.sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "std_error    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## driver cancel rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7936261198368971\n",
      "0.055376341856574765\n",
      "0.01698764041156972\n"
     ]
    }
   ],
   "source": [
    "# calculate ground truth using full sample\n",
    "data = pd.read_csv(\"data/driver_cancel_rate_by_city_id_march_1_2022.csv\")\n",
    "data = data[data.city_id.isin(mega_region_to_city_id[region])]\n",
    "mu = data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/data.num_rows.sum()\n",
    "print(mu)\n",
    "\n",
    "# estimation standard error with random sampling\n",
    "cities = data.city_id.tolist()\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = set(np.random.choice(cities, sample_size))\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "print(std_error)\n",
    "\n",
    "clusters = joblib.load(\"clusters/{}.pkl\".format(region))\n",
    "total_cluster_gb = sum([c['cluster_gb'] for c in clusters])\n",
    "\n",
    "# sample_size= 10\n",
    "def MIN_CITIES_PER_CLUSTER(gb):\n",
    "    return 1\n",
    "#     return 1 if gb > 5 else 1\n",
    "\n",
    "target = {'apac': 15, 'emea': 14, 'latam': 15, 'usc': 14}\n",
    "sample_size_by_cluster = [int(target[region]*c['cluster_gb']/total_cluster_gb) + MIN_CITIES_PER_CLUSTER(c['cluster_gb']) for c in clusters]\n",
    "\n",
    "\n",
    "def stratified_sample():\n",
    "    sampled_cities = set()\n",
    "    for i, c in enumerate(clusters):\n",
    "        cities = [city['data.city_id'] for city in c['cities']]\n",
    "        sampled_cities = sampled_cities | set(np.random.choice(cities, sample_size_by_cluster[i]))\n",
    "    return sampled_cities\n",
    "\n",
    "mu_estimates = []\n",
    "for trial in range(n_trials):\n",
    "    sampled_cities = stratified_sample()\n",
    "    sample_data = data[data.city_id.isin(sampled_cities)]\n",
    "    mu_estimate = sample_data.apply(lambda r: r['driver_cancel_rate']*r['num_rows'], axis=1).sum()/sample_data.num_rows.sum()\n",
    "    mu_estimates.append(mu_estimate)\n",
    "    \n",
    "std_error = np.sqrt(sum([(mu_estimate-mu)**2 for mu_estimate in mu_estimates])/n_trials)\n",
    "print(std_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
