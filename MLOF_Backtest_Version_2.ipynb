{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: dataclasses in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.8)\n",
      "Looking in indexes: https://yoober11:****@pypi.uberinternal.com/index\n",
      "Requirement already satisfied: matching-ds-tools in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (0.7.7)\n",
      "Requirement already satisfied: protobuf==3.12.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (3.12.2)\n",
      "Requirement already satisfied: requests==2.22.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (2.22.0)\n",
      "Requirement already satisfied: pytz>2021 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (2021.3)\n",
      "Requirement already satisfied: queryrunner-client>3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (3.4.1)\n",
      "Requirement already satisfied: h3==3.6.4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (3.6.4)\n",
      "Requirement already satisfied: pandas>1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (1.1.5)\n",
      "Requirement already satisfied: ujson==1.35 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (1.35)\n",
      "Requirement already satisfied: haversine==2.1.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from matching-ds-tools) (2.1.2)\n",
      "Requirement already satisfied: setuptools in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from protobuf==3.12.2->matching-ds-tools) (59.6.0)\n",
      "Requirement already satisfied: six>=1.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from protobuf==3.12.2->matching-ds-tools) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from requests==2.22.0->matching-ds-tools) (1.25.11)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas>1->matching-ds-tools) (1.16.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from pandas>1->matching-ds-tools) (2.8.1)\n",
      "Requirement already satisfied: wonkapy>=3 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (3.0.2)\n",
      "Requirement already satisfied: pysocks>=1.5.7 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (1.7.1)\n",
      "Requirement already satisfied: pyyaml in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (5.3.1)\n",
      "Requirement already satisfied: future==0.16.0 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (0.16.0)\n",
      "Requirement already satisfied: clay-config>=2 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (2.1.1)\n",
      "Requirement already satisfied: m3>=4 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (4.3.1)\n",
      "Requirement already satisfied: querybuilder-client==0.6.1 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (0.6.1)\n",
      "Requirement already satisfied: hive-csv-reader==0.1.9 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from queryrunner-client>3->matching-ds-tools) (0.1.9)\n",
      "Requirement already satisfied: clay-config-file in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from clay-config>=2->queryrunner-client>3->matching-ds-tools) (1.2.1)\n",
      "Requirement already satisfied: tornado<6 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=4->queryrunner-client>3->matching-ds-tools) (4.5.3)\n",
      "Requirement already satisfied: thriftrw>=1.8 in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from m3>=4->queryrunner-client>3->matching-ds-tools) (1.8.1)\n",
      "Requirement already satisfied: ply in /dsw/snapshots/snapshot_dsw_default_jupyter/python3/lib/python3.6/site-packages (from thriftrw>=1.8->m3>=4->queryrunner-client>3->matching-ds-tools) (3.11)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source $VIRTUAL_ENV_DIR/python3/bin/activate\n",
    " \n",
    "# Install latest mxpkg version (to specify version, use syntax: pip install mxpkg==1.1.7)\n",
    "pip install dataclasses\n",
    "pip install matching-ds-tools\n",
    " \n",
    "deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from queryrunner_client import Client\n",
    "qclient = Client(user_email='thai@uber.com')\n",
    "USER_EMAIL = 'thai@uber.com'\n",
    "CONSUMER_NAME = 'intelligentdispatch'\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "#num_cores = multiprocessing.cpu_count()  -- 48\n",
    "n_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from queryrunner_client import Client as QRClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdstk\n",
    "from mdstk.data_fetcher.data_fetcher import DataFetcher\n",
    "from mdstk.data_fetcher.cached_data_fetcher import CachedDataFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "QUERY = \"\"\"\n",
    "with dispatch as (\n",
    "select \n",
    "    datestr,\n",
    "    msg.cityid,\n",
    "    msg.ctplangenrequestuuid as plangen_uuid,\n",
    "    msg.ctrequestuuid as scan_uuid,\n",
    "    j as job_uuid,\n",
    "    msg.supplyuuid,\n",
    "    msg.planactiontype\n",
    "from \n",
    "    rawdata_user.kafka_hp_multileg_dispatched_plan_nodedup\n",
    "cross join \n",
    "    unnest(msg.jobuuid) jobs(j)\n",
    "where \n",
    "    datestr = '{datestr}'\n",
    "    and msg.cityid = {city_id}\n",
    "    and msg.vehicleviewid = {vvid} \n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and CARDINALITY(msg.jobuuid) > 0\n",
    "    and substr(msg.ctrequestuuid, 1, length('{digits}')) = '{digits}'\n",
    "),\n",
    "plangen as (\n",
    "select \n",
    "    msg.scanuuid as plangen_uuid, \n",
    "    p.uuid as job_uuid,\n",
    "    j.supplyuuid\n",
    "from \n",
    "    rawdata_user.kafka_hp_multileg_matching_observability_proposals_v2_nodedup\n",
    "cross join \n",
    "    unnest(msg.proposals) as job(j)\n",
    "cross join \n",
    "    unnest(j.jobs) as plan(p)\n",
    "where \n",
    "    datestr = '{datestr}'\n",
    "    and msg.cityid = {city_id}\n",
    "    and msg.flowtype = 'solo_batch'\n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and j.status = 'eligible'\n",
    "),\n",
    "mgv as (\n",
    "select datestr,\n",
    "       msg.city_id,\n",
    "       msg.job_uuid,\n",
    "       msg.client_uuid,\n",
    "       msg.ct_request_uuid as plangen_uuid,\n",
    "       msg.supply_uuid,\n",
    "       msg.supply_plan_uuid as plan_uuid,\n",
    "       msg.unadjusted_eta as eta,\n",
    "       (CASE\n",
    "          WHEN msg.adjustedeta > 1500 THEN 1500.0\n",
    "          WHEN msg.adjustedeta < 0 THEN 0.0\n",
    "          ELSE msg.adjustedeta\n",
    "       END) as adjustedeta,\n",
    "       round(msg.job_surge, 4) as surge_mul,\n",
    "       round(msg.eventual_completion_probability, 4) as eventual_comp_prob,\n",
    "       msg.ranking_metric,\n",
    "       round(1 - msg.solo_cancel_model_driver_accept_prob, 4) as d_proba,\n",
    "       round(1 - msg.solo_cancel_model_rider_accept_prob, 4) as r_proba,\n",
    "       round(1 - msg.spinner_survive_prob_before_next_scan, 4) as s_proba,\n",
    "       msg.preferred_destination_adjustment,\n",
    "       msg.objective_value as of_value,\n",
    "       msg.inconvenience_etd - msg.ranking_metric as trip_length\n",
    "from   \n",
    "    rawdata.kafka_hp_multileg_mgv_log_nodedup\n",
    "where  \n",
    "    datestr = '{datestr}'\n",
    "    and msg.city_id = {city_id}\n",
    "    and msg.tenancy = 'uber/production'\n",
    "    and msg.vehicle_view_id = {vvid} \n",
    "    and msg.flow_type = 'solo_batch'\n",
    "    and msg.job_uuid <> msg.client_uuid\n",
    "    and msg.calculator_type = 'markov_eta_v2'\n",
    "),\n",
    "test as (\n",
    "select \n",
    "    mgv.datestr,\n",
    "    mgv.city_id,\n",
    "    dispatch.scan_uuid,\n",
    "    mgv.plangen_uuid,\n",
    "    mgv.job_uuid,\n",
    "    dispatch.planactiontype,\n",
    "    mgv.supply_uuid,\n",
    "    case when dispatch.supplyuuid = mgv.supply_uuid then 1 else 0 end as is_selected,\n",
    "    mgv.eta,\n",
    "    mgv.adjustedeta,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1), 4) as eta_one,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 1.25), 4) as eta_one_quarter,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 2), 4) as eta_square,\n",
    "    round(POWER(1 - mgv.adjustedeta / 1500.0, 3), 4) as eta_cube,\n",
    "    mgv.surge_mul,\n",
    "    mgv.eventual_comp_prob,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 2)), 4) as network_contention_2,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 3)), 4) as network_contention_3,\n",
    "    round(1.0 / (1.0 + POWER(mgv.surge_mul, 5)), 4) as network_contention_5,\n",
    "    mgv.ranking_metric,\n",
    "    mgv.d_proba,\n",
    "    mgv.r_proba,\n",
    "    mgv.s_proba,\n",
    "    round((1.0 - mgv.d_proba) * (1.0 - mgv.r_proba) * (1.0 - mgv.s_proba) + mgv.eventual_comp_prob * mgv.d_proba, 4) as cr_ratio,\n",
    "    mgv.preferred_destination_adjustment,\n",
    "    mgv.of_value,\n",
    "    mgv.trip_length,\n",
    "    fare.est_rider_quoted_final_fare as fare\n",
    "from\n",
    "    mgv\n",
    "join\n",
    "    plangen\n",
    "on \n",
    "    mgv.plangen_uuid = plangen.plangen_uuid\n",
    "    and mgv.job_uuid = plangen.job_uuid\n",
    "    and mgv.supply_uuid = plangen.supplyuuid\n",
    "join\n",
    "    dispatch\n",
    "on\n",
    "    mgv.plangen_uuid = dispatch.plangen_uuid\n",
    "    and mgv.job_uuid = dispatch.job_uuid\n",
    "join\n",
    "    dwh.fact_trip_fare fare \n",
    "on\n",
    "    mgv.job_uuid = fare.trip_uuid\n",
    "    and fare.datestr = '{datestr}'\n",
    "    and fare.city_id = {city_id}\n",
    ")\n",
    "select * from test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Query:\n",
    "    prefix: str\n",
    "    hex_digits: str\n",
    "    city_id: int\n",
    "    vvid: str\n",
    "    datestr: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.name = f'{self.prefix}_city{self.city_id}_{self.vvid}_{self.datestr}_segment{self.hex_digits}'\n",
    "        self.qry = QUERY.format(city_id=self.city_id, vvid=self.vvid, digits=self.hex_digits, datestr=self.datestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataFetcher(DataFetcher):\n",
    "    def query_many_presto(self, *args, **kwargs):\n",
    "        return super().query_many_presto(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new objective function\n",
    "def clean_df(df):\n",
    "    df = df[df['fare'].notnull()]\n",
    "    df['trip_length'][df['trip_length'] <= 100] = 100\n",
    "    df = df.drop_duplicates(subset=['job_uuid', 'supply_uuid'])\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def compute_new_of(df):\n",
    "#     df['new_of'] = - df['eta_square'] * df['cr_ratio'] * df['fare']\n",
    "    df['new_of'] = - (- 0.3268 * df['cr_ratio'] \\\n",
    "                      - 0.6322 * df['eta_one'] * df['cr_ratio'] \\\n",
    "                      + 0.3956 * df['eta_one'] * (1 - df['network_contention_2']) * df['cr_ratio'] * df['fare']\n",
    "                     )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local solver\n",
    "def solve_dict(\n",
    "    scan: dict, \n",
    "    cost_col: str, \n",
    "    job_singleton: float = 1500,\n",
    "    infinity: float = 1000000\n",
    "):\n",
    "    job_list = list(set([k[0] for k in scan.keys()]))\n",
    "    job_idx = {j: i for i, j in enumerate(job_list)}\n",
    "    job_count = len(job_list)\n",
    "\n",
    "    supply_list = list(set([k[1] for k in scan.keys()]))\n",
    "    supply_idx = {s: i for i, s in enumerate(supply_list)}\n",
    "    supply_count = len(supply_list)\n",
    "    \n",
    "    utility = np.full((len(job_list), len(supply_list) + len(job_list)), infinity, dtype=np.float32)\n",
    "    for k in scan.keys():\n",
    "        jidx = job_idx[k[0]]\n",
    "        sidx = supply_idx[k[1]]\n",
    "        utility[jidx, sidx] = scan[k][cost_col]\n",
    "    for i in range(len(job_list)):\n",
    "        utility[i, supply_count + i] = job_singleton\n",
    "            \n",
    "    # solve\n",
    "    job_sol, supply_sol = linear_sum_assignment(utility)\n",
    "\n",
    "    result = set()\n",
    "    for jidx, sidx in zip(job_sol, supply_sol):\n",
    "        j = job_list[jidx]\n",
    "        if sidx >= supply_count:\n",
    "            result.add((j,))\n",
    "        else:\n",
    "            s = supply_list[sidx]\n",
    "            result.add((j, s))\n",
    "            \n",
    "    assert len(result) == len(job_list)\n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScanMetrics:\n",
    "    total_jobs: int = 0.\n",
    "    total_eta: float = 0.\n",
    "    total_offer: float = 0.\n",
    "    total_ar: float = 0.\n",
    "    total_rc: float = 0.\n",
    "    total_trip: float = 0.\n",
    "    total_gb: float = 0.\n",
    "    total_overwrite: int = 0.\n",
    "    \n",
    "    def __add__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        return ScanMetrics(\n",
    "            self.total_jobs + o.total_jobs,\n",
    "            self.total_eta + o.total_eta,\n",
    "            self.total_offer + o.total_offer,\n",
    "            self.total_ar + o.total_ar,\n",
    "            self.total_rc + o.total_rc,\n",
    "            self.total_trip + o.total_trip,\n",
    "            self.total_overwrite + o.total_overwrite,\n",
    "            self.total_gb + o.total_gb\n",
    "        )\n",
    "    def __iadd__(self, o: 'ScanMetrics') -> 'ScanMetrics':\n",
    "        self.total_jobs += o.total_jobs\n",
    "        self.total_eta += o.total_eta\n",
    "        self.total_offer += o.total_offer\n",
    "        self.total_ar += o.total_ar\n",
    "        self.total_rc += o.total_rc\n",
    "        self.total_trip += o.total_trip\n",
    "        self.total_overwrite += o.total_overwrite\n",
    "        self.total_gb += o.total_gb\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Summary\n",
    "def metric_summary_dict(\n",
    "    scan_dict: Dict[str, Dict[str, Any]],\n",
    "    matching: set, \n",
    "    overwrite: int,\n",
    ") -> ScanMetrics:\n",
    "    sm = ScanMetrics()\n",
    "    sm.total_jobs = len(matching)\n",
    "    sm.total_overwrite = overwrite\n",
    "    \n",
    "    for m in matching:\n",
    "        if len(m) == 2:\n",
    "            row = scan_dict[(m[0], m[1])]\n",
    "            sm.total_offer += 1\n",
    "            sm.total_eta += row['eta']\n",
    "            sm.total_ar += 1 - row['d_proba']\n",
    "            sm.total_rc += row['r_proba']\n",
    "            if row['trip_length'] < 7200:\n",
    "                sm.total_trip += row['trip_length']\n",
    "            if row['fare'] > 0:\n",
    "                sm.total_gb += (1 - row['d_proba']) * (1 - row['r_proba']) * row['fare']\n",
    "    return sm\n",
    "\n",
    "def solve_all_dict(df, solver: Callable[[dict], set]):\n",
    "    total_scans = dict(tuple(df.groupby('scan_uuid')))\n",
    "\n",
    "    sm = ScanMetrics()\n",
    "    for scan_uuid, scan_df in total_scans.items():\n",
    "        scan = (scan_df.set_index(['job_uuid', 'supply_uuid']).to_dict(orient='index'))\n",
    "        matching, overwrite = solver(scan)\n",
    "        sm += metric_summary_dict(scan, matching, overwrite)\n",
    "        \n",
    "    return {'total_jobs': sm.total_jobs,\n",
    "            'match_rate': sm.total_offer * 1.0 / sm.total_jobs,\n",
    "            'overwrite': sm.total_overwrite * 1.0 / sm.total_jobs, # different decisions compared to Markov\n",
    "            'Average Matched ETA': sm.total_eta * 1.0 / sm.total_offer,\n",
    "            'Driver AR': sm.total_ar * 1.0 / sm.total_offer,\n",
    "            'Rider cancel': sm.total_rc * 1.0 / sm.total_offer,\n",
    "            'Average trip length': sm.total_trip * 1.0 / sm.total_offer,\n",
    "            'Average GB': sm.total_gb * 1.0 / sm.total_offer\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_matching_decision(m1,m2):\n",
    "    return m1.difference(m2), m2.difference(m1)\n",
    "\n",
    "def supply_cost_solve_dict(scan, markov = False, secondary_singleton = 0):\n",
    "    #Markov solve\n",
    "    primary_matching = solve_dict(scan, 'of_value', job_singleton = 1500)\n",
    "    if markov:      \n",
    "        return primary_matching, 0\n",
    "    #SCA solve\n",
    "    secondary_matching = solve_dict(scan, 'new_of', job_singleton = secondary_singleton)\n",
    "    different_matches = len(different_matching_decision(primary_matching, secondary_matching)[0])\n",
    "    return secondary_matching, different_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28/28 dataframes from cache!\n"
     ]
    }
   ],
   "source": [
    "prefix = 'replay'\n",
    "hex_digits = '36'\n",
    "city_id_vvids = {1269: '(10004148)', 789: '(11279)'}\n",
    "\n",
    "datestrs = [  # 2 weeks\n",
    "    '2022-08-09',\n",
    "    '2022-08-10',\n",
    "    '2022-08-11',\n",
    "    '2022-08-12',\n",
    "    '2022-08-13',\n",
    "    '2022-08-14',\n",
    "    '2022-08-15',\n",
    "    '2022-08-16',\n",
    "    '2022-08-17',\n",
    "    '2022-08-18',\n",
    "    '2022-08-19',\n",
    "    '2022-08-20',\n",
    "    '2022-08-21',\n",
    "    '2022-08-22',\n",
    "]\n",
    "\n",
    "queries = [\n",
    "    Query(prefix=prefix, hex_digits=hex_digits, city_id=city_id, vvid=vvid, datestr=datestr)\n",
    "    for (city_id, vvid), datestr in itertools.product(city_id_vvids.items(), datestrs)\n",
    "]\n",
    "\n",
    "cache_qry_map = {\n",
    "    q.name: q.qry \n",
    "    for q in queries\n",
    "}\n",
    "\n",
    "cdf = CachedDataFetcher(\n",
    "    data_fetcher=MyDataFetcher(\n",
    "        user_email=USER_EMAIL,\n",
    "        consumer_name=CONSUMER_NAME,\n",
    "    ),\n",
    "    cache_qry_map=cache_qry_map,\n",
    "    datacenter='dca1',\n",
    "    datasource='presto-secure',\n",
    ")\n",
    "\n",
    "cdf.fetch(bust_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 s, sys: 827 ms, total: 3.83 s\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# clean data\n",
    "scans = pd.concat(cdf.dfs.values(), axis=0, ignore_index=True) \n",
    "df = scans\n",
    "df = clean_df(df)\n",
    "df = compute_new_of(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jaeger_tracing:Tracing sampler started with sampling refresh interval 60 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.1 s, sys: 331 ms, total: 32.4 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# SCA solve\n",
    "sca_matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Markov solve\n",
    "# markov_matching = solve_all_dict(df,lambda scan: supply_cost_solve_dict(scan, markov = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_jobs': 59067.0,\n",
       " 'match_rate': 0.9887754583777744,\n",
       " 'overwrite': 0.4162053261550443,\n",
       " 'Average Matched ETA': 335.22525854393535,\n",
       " 'Driver AR': 0.33310006677624865,\n",
       " 'Rider cancel': 0.10168994075748239,\n",
       " 'Average trip length': 870.3670981439627,\n",
       " 'Average GB': 5.424491636593753}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sca_matching\n",
    "\n",
    "# EFOF single OF\n",
    "# {'total_jobs': 59067.0,\n",
    "#  'match_rate': 0.9922968832004334,\n",
    "#  'overwrite': 0.3599302487006281,\n",
    "#  'Average Matched ETA': 325.8672285538797,\n",
    "#  'Driver AR': 0.30801089196751497,\n",
    "#  'Rider cancel': 0.101107730498874,\n",
    "#  'Average trip length': 868.1338292499829,\n",
    "#  'Average GB': 4.953158812096452}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# markov_matching\n",
    "\n",
    "# {'total_jobs': 59067.0,\n",
    "#  'match_rate': 0.9908070496216161,\n",
    "#  'overwrite': 0.0,\n",
    "#  'Average Matched ETA': 308.2193117353564,\n",
    "#  'Driver AR': 0.27074625281935616,\n",
    "#  'Rider cancel': 0.09404085845123371,\n",
    "#  'Average trip length': 866.5610689631604,\n",
    "#  'Average GB': 4.402908360236581}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (General DS)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
