{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "\n",
    "%%configure -f\n",
    "{\n",
    "  \"pyFiles\": [], \n",
    "  \"kind\": \"pyspark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\",\n",
    "  \"queue\": \"maps_trueta\",\n",
    "  \"numExecutors\": 800,\n",
    "  \"driverMemory\": \"12g\",\n",
    "  \"executorMemory\": \"12g\",\n",
    "  \"driverCores\": 4,\n",
    "  \"executorCores\": 1, \n",
    "  \"jars\": [], \n",
    "  \"conf\": {\n",
    "    \"spark.executor.memoryOverhead\": \"4g\",\n",
    "    \"spark.driver.memoryOverhead\": \"4g\",\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"hive.exec.dynamic.partition\": \"true\",\n",
    "    \"hive.exec.dynamic.partition.mode\": \"nonstrict\",\n",
    "    \"spark.locality.wait\": \"6s\",\n",
    "    \"spark.maxRemoteBlockSizeFetchToMem\": \"200m\",\n",
    "    \"spark.network.timeout\": \"2400s\",\n",
    "    \"spark.executor.heartbeatInterval\": \"120s\",\n",
    "    \"spark.yarn.scheduler.heartbeat.interval-ms\": 120000,\n",
    "    \"spark.driver.extraJavaOptions\": \"-XX:+UseCompressedOops -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\",\n",
    "    \"spark.executor.extraJavaOptions\": \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=6 -XX:+UseCompressedOops -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\":-1\n",
    "    }, \n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"DCA1/NonSecure\"  \n",
    "  }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the current prod model for TOMTOM\n",
    "tt_model = spark.read.parquet(\"hdfs:///user/dhruven.vora/osm_bike_1031_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema for the model read\n",
    "tt_model.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print element count for the model read\n",
    "tt_model.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the segments from rawdata_user.kafka_hp_maps_historical_streaks_tomtom_nodedup table\n",
    "\n",
    "tt_segments = spark.sql(\"\"\"\n",
    "select \n",
    "    msg.graphsegment.segmentuuid as segmentid, msg.cityid as cityid\n",
    "from \n",
    "    rawdata_user.kafka_hp_maps_historical_streaks_osm_nodedup\n",
    "where\n",
    "    datestr between '2021-12-16' and '2022-01-02'\n",
    "    and msg.classification = 'valid'\n",
    "    and msg.vehicletype = 'BICYCLE'\n",
    "    and msg.lengthmeters > 0\n",
    "    and msg.speedkmph > 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display segments schema\n",
    "tt_segments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display segments count\n",
    "tt_segments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out what % of segments are present in the model.\n",
    "# compute (Segments in kafka - segments in model) * 100 / Segments in kafka\n",
    "joined_tt = tt_segments.join(tt_model, tt_model.segmentuuid == tt_segments.segmentid, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column in tomtom model which sets 1 if segment id is present\n",
    "result_tt = joined_tt.withColumn('prod_model', when(joined_tt.segmentuuid.isNull(), 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema \n",
    "result_tt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total streaks by segments group by city ids\n",
    "final_result_tt = result_tt \\\n",
    "    .groupBy('cityid') \\\n",
    "    .agg(F.count('prod_model').alias('total_streaks'), F.sum('prod_model').alias('covered_streaks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print schema \n",
    "final_result_tt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show snippet of final aggregated result\n",
    "final_result_tt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for coverage percentage\n",
    "final_result_tt = final_result_tt.withColumn('% covered', col('covered_streaks') / col('total_streaks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 1000 values\n",
    "final_result_tt.orderBy('cityid').show(1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkMagic (Remote Python2)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
