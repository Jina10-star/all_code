{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Thanks for using Drogon for your interactive Spark application. We update Drogon/SparkMagic as often as possible to make it easier, faster and more reliable for you. Have a question or feedback? Ping us on [uChat](https://uchat.uberinternal.com/uber/channels/spark).</span>\n",
    "\n",
    "What's New\n",
    "- Now you can use `%%configure` and `%%spark` magics to configure and start a Spark session (deprecating hard-to-use `%load_ext sparkmagic.magics` and `manage_spark` magics). Check out [this example](https://workbench.uberinternal.com/explore/knowledge/localfile/cwang/sparkmagic_python2_example.ipynb) for more details.\n",
    "- Improved `%%configure` magic. You now can use it to make all Spark and Drogon configurations from within notebook itself. Check out our [latest documentation & examples](https://docs.google.com/document/d/1mkYtDHquh4FjqTeA0Fxii8lyV-P6qzmoABhmmRwm_00/edit#heading=h.xn14pmoorsn0) for more details.\n",
    "- Bug fixes and performance updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application (can take 60s or more)...\n",
      "Starting heartbeat thread...done.\n",
      "Waiting for Drogon session to be ready......................................................\n",
      "Drogon session is ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Drogon Session ID</th><th>Spark Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>569853823</td><td>application_1674066407532_1916582</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://hadoop-ui.uberinternal.com/gateway/prodsec_phx2/yarn/nodemanager/node/containerlogs/container_e249_1674066407532_1916582_01_000002/radhesh?scheme=http&host=phx3-k5h.prod.uber.internal&port=8042\">Link</a></td><td><a target=\"_blank\" href=\"https://shs-phx2.uberinternal.com/proxy/application_1674066407532_1916582\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "\n",
      "\n",
      "Cell execution took 109 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs:\n",
       "<pre>{\n",
       "  \"pyFiles\": [], \n",
       "  \"kind\": \"spark\", \n",
       "  \"proxyUser\": \"radhesh\", \n",
       "  \"sparkEnv\": \"SPARK_24\", \n",
       "  \"driverMemory\": \"8g\", \n",
       "  \"queue\": \"uber_eats_ml\", \n",
       "  \"conf\": {\n",
       "    \"spark.dynamicAllocation.enabled\": \"true\", \n",
       "    \"spark.driver.memory\": \"12g\", \n",
       "    \"spark.dynamicAllocation.maxExecutors\": 200, \n",
       "    \"spark.driver.memoryOverhead\": \"4g\", \n",
       "    \"spark.executor.memoryOverhead\": \"4g\", \n",
       "    \"spark.hadoop.hadoop.security.authentication\": \"simple\", \n",
       "    \"spark.dynamicAllocation.minExecutors\": 100, \n",
       "    \"spark.executor.memory\": \"12g\", \n",
       "    \"spark.dynamicAllocation.initialExecutors\": 100, \n",
       "    \"spark.shuffle.service.enabled\": true, \n",
       "    \"spark.sql.shuffle.partitions\": 500\n",
       "  }, \n",
       "  \"executorCores\": 2, \n",
       "  \"driverCores\": 2, \n",
       "  \"jars\": [\n",
       "    \"/user/radhesh/spark-corenlp-0.4.0-spark2.4-scala2.11.jar\", \n",
       "    \"/user/radhesh/stanford-corenlp-3.9.1-models.jar\"\n",
       "  ], \n",
       "  \"executorMemory\": \"12g\", \n",
       "  \"drogonHeaders\": {\n",
       "    \"X-DROGON-CLUSTER\": \"phx2/secure\"\n",
       "  }\n",
       "}</pre><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"pyFiles\": [],\n",
    "  \"kind\": \"spark\",\n",
    "  \"proxyUser\": \"radhesh\",\n",
    "  \"sparkEnv\": \"SPARK_24\",\n",
    "  \"driverMemory\": \"8g\",\n",
    "  \"queue\": \"uber_eats_ml\",\n",
    "  \"conf\": { \n",
    "      \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "      \"spark.dynamicAllocation.initialExecutors\":100,\n",
    "      \"spark.dynamicAllocation.minExecutors\":100,\n",
    "      \"spark.dynamicAllocation.maxExecutors\" : 200,\n",
    "      \"spark.executor.memory\": \"12g\",\n",
    "      \"spark.executor.memoryOverhead\": \"4g\",\n",
    "      \"spark.driver.memory\": \"12g\",\n",
    "      \"spark.driver.memoryOverhead\" : \"4g\",\n",
    "      \"spark.hadoop.hadoop.security.authentication\": \"simple\",\n",
    "      \"spark.shuffle.service.enabled\" : true,\n",
    "      \"spark.sql.shuffle.partitions\" : 500\n",
    "},\n",
    "  \"executorCores\": 2,\n",
    "  \"driverCores\": 2,\n",
    "  \"executorMemory\": \"12g\",\n",
    "    \"jars\": [\"/user/radhesh/spark-corenlp-0.4.0-spark2.4-scala2.11.jar\",\n",
    "            \"/user/radhesh/stanford-corenlp-3.9.1-models.jar\"],\n",
    "   \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"phx2/secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@22f49911"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [item_name: string, semantic_tags: string]"
     ]
    }
   ],
   "source": [
    "var data = spark.sql(\"SELECT * FROM uber_eats.semantic_tags_grocery_data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Long = 2377850"
     ]
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [item_name: string, semantic_tags: string ... 1 more field]"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"tag\",explode(split(col(\"semantic_tags\"),\":\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           item_name|       semantic_tags|                 tag|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Nimbus Sauvignon ...|     wine:white wine|                wine|\n",
      "|Nimbus Sauvignon ...|     wine:white wine|          white wine|\n",
      "|Dempster's Hambur...|dairy deli egg br...|dairy deli egg bread|\n",
      "|Dempster's Hambur...|dairy deli egg br...|              bakery|\n",
      "|Dempster's Hambur...|dairy deli egg br...|           bread bun|\n",
      "|Dempster's Hambur...|dairy deli egg br...|               bread|\n",
      "|Dempster's Hambur...|dairy deli egg br...|      milk bread egg|\n",
      "|Dempster's Hambur...|dairy deli egg br...|             grocery|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|               snack|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|                chip|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|         salty snack|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|           candy bag|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|            chip bag|\n",
      "|   Munchos Chips 60g|snack:chip:salty ...|              market|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|               booze|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|             alcohol|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|            beverage|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|                wine|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|     alcoholic drink|\n",
      "|Llano \"Cab Sav\", ...|booze:alcohol:bev...|              spirit|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res5: Long = 23150151"
     ]
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.databricks.spark.corenlp.functions._"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import com.databricks.spark.corenlp.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: org.apache.spark.sql.DataFrame = [item_name: string, semantic_tags: string ... 2 more fields]"
     ]
    }
   ],
   "source": [
    "var lemmas = data.withColumn(\"tag_lemmas\",lemma('tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res6: Long = 23150151"
     ]
    }
   ],
   "source": [
    "lemmas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|           item_name|       semantic_tags|                 tag|          tag_lemmas|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|Dollarama Xmas-EN...|paper tableware:c...|     paper tableware|  [paper, tableware]|\n",
      "|Dollarama Xmas-EN...|paper tableware:c...|           christmas|         [christmas]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|                 dog|               [dog]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|             poultry|           [poultry]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|meat seafood plan...|[meat, seafood, p...|\n",
      "|         Turkey Subs|dog:poultry:meat ...|    homemade pot pie|[homemade, pot, pie]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|             healthy|           [healthy]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|salami pepperoni ...|[salami, pepperon...|\n",
      "|         Turkey Subs|dog:poultry:meat ...|           meat deli|        [meat, deli]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|               store|             [store]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|       sandwich wrap|    [sandwich, wrap]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|        frozen ready|     [frozen, ready]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|      turkey chicken|   [turkey, chicken]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|                 pet|               [pet]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|     thanksgive sale|  [thanksgive, sale]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|          super hero|       [super, hero]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|             chiller|           [chiller]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|    frozen vegetable| [frozen, vegetable]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|            prepared|          [prepared]|\n",
      "|         Turkey Subs|dog:poultry:meat ...|        meat seafood|     [meat, seafood]|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "lemmas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: org.apache.spark.sql.DataFrame = [tag: string, item_name: string ... 1 more field]"
     ]
    }
   ],
   "source": [
    "lemmas = lemmas.select(col(\"tag\"),col(\"item_name\"),col(\"tag_lemmas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                 tag|           item_name|          tag_lemmas|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|              coffee|Frappuccino Mocha...|            [coffee]|\n",
      "|            beverage|Frappuccino Mocha...|          [beverage]|\n",
      "|  medicine treatment|Walgreens Waterpr...|[medicine, treatm...|\n",
      "|       support brace|Walgreens Waterpr...|    [support, brace]|\n",
      "|         honey syrup|Clover natural honey|      [honey, syrup]|\n",
      "|               casey|Casey's Sour Wate...|             [casey]|\n",
      "|             maynard|Casey's Sour Wate...|           [maynard]|\n",
      "|               candy|Casey's Sour Wate...|             [candy]|\n",
      "|     chocolate sweet|Casey's Sour Wate...|  [chocolate, sweet]|\n",
      "|maynard jolly ran...|Casey's Sour Wate...|[maynard, jolly, ...|\n",
      "|                baby|Enfamil - Poly - ...|              [baby]|\n",
      "|vitamin mineral s...|Enfamil - Poly - ...|[vitamin, mineral...|\n",
      "|          healthcare|Enfamil - Poly - ...|        [healthcare]|\n",
      "|  household wellness|Enfamil - Poly - ...|[household, welln...|\n",
      "|        infant child|Enfamil - Poly - ...|     [infant, child]|\n",
      "|           pink wine|Revlon Insta Blus...|        [pink, wine]|\n",
      "|               cheek|Revlon Insta Blus...|             [cheek]|\n",
      "|           wine rise|Revlon Insta Blus...|        [wine, rise]|\n",
      "|           rise wine|Revlon Insta Blus...|        [rise, wine]|\n",
      "|                wine|Revlon Insta Blus...|              [wine]|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "lemmas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res9: Long = 23150151"
     ]
    }
   ],
   "source": [
    "lemmas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmas: org.apache.spark.sql.DataFrame = [tag: string, item_name: string ... 2 more fields]"
     ]
    }
   ],
   "source": [
    "lemmas = lemmas.withColumn(\"size\", size($\"tag_lemmas\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_tags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tag: string, item_name: string ... 2 more fields]"
     ]
    }
   ],
   "source": [
    "var long_tags = lemmas.filter(col(\"size\") > 5).sort(col(\"size\").desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----+\n",
      "|                 tag|           item_name|          tag_lemmas|size|\n",
      "+--------------------+--------------------+--------------------+----+\n",
      "|cold sinus heartb...|Monistat 3 Combo ...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Boiron's Oscilloc...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|   Motrin IB (6 Tab)|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Boiron's Oscilloc...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Boiron's Oscilloc...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|        Monistat - 1|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|   Motrin IB 100 ct |[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|MONISTAT 7 DAY CO...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|         Rosti bites|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|          Motrin 6ct|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Motrin - I b u p ...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Monistat 7 Combin...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|         Monistat 1 |[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|          MONISTAT 3|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|         Motrin 4 PC|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|Monistat 3 Combin...|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|          Monistat 3|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|    Motrin (24 Tabs)|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|     Motrin IB  24ct|[cold, sinus, hea...|  14|\n",
      "|cold sinus heartb...|          Monistat-3|[cold, sinus, hea...|  14|\n",
      "+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "long_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res11: Long = 116738"
     ]
    }
   ],
   "source": [
    "long_tags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_tags: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [item_name: string, tag: string]"
     ]
    }
   ],
   "source": [
    "long_tags = long_tags.select(col(\"item_name\"), col(\"tag\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res14: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [item_name: string, tag: string]"
     ]
    }
   ],
   "source": [
    "long_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkException: Job 14 cancelled\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2017)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1952)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2222)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2205)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2194)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2271)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2290)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2315)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:978)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$withScope$1.apply(RDD.scala:370)\n",
      "  at org.apache.spark.rdd.RDD.withAction(RDD.scala:1997)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:370)\n",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:977)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2850)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2849)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3410)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:88)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:135)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3409)\n",
      "  at org.apache.spark.sql.Dataset.count(Dataset.scala:2849)\n",
      "  ... 51 elided\n"
     ]
    }
   ],
   "source": [
    "long_tags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tags.write.mode(\"overwrite\").insertInto(\"tmp.eats_semantic_tags_long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
