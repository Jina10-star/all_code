{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Thanks for using Drogon for your interactive Spark application. We update Drogon/SparkMagic as often as possible to make it easier, faster and more reliable for you. Have a question or feedback? Ping us on [uChat](https://uchat.uberinternal.com/uber/channels/spark).</span>\n",
    "\n",
    "What's New\n",
    "- Now you can use `%%configure` and `%%spark` magics to configure and start a Spark session (deprecating hard-to-use `%load_ext sparkmagic.magics` and `manage_spark` magics). Check out [this example](https://workbench.uberinternal.com/explore/knowledge/localfile/cwang/sparkmagic_python2_example.ipynb) for more details.\n",
    "- Improved `%%configure` magic. You now can use it to make all Spark and Drogon configurations from within notebook itself. Check out our [latest documentation & examples](https://docs.google.com/document/d/1mkYtDHquh4FjqTeA0Fxii8lyV-P6qzmoABhmmRwm_00/edit#heading=h.xn14pmoorsn0) for more details.\n",
    "- Bug fixes and performance updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * Computation for a pre fix trip impact for a given region or city.\n",
    " */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs:\n",
       "<pre>{\n",
       "  \"kind\": \"spark\", \n",
       "  \"proxyUser\": \"dhruven.vora\", \n",
       "  \"sparkEnv\": \"SPARK_24\", \n",
       "  \"driverMemory\": \"12g\", \n",
       "  \"queue\": \"maps_route_analytics\", \n",
       "  \"numExecutors\": 200, \n",
       "  \"conf\": {\n",
       "    \"spark.executor.memoryOverhead\": 3072, \n",
       "    \"spark.default.parallelism\": 10000, \n",
       "    \"spark.locality.wait\": \"0\", \n",
       "    \"spark.driver.maxResultSize\": \"10g\"\n",
       "  }, \n",
       "  \"executorCores\": 1, \n",
       "  \"driverCores\": 4, \n",
       "  \"drogonHeaders\": {\n",
       "    \"X-DROGON-CLUSTER\": \"phx2/Secure\"\n",
       "  }, \n",
       "  \"executorMemory\": \"24g\"\n",
       "}</pre><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"kind\": \"spark\", \n",
    "  \"proxyUser\": \"dhruven.vora\", \n",
    "  \"sparkEnv\": \"SPARK_24\", \n",
    "  \"driverMemory\": \"12g\", \n",
    "  \"queue\": \"maps_route_analytics\", \n",
    "  \"numExecutors\": 200, \n",
    "  \"executorCores\": 1, \n",
    "  \"driverCores\": 4,\n",
    "  \"conf\": {\n",
    "    \"spark.driver.maxResultSize\": \"10g\",\n",
    "    \"spark.executor.memoryOverhead\": 3072, \n",
    "    \"spark.locality.wait\": \"0\",\n",
    "    \"spark.default.parallelism\":10000\n",
    "  },\n",
    "  \"executorMemory\": \"24g\",\n",
    "  \"drogonHeaders\": {\n",
    "    \"X-DROGON-CLUSTER\": \"phx2/Secure\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application (can take 60s or more)...\n",
      "Starting heartbeat thread...done.\n",
      "Waiting for Drogon session to be ready...............................................\n",
      "Drogon session is ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>Drogon Session ID</th><th>Spark Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>518766339</td><td>application_1669142328971_34228</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"https://shs-phx2.uberinternal.com/proxy/application_1669142328971_34228\">Link</a></td><td><a target=\"_blank\" href=\"https://hadoop-ui.uberinternal.com/gateway/prodsec_phx2/yarn/nodemanager/node/containerlogs/container_e244_1669142328971_34228_01_000002/dhruven.vora?scheme=http&host=phx5-a85.prod.uber.internal&port=8042\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "\n",
      "\n",
      "Cell execution took 92 seconds.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class NavRouteDivergenceCount"
     ]
    }
   ],
   "source": [
    "/**\n",
    "class definition used in the script\n",
    "*/\n",
    "case class Segment (\n",
    "    segment_uuid: String,\n",
    "    start_junction_uuid: String,\n",
    "    end_junction_uuid: String\n",
    ")\n",
    "\n",
    "case class Location (\n",
    "    latitude: Double,\n",
    "    longitude: Double\n",
    ")\n",
    "\n",
    "case class SegmentTraversalCount (\n",
    "    segment: Segment,\n",
    "    suggestedCount: Int,\n",
    "    overlapCount: Int,\n",
    "    actualCount: Int\n",
    ")\n",
    "\n",
    "case class MapFeature (\n",
    "    uuid: String,\n",
    "    featureType: String,\n",
    "    segment: String,\n",
    "    direction: String,\n",
    "    isCondition: Boolean\n",
    ")\n",
    "\n",
    "case class UMMIssue (\n",
    "    issueuuid: String,\n",
    "    ummbuilduuid: String,\n",
    "    latitude: Double,\n",
    "    longitude: Double,\n",
    "    sampletripuuids: List[String],\n",
    "    featureuuids: String,\n",
    "    numberoftrips: Int,\n",
    "    cityid: Int,\n",
    "    detectorname: String\n",
    ")\n",
    "\n",
    "case class NavRouteDivergence (\n",
    "    trip_id: List[String],\n",
    "    pre_div_segment: Segment,\n",
    "    div_segment: Segment,\n",
    "    post_div_suggested_segment: Segment,\n",
    "    post_div_traversed_segment: Segment,\n",
    "    observations: Int\n",
    ")\n",
    "\n",
    "case class NavRouteDivergenceCount (\n",
    "    preDivSegment: Segment,\n",
    "    divSegment: Segment,\n",
    "    postDivSuggestedSegment: Segment,\n",
    "    postDivTraversedSegment: Segment,\n",
    "    observations: Int,\n",
    "    sampleTrips: List[String]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issueCreationDate: String = 2022-11-16"
     ]
    }
   ],
   "source": [
    "/**\n",
    "input params\n",
    "*/\n",
    "val startDate= \"2022-11-09\"\n",
    "val endDate = \"2022-11-15\"\n",
    "val cityId = 202\n",
    "val issueCreationDate = \"2022-11-16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined object UmmIssueLoader"
     ]
    }
   ],
   "source": [
    "/**\n",
    "this class loads issues from map_creation.meds_umm_issues\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "object UmmIssueLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def loadUmmIssues(utcFromDateStr: String, utcToDateStr: String, cityId: Integer): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select \n",
    "         | issueuuid,\n",
    "         | ummbuilduuid,\n",
    "         | latitude,\n",
    "         | longitude,\n",
    "         | sampletripuuids,\n",
    "         | featureuuids,\n",
    "         | numberoftrips,\n",
    "         | cityid,\n",
    "         | detectorname\n",
    "         | from map_creation.meds_umm_issues\n",
    "         | where createddate between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | and productionrun = true\n",
    "         | and cityid = $cityId\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[UMMIssue] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "        var segments = ListBuffer[String]()\n",
    "        r.getAs[Seq[String]](\"featureuuids\").foreach(row => segments += row)\n",
    "        \n",
    "        var trips = ListBuffer[String]()            \n",
    "        \n",
    "        UMMIssue(\n",
    "            issueuuid = r.getAs[String](\"issueuuid\"),\n",
    "            ummbuilduuid = r.getAs[String](\"ummbuilduuid\"),\n",
    "            latitude = r.getAs[Double](\"latitude\"),\n",
    "            longitude = r.getAs[Double](\"longitude\"),\n",
    "            sampletripuuids = trips.toList,\n",
    "            featureuuids = segments.toList.head,\n",
    "            numberoftrips = r.getAs[Int](\"numberoftrips\"),\n",
    "            cityid = r.getAs[Int](\"cityid\"),\n",
    "            detectorname = r.getAs[String](\"detectorname\")\n",
    "          )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res16: Long = 1315"
     ]
    }
   ],
   "source": [
    "/** \n",
    "load map_creation.meds_umm_issues \n",
    "*/\n",
    "val ummIssuesRaw = UmmIssueLoader.loadUmmIssues(issueCreationDate, issueCreationDate, cityId)\n",
    "val ummIssues = UmmIssueLoader.makeDataset(ummIssuesRaw)\n",
    "ummIssues.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// categorize issues as segments or features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object SegmentTraversalCountLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(utcFromDateStr: String, utcToDateStr: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select segment_uuid, start_junction_uuid, end_junction_uuid, \n",
    "         | sum(suggested_traversals) as suggested_traversals,\n",
    "         | sum(overlap_traversals) as overlap_traversals,\n",
    "         | sum(actual_traversals) as actual_traversals\n",
    "         | from route_corpus_features.segment_traversal_counts\n",
    "         | where segment_uuid is not null \n",
    "         | and line_of_business = 'rides'\n",
    "         | AND vehicle_type in ('CAR')\n",
    "         | and datestr between '$utcFromDateStr' and '$utcToDateStr'\n",
    "         | group by segment_uuid, start_junction_uuid, end_junction_uuid\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[SegmentTraversalCount] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      SegmentTraversalCount(\n",
    "        segment = Segment(r.getAs[String](\"segment_uuid\"), r.getAs[String](\"start_junction_uuid\"), r.getAs[String](\"end_junction_uuid\")),\n",
    "        suggestedCount = r.getAs[Long](\"suggested_traversals\").toInt,\n",
    "        overlapCount = r.getAs[Long](\"overlap_traversals\").toInt,\n",
    "        actualCount = r.getAs[Long](\"actual_traversals\").toInt\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load route_corpus_features.segment_traversal_counts\n",
    "*/\n",
    "val stcRaw = SegmentTraversalCountLoader.load(startDate, endDate)\n",
    "val stc = SegmentTraversalCountLoader.makeDataset(stcRaw).cache\n",
    "stc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "this class loads umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import spark.implicits._\n",
    "\n",
    "object UmmMapFeatureLoader {\n",
    "\n",
    "  /** Run query to load trips from the table by city and day\n",
    "    * @param utcDateStr\n",
    "    * @param cityIds\n",
    "    * */\n",
    "  def load(buildId: String): DataFrame = {\n",
    "\n",
    "    var query =\n",
    "      s\"\"\"select uuid, \n",
    "         | data.roadfurniture.type as type, \n",
    "         | data.roadfurniture.condition as condition, \n",
    "         | data.roadfurniture.onsegment as onsegment\n",
    "         | from umm.map_feature_road_furnitures_tomtom\n",
    "         | where uuid is not null\n",
    "         | AND builduuid = '$buildId'\n",
    "         | AND data.roadfurniture.onsegment is not null\n",
    "         | AND data.roadfurniture.onsegment.uuid is not null\n",
    "         | AND data.roadfurniture.condition is null\n",
    "         | AND data.roadfurniture.type = 'PERMANENT_BARRIER'\"\"\".stripMargin\n",
    "        .replaceAll(\"\\n\", \" \")\n",
    "\n",
    "    spark.sql(query)\n",
    "  }\n",
    "\n",
    "  /** Store dataset in the right schema\n",
    "    * @param rawDataset\n",
    "    * */\n",
    "  def makeDataset(rawDataset: DataFrame): Dataset[MapFeature] = {\n",
    "\n",
    "    rawDataset.map(r => {\n",
    "      MapFeature(\n",
    "        uuid = r.getAs[String](\"uuid\"),\n",
    "        featureType = r.getAs[String](\"type\"),\n",
    "        segment = r.getAs[Row](\"onsegment\").getAs[String](\"uuid\"),\n",
    "        direction = r.getAs[Row](\"onsegment\").getAs[String](\"direction\"),\n",
    "        isCondition = false\n",
    "      )\n",
    "    })\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "load umm.map_feature_road_furnitures_tomtom\n",
    "*/\n",
    "val mapFeaturesRaw = UmmMapFeatureLoader.load(\"418384f4-6680-11ed-ae4a-506b4bb1373e\")\n",
    "val mapFeatures = UmmMapFeatureLoader.makeDataset(mapFeaturesRaw).cache\n",
    "mapFeatures.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "07. SparkMagic (Remote Scala)",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
